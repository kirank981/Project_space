{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirank981/Project_space/blob/main/project_space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP5nJnDbJPYO"
      },
      "source": [
        "# Installing dependences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OravxoTjLhVa"
      },
      "source": [
        "Install the necessary packages for PyTorch (torch and torchvision) and Flower (flwr) and pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "kpAFZGHjJOOk"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQeKkPPLUkF"
      },
      "source": [
        "Import everything we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdmn7_KHLVVI",
        "outputId": "8dbc31fc-cc0c-4c31-e132-42376c2e5e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Training on cpu using PyTorch 2.0.1+cu118 and Flower 1.4.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "# from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # Import the train_test_split function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "!pip install catboost\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from tensorflow.keras.layers import Conv2D, Multiply, Input\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Dense, Multiply\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFfbJxnLxF7"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6l5c_376GqC"
      },
      "source": [
        "Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "QSv4I6ji-3of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c4bb0a-6aeb-4dc5-e433-7dccf47c6b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n4a987x-7kd"
      },
      "source": [
        "Setting the path to the location of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "jQ1t9RTF91ZQ"
      },
      "outputs": [],
      "source": [
        "# Define the path to daily dataset folder\n",
        "daily_dataset_path = Path('/content/drive/MyDrive/Federated learning implementation/dataset/dataset_archive/daily_dataset/daily_dataset')\n",
        "\n",
        "# Define the path to daily dataset folder\n",
        "weather_daily_dataset_path = Path('/content/drive/MyDrive/Federated learning implementation/dataset/dataset_archive/weather_daily_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2owcEz6M9K"
      },
      "source": [
        "## Loading daily data\n",
        "(of energy consumption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "QdpP9rwQBqJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initializing list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Loop through the CSV files and reading them into dataframes\n",
        "for i in range(1):\n",
        "    filename = f'block_{i}.csv'\n",
        "    df = pd.read_csv(daily_dataset_path / filename)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenating all the dataframes into a single dataframe\n",
        "energy_daily_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# NRATM(Not Required At The Moment)\n",
        "# # Group the data by LCLid and create a dictionary of dataframes\n",
        "# grouped_data = dict(tuple(energy_daily_data.groupby('LCLid')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBAsp3Ky51Bn"
      },
      "source": [
        "Loading data using file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "OeJMbTTY0YZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c263f62-ae87-4711-d01e-f4ede79850eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          LCLid         day  energy_median  energy_mean  energy_max  \\\n",
            "0     MAC000002  2012-10-12         0.1385     0.154304       0.886   \n",
            "1     MAC000002  2012-10-13         0.1800     0.230979       0.933   \n",
            "2     MAC000002  2012-10-14         0.1580     0.275479       1.085   \n",
            "3     MAC000002  2012-10-15         0.1310     0.213688       1.164   \n",
            "4     MAC000002  2012-10-16         0.1450     0.203521       0.991   \n",
            "...         ...         ...            ...          ...         ...   \n",
            "1316  MAC000246  2014-02-23         0.4850     0.619083       1.644   \n",
            "1317  MAC000246  2014-02-24         0.1245     0.394479       2.643   \n",
            "1318  MAC000246  2014-02-25         0.2870     0.494104       2.205   \n",
            "1319  MAC000246  2014-02-26         0.2570     0.324208       1.158   \n",
            "1320  MAC000246  2014-02-27         0.1365     0.333875       2.436   \n",
            "\n",
            "      energy_count  energy_std  energy_sum  energy_min  \n",
            "0               46    0.196034       7.098       0.000  \n",
            "1               48    0.192329      11.087       0.076  \n",
            "2               48    0.274647      13.223       0.070  \n",
            "3               48    0.224483      10.257       0.070  \n",
            "4               48    0.184115       9.769       0.087  \n",
            "...            ...         ...         ...         ...  \n",
            "1316            48    0.481155      29.716       0.045  \n",
            "1317            48    0.577050      18.935       0.031  \n",
            "1318            48    0.484374      23.717       0.031  \n",
            "1319            48    0.289996      15.562       0.062  \n",
            "1320            48    0.477727      16.026       0.032  \n",
            "\n",
            "[1321 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "# Loading data from a specific CSV file\n",
        "specific_file_data = pd.read_csv(daily_dataset_path / 'LCLid_3nos.csv')\n",
        "\n",
        "# Displaying data\n",
        "print(specific_file_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0b947PkSfrG"
      },
      "source": [
        "### Loading required data\n",
        "Creating a DataFrame that have only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "WMsi4uZITUt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db80f704-841c-44e7-f62b-5e28a65c70a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Data report of MAC000002\n",
            "         LCLid  energy_sum         day\n",
            "0    MAC000002       7.098  2012-10-12\n",
            "1    MAC000002      11.087  2012-10-13\n",
            "2    MAC000002      13.223  2012-10-14\n",
            "3    MAC000002      10.257  2012-10-15\n",
            "4    MAC000002       9.769  2012-10-16\n",
            "..         ...         ...         ...\n",
            "498  MAC000002      14.886  2014-02-23\n",
            "499  MAC000002      12.528  2014-02-24\n",
            "500  MAC000002      11.826  2014-02-25\n",
            "501  MAC000002      12.328  2014-02-26\n",
            "502  MAC000002      20.518  2014-02-27\n",
            "\n",
            "[503 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "selected_column = ['LCLid','energy_sum','day']\n",
        "# energy_daily_selected=energy_daily_data[selected_column] (check)\n",
        "energy_daily_selected=specific_file_data[selected_column]\n",
        "# print(energy_daily_selected)\n",
        "\n",
        "# Group the data by LCLid and create a dictionary of dataframes, allowing to access each dataframe separately using the LCLid as the key\n",
        "grouped_data_selected = dict(tuple(energy_daily_selected.groupby('LCLid')))\n",
        "# Display the data for 'MAC000002'\n",
        "print('\\n')\n",
        "print('Data report of MAC000002')\n",
        "print(grouped_data_selected['MAC000002'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZumIN7H_Zt-"
      },
      "source": [
        "## Loading daily weather data\n",
        "\n",
        "Creating a 'day' column that stores only the date values from 'time' column\n",
        "(for linking weather dataset 'day' with daily dataset 'day')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Ch2Azrah7Cuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3643a0-1120-4f00-e92b-86604913a912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     temperatureMax   temperatureMaxTime  windBearing                 icon  \\\n",
            "0             11.96  2011-11-11 23:00:00          123                  fog   \n",
            "1              8.59  2011-12-11 14:00:00          198    partly-cloudy-day   \n",
            "2             10.33  2011-12-27 02:00:00          225    partly-cloudy-day   \n",
            "3              8.07  2011-12-02 23:00:00          232                 wind   \n",
            "4              8.22  2011-12-24 23:00:00          252  partly-cloudy-night   \n",
            "..              ...                  ...          ...                  ...   \n",
            "877            9.03  2014-01-26 16:00:00          233    partly-cloudy-day   \n",
            "878           10.31  2014-02-27 14:00:00          224    partly-cloudy-day   \n",
            "879           18.97  2014-03-09 14:00:00          172  partly-cloudy-night   \n",
            "880            8.83  2014-02-12 16:00:00          210                 wind   \n",
            "881            9.90  2014-02-15 12:00:00          233                 wind   \n",
            "\n",
            "     dewPoint   temperatureMinTime  cloudCover  windSpeed  pressure  \\\n",
            "0        9.40  2011-11-11 07:00:00        0.79       3.88   1016.08   \n",
            "1        4.49  2011-12-11 01:00:00        0.56       3.94   1007.71   \n",
            "2        5.47  2011-12-27 23:00:00        0.85       3.54   1032.76   \n",
            "3        3.69  2011-12-02 07:00:00        0.32       3.00   1012.12   \n",
            "4        2.79  2011-12-24 07:00:00        0.37       4.46   1028.17   \n",
            "..        ...                  ...         ...        ...       ...   \n",
            "877      2.39  2014-01-26 21:00:00        0.40       4.55   1002.10   \n",
            "878      3.08  2014-02-27 23:00:00        0.32       4.14   1007.02   \n",
            "879      4.30  2014-03-09 07:00:00        0.04       2.78   1022.44   \n",
            "880      1.94  2014-02-12 01:00:00        0.59       7.24    994.27   \n",
            "881      2.95  2014-02-15 23:00:00        0.35       9.96    988.63   \n",
            "\n",
            "    apparentTemperatureMinTime  ...          sunriseTime  temperatureHighTime  \\\n",
            "0          2011-11-11 07:00:00  ...  2011-11-11 07:12:14  2011-11-11 19:00:00   \n",
            "1          2011-12-11 02:00:00  ...  2011-12-11 07:57:02  2011-12-11 14:00:00   \n",
            "2          2011-12-27 22:00:00  ...  2011-12-27 08:07:06  2011-12-27 14:00:00   \n",
            "3          2011-12-02 07:00:00  ...  2011-12-02 07:46:09  2011-12-02 12:00:00   \n",
            "4          2011-12-24 07:00:00  ...  2011-12-24 08:06:15  2011-12-24 15:00:00   \n",
            "..                         ...  ...                  ...                  ...   \n",
            "877        2014-01-26 22:00:00  ...  2014-01-26 07:48:49  2014-01-26 16:00:00   \n",
            "878        2014-02-27 22:00:00  ...  2014-02-27 06:51:45  2014-02-27 14:00:00   \n",
            "879        2014-03-09 07:00:00  ...  2014-03-09 06:29:49  2014-03-09 14:00:00   \n",
            "880        2014-02-12 01:00:00  ...  2014-02-12 07:21:44  2014-02-12 16:00:00   \n",
            "881        2014-02-15 23:00:00  ...  2014-02-15 07:16:06  2014-02-15 12:00:00   \n",
            "\n",
            "             uvIndexTime                                            summary  \\\n",
            "0    2011-11-11 11:00:00                             Foggy until afternoon.   \n",
            "1    2011-12-11 12:00:00                  Partly cloudy throughout the day.   \n",
            "2    2011-12-27 00:00:00                  Mostly cloudy throughout the day.   \n",
            "3    2011-12-02 10:00:00  Partly cloudy throughout the day and breezy ov...   \n",
            "4    2011-12-24 13:00:00                  Mostly cloudy throughout the day.   \n",
            "..                   ...                                                ...   \n",
            "877  2014-01-26 11:00:00                       Mostly cloudy until evening.   \n",
            "878  2014-02-27 12:00:00                       Partly cloudy until evening.   \n",
            "879  2014-03-09 12:00:00                      Partly cloudy in the evening.   \n",
            "880  2014-02-12 10:00:00  Mostly cloudy until evening and breezy through...   \n",
            "881  2014-02-15 10:00:00           Windy and mostly cloudy until afternoon.   \n",
            "\n",
            "      temperatureLowTime  apparentTemperatureMin  apparentTemperatureMaxTime  \\\n",
            "0    2011-11-11 19:00:00                    6.48         2011-11-11 23:00:00   \n",
            "1    2011-12-12 07:00:00                    0.11         2011-12-11 20:00:00   \n",
            "2    2011-12-27 23:00:00                    5.59         2011-12-27 02:00:00   \n",
            "3    2011-12-02 19:00:00                    0.46         2011-12-02 12:00:00   \n",
            "4    2011-12-24 19:00:00                   -0.51         2011-12-24 23:00:00   \n",
            "..                   ...                     ...                         ...   \n",
            "877  2014-01-27 05:00:00                   -1.30         2014-01-26 15:00:00   \n",
            "878  2014-02-28 02:00:00                    1.41         2014-02-27 14:00:00   \n",
            "879  2014-03-10 05:00:00                    7.08         2014-03-09 14:00:00   \n",
            "880  2014-02-13 05:00:00                   -1.20         2014-02-12 16:00:00   \n",
            "881  2014-02-16 07:00:00                    1.77         2014-02-15 12:00:00   \n",
            "\n",
            "     apparentTemperatureLowTime moonPhase mean_temp  \n",
            "0           2011-11-11 19:00:00      0.52    10.405  \n",
            "1           2011-12-12 08:00:00      0.53     5.535  \n",
            "2           2011-12-28 00:00:00      0.10     9.180  \n",
            "3           2011-12-02 19:00:00      0.25     5.315  \n",
            "4           2011-12-24 20:00:00      0.99     5.695  \n",
            "..                          ...       ...       ...  \n",
            "877         2014-01-27 04:00:00      0.84     6.145  \n",
            "878         2014-02-28 02:00:00      0.93     7.120  \n",
            "879         2014-03-10 06:00:00      0.28    13.310  \n",
            "880         2014-02-13 02:00:00      0.42     5.930  \n",
            "881         2014-02-16 07:00:00      0.52     7.640  \n",
            "\n",
            "[882 rows x 33 columns]\n"
          ]
        }
      ],
      "source": [
        "# Load the weather dataset into a DataFrame\n",
        "weather_daily_data = pd.read_csv(weather_daily_dataset_path)\n",
        "\n",
        "# Convert the 'time' column to datetime format\n",
        "weather_daily_data['time'] = pd.to_datetime(weather_daily_data['time'])\n",
        "\n",
        "# Calculate the mean temperature for each day and store it in a new column 'mean_temp'\n",
        "weather_daily_data['mean_temp'] = (weather_daily_data['temperatureMax'] + weather_daily_data['temperatureMin']) / 2\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(weather_daily_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUsgV_WfSTVp"
      },
      "source": [
        "### Loading required data\n",
        "Creating a DataFrame that have only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "iRn0x1J8RlPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "205579ab-825d-4a9b-fd0e-159071e8cb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     mean_temp  pressure  humidity  windSpeed       time\n",
            "0       10.405   1016.08      0.95       3.88 2011-11-11\n",
            "1        5.535   1007.71      0.88       3.94 2011-12-11\n",
            "2        9.180   1032.76      0.74       3.54 2011-12-27\n",
            "3        5.315   1012.12      0.87       3.00 2011-12-02\n",
            "4        5.695   1028.17      0.80       4.46 2011-12-24\n",
            "..         ...       ...       ...        ...        ...\n",
            "877      6.145   1002.10      0.79       4.55 2014-01-26\n",
            "878      7.120   1007.02      0.74       4.14 2014-02-27\n",
            "879     13.310   1022.44      0.58       2.78 2014-03-09\n",
            "880      5.930    994.27      0.75       7.24 2014-02-12\n",
            "881      7.640    988.63      0.69       9.96 2014-02-15\n",
            "\n",
            "[882 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with selected columns\n",
        "selected_columns = ['mean_temp', 'pressure', 'humidity', 'windSpeed', 'time']\n",
        "weather_selected = weather_daily_data[selected_columns]\n",
        "\n",
        "# Print the new dataset\n",
        "print(weather_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi3pXETWEhtL"
      },
      "source": [
        "# Dataset with Household energy consumption values and weather values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlEy0Fm_JaT5"
      },
      "source": [
        "## For all households"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "wqUqCBNEHb8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7fa276-a9cc-4cc9-c8f2-ed10cae078e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0       10.000   1001.79      0.81       6.54 2012-11-25  MAC000002   \n",
            "1       12.170   1008.74      0.90       5.74 2012-11-20  MAC000002   \n",
            "2        7.830    979.63      0.85       4.07 2012-11-01  MAC000002   \n",
            "3        5.295   1020.29      0.87       3.58 2012-11-06  MAC000002   \n",
            "4        4.650   1001.72      0.80       5.63 2012-12-07  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "662      4.470   1001.76      0.91       1.52 2014-01-30  MAC000246   \n",
            "663      6.145   1002.10      0.79       4.55 2014-01-26  MAC000246   \n",
            "664      7.120   1007.02      0.74       4.14 2014-02-27  MAC000246   \n",
            "665      5.930    994.27      0.75       7.24 2014-02-12  MAC000246   \n",
            "666      7.640    988.63      0.69       9.96 2014-02-15  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        10.545  \n",
            "1        11.221  \n",
            "2        12.209  \n",
            "3        11.663  \n",
            "4        13.248  \n",
            "..          ...  \n",
            "662      22.531  \n",
            "663      35.068  \n",
            "664      16.026  \n",
            "665      12.374  \n",
            "666       8.263  \n",
            "\n",
            "[667 rows x 7 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-93-fc90630300b6>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n"
          ]
        }
      ],
      "source": [
        "# Convert 'time' column in weather_selected to datetime objects\n",
        "weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n",
        "\n",
        "# Create a list to store the merged data DataFrames\n",
        "merged_data_list = []\n",
        "\n",
        "# Iterate through each LCLid in energy_daily_selected\n",
        "for lclid, data in grouped_data_selected.items():\n",
        "    # Convert 'day' column in current LCLid data to datetime objects\n",
        "    data['day'] = pd.to_datetime(data['day'])\n",
        "\n",
        "    # Merge the current LCLid data with weather_selected based on the common date values\n",
        "    merged_data_lclid = pd.merge(weather_selected, data, left_on='time', right_on='day', how='inner')\n",
        "\n",
        "    # Drop the redundant 'day' column from the merged data\n",
        "    merged_data_lclid.drop(columns=['day'], inplace=True)\n",
        "\n",
        "    # Append the merged data to the merged_data_list\n",
        "    merged_data_list.append(merged_data_lclid)\n",
        "\n",
        "# Concatenate the merged data DataFrames in the list\n",
        "merged_data = pd.concat(merged_data_list, ignore_index=True)\n",
        "\n",
        "# Display the merged dataset\n",
        "print(merged_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq2mLT2oJWB5"
      },
      "source": [
        "## For one household"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "LBmTUuggiWZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43968064-7eb1-49c3-ef34-8a103f7919f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0       10.000   1001.79      0.81       6.54 2012-11-25  MAC000002   \n",
            "1       12.170   1008.74      0.90       5.74 2012-11-20  MAC000002   \n",
            "2        7.830    979.63      0.85       4.07 2012-11-01  MAC000002   \n",
            "3        5.295   1020.29      0.87       3.58 2012-11-06  MAC000002   \n",
            "4        4.650   1001.72      0.80       5.63 2012-12-07  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "271      4.470   1001.76      0.91       1.52 2014-01-30  MAC000002   \n",
            "272      6.145   1002.10      0.79       4.55 2014-01-26  MAC000002   \n",
            "273      7.120   1007.02      0.74       4.14 2014-02-27  MAC000002   \n",
            "274      5.930    994.27      0.75       7.24 2014-02-12  MAC000002   \n",
            "275      7.640    988.63      0.69       9.96 2014-02-15  MAC000002   \n",
            "\n",
            "     energy_sum  \n",
            "0        10.545  \n",
            "1        11.221  \n",
            "2        12.209  \n",
            "3        11.663  \n",
            "4        13.248  \n",
            "..          ...  \n",
            "271      14.166  \n",
            "272      24.040  \n",
            "273      20.518  \n",
            "274      13.812  \n",
            "275      12.570  \n",
            "\n",
            "[276 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "print(merged_data[merged_data['LCLid'] == \"MAC000002\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVelsQ53430r"
      },
      "source": [
        "# Splitting dataset to training and testing sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw5aaS7xkIWr"
      },
      "source": [
        "## Preprocessing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWB2ilpvq0_-"
      },
      "source": [
        "### Counting amount of data for each LCLid (data available for each household)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "k_1dDvPeoXFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3282465-c3d0-44af-d928-c0cb5057ad11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data row counts for each LCLid:\n",
            "MAC000246    391\n",
            "MAC000002    276\n",
            "Name: LCLid, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-27ee962d4454>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n"
          ]
        }
      ],
      "source": [
        "# Convert 'time' column in weather_selected to datetime objects\n",
        "weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n",
        "\n",
        "# Merge the data into merged_data DataFrame as described in your previous code\n",
        "\n",
        "# Count the number of data rows for each LCLid\n",
        "lclid_data_counts = merged_data['LCLid'].value_counts()\n",
        "\n",
        "# Display the counts for each LCLid\n",
        "print(\"Data row counts for each LCLid:\")\n",
        "print(lclid_data_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIjz5a0irBRy"
      },
      "source": [
        "### Identifiting the no of households with insufficient amount of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "HlmWA354pGgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2444e1-0dbf-42ad-e976-ce60b3cbeeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of LCLid values with less than 100 data rows: 0\n",
            "Total number of unique LCLid values: 2\n"
          ]
        }
      ],
      "source": [
        "# Count the number of data rows for each LCLid\n",
        "lclid_data_counts = merged_data['LCLid'].value_counts()\n",
        "\n",
        "\n",
        "# Get the total number of unique LCLid values\n",
        "total_lclids = len(lclid_data_counts)\n",
        "\n",
        "# Count the number of LCLid values with less than 100 data rows\n",
        "count_less_than_100 = (lclid_data_counts < 100).sum()\n",
        "\n",
        "# Display the count of LCLid values with less than 100 data rows\n",
        "print(\"Number of LCLid values with less than 100 data rows:\", count_less_than_100)\n",
        "\n",
        "# Display the total number of unique LCLid values\n",
        "print(\"Total number of unique LCLid values:\", total_lclids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA9X4z0irXP7"
      },
      "source": [
        "### Removing the households with less data, from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "3VuTC5T5pphX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84e20ea-ccf2-469c-83e8-c618c3c89abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0       10.000   1001.79      0.81       6.54 2012-11-25  MAC000002   \n",
            "1       12.170   1008.74      0.90       5.74 2012-11-20  MAC000002   \n",
            "2        7.830    979.63      0.85       4.07 2012-11-01  MAC000002   \n",
            "3        5.295   1020.29      0.87       3.58 2012-11-06  MAC000002   \n",
            "4        4.650   1001.72      0.80       5.63 2012-12-07  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "662      4.470   1001.76      0.91       1.52 2014-01-30  MAC000246   \n",
            "663      6.145   1002.10      0.79       4.55 2014-01-26  MAC000246   \n",
            "664      7.120   1007.02      0.74       4.14 2014-02-27  MAC000246   \n",
            "665      5.930    994.27      0.75       7.24 2014-02-12  MAC000246   \n",
            "666      7.640    988.63      0.69       9.96 2014-02-15  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        10.545  \n",
            "1        11.221  \n",
            "2        12.209  \n",
            "3        11.663  \n",
            "4        13.248  \n",
            "..          ...  \n",
            "662      22.531  \n",
            "663      35.068  \n",
            "664      16.026  \n",
            "665      12.374  \n",
            "666       8.263  \n",
            "\n",
            "[667 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Get the list of LCLid values with less than 100 data rows\n",
        "lclids_to_remove = lclid_data_counts[lclid_data_counts < 100].index\n",
        "\n",
        "# Remove rows corresponding to LCLid values with less than 100 data rows\n",
        "filtered_data = merged_data[~merged_data['LCLid'].isin(lclids_to_remove)]\n",
        "\n",
        "# also removing rows with NaN values in the dataset\n",
        "filtered_data = filtered_data.dropna()\n",
        "\n",
        "# Display the filtered data\n",
        "print(filtered_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlfwiI3Rrm1X"
      },
      "source": [
        "## Splitting the dataset to train and test\n",
        " where the split ratio(70:30) is made with every househould's data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "mjDCXMXoizi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbdf9db-70a0-4fb2-b003-f98baf19c365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        3.140   1012.97      0.83       4.76 2013-01-12  MAC000002   \n",
            "1        3.200   1019.29      0.78       3.08 2013-02-08  MAC000002   \n",
            "2        8.050   1033.95      0.89       1.35 2013-11-27  MAC000002   \n",
            "3        6.260   1018.89      0.80       5.44 2012-12-09  MAC000002   \n",
            "4        5.900   1015.79      0.79       2.61 2013-11-10  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "394     11.210   1003.59      0.86       5.51 2012-12-23  MAC000246   \n",
            "395      8.580   1010.84      0.81       7.01 2011-12-08  MAC000246   \n",
            "396      7.755    997.84      0.91       2.38 2012-11-10  MAC000246   \n",
            "397      4.835   1029.69      0.80       4.25 2013-02-28  MAC000246   \n",
            "398      4.815   1023.19      0.83       2.14 2012-12-08  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        10.586  \n",
            "1         9.635  \n",
            "2        11.003  \n",
            "3        11.355  \n",
            "4        25.556  \n",
            "..          ...  \n",
            "394      24.395  \n",
            "395      13.664  \n",
            "396      24.064  \n",
            "397      24.677  \n",
            "398      15.284  \n",
            "\n",
            "[399 rows x 7 columns]\n",
            "Validation set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        4.965   1006.45      0.86       3.61 2013-01-26  MAC000002   \n",
            "1        7.380    991.64      0.82       2.94 2013-11-04  MAC000002   \n",
            "2        4.140   1014.89      0.84       3.07 2013-12-29  MAC000002   \n",
            "3        3.065   1026.97      0.83       4.80 2013-02-25  MAC000002   \n",
            "4        5.515   1004.26      0.78       4.83 2013-11-21  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "128      1.690   1015.13      0.76       2.87 2013-02-12  MAC000246   \n",
            "129     -0.490   1017.44      0.83       1.01 2013-01-17  MAC000246   \n",
            "130      1.990   1009.85      0.92       6.42 2013-03-23  MAC000246   \n",
            "131      6.025   1032.00      0.82       2.30 2013-11-13  MAC000246   \n",
            "132      7.380    991.64      0.82       2.94 2013-11-04  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        16.348  \n",
            "1         9.736  \n",
            "2        14.982  \n",
            "3        11.198  \n",
            "4        11.730  \n",
            "..          ...  \n",
            "128      22.875  \n",
            "129      25.718  \n",
            "130      16.612  \n",
            "131      22.343  \n",
            "132      12.910  \n",
            "\n",
            "[133 rows x 7 columns]\n",
            "Test set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        4.435   1004.33      0.81       2.50 2014-01-14  MAC000002   \n",
            "1        5.730    992.43      0.68       5.83 2014-02-13  MAC000002   \n",
            "2        8.965    993.32      0.86       7.20 2014-01-01  MAC000002   \n",
            "3        8.175   1009.09      0.71       4.77 2014-02-22  MAC000002   \n",
            "4        6.840   1000.02      0.87       4.40 2014-01-05  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "130      5.335    997.47      0.77       3.17 2013-12-28  MAC000246   \n",
            "131      6.970    988.77      0.79       3.98 2014-02-07  MAC000246   \n",
            "132      7.545   1005.39      0.72       4.75 2014-02-02  MAC000246   \n",
            "133      0.895   1014.52      0.80       3.20 2013-01-25  MAC000246   \n",
            "134     10.955    994.61      0.78       8.30 2014-01-06  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        18.532  \n",
            "1        11.723  \n",
            "2        15.496  \n",
            "3        12.734  \n",
            "4        14.418  \n",
            "..          ...  \n",
            "130      21.253  \n",
            "131       9.560  \n",
            "132      31.426  \n",
            "133      21.594  \n",
            "134      16.022  \n",
            "\n",
            "[135 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the split percentages\n",
        "train_percentage = 0.6 # 60% for training\n",
        "val_percentage = 0.3   # 30% for validation\n",
        "test_percentage = 0.3  # 30% for testing\n",
        "min_data_points = 10   # Minimum number of data points required for an LCLid\n",
        "\n",
        "# Create lists to store DataFrames for training, validation, and testing\n",
        "train_data_list = []\n",
        "val_data_list = []\n",
        "test_data_list = []\n",
        "\n",
        "# Iterate through each unique LCLid and split the data based on train_percentage, val_percentage, and test_percentage\n",
        "unique_lclids = filtered_data['LCLid'].unique()\n",
        "for lclid in unique_lclids:\n",
        "    lclid_data = filtered_data[filtered_data['LCLid'] == lclid]\n",
        "\n",
        "    # Check if there are sufficient data points for the current LCLid\n",
        "    if len(lclid_data) >= min_data_points:\n",
        "        # Split the data for the current LCLid into training, validation, and test sets\n",
        "        train_data_lclid, remaining_data_lclid = train_test_split(lclid_data, train_size=train_percentage, shuffle=False)\n",
        "        val_data_lclid, test_data_lclid = train_test_split(remaining_data_lclid, train_size=val_percentage/(val_percentage + test_percentage), shuffle=False)\n",
        "\n",
        "        # Randomize the rows within each subset\n",
        "        train_data_lclid = train_data_lclid.sample(frac=1, random_state=42)\n",
        "        val_data_lclid = val_data_lclid.sample(frac=1, random_state=42)\n",
        "        test_data_lclid = test_data_lclid.sample(frac=1, random_state=42)\n",
        "\n",
        "        # Append to the train_data_list, val_data_list, and test_data_list\n",
        "        train_data_list.append(train_data_lclid)\n",
        "        val_data_list.append(val_data_lclid)\n",
        "        test_data_list.append(test_data_lclid)\n",
        "\n",
        "# Concatenate the DataFrames in the lists\n",
        "FL_train_set = pd.concat(train_data_list, ignore_index=True)\n",
        "FL_val_set = pd.concat(val_data_list, ignore_index=True)\n",
        "FL_test_set = pd.concat(test_data_list, ignore_index=True)\n",
        "\n",
        "# Store the training, validation, and test sets in separate lists\n",
        "FL_train_sets_list = train_data_list\n",
        "FL_val_sets_list = val_data_list\n",
        "FL_test_sets_list = test_data_list\n",
        "\n",
        "# Display the training, validation, and test sets\n",
        "print(\"Training set:\")\n",
        "print(FL_train_set)\n",
        "print(\"Validation set:\")\n",
        "print(FL_val_set)\n",
        "print(\"Test set:\")\n",
        "print(FL_test_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAZQP_IFjqJd"
      },
      "source": [
        "### For FL training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "7RGFE7Iwxbjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b9b3fe-2782-41a6-d6ce-932b1438f277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Training set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "135      3.140   1012.97      0.83       4.76 2013-01-12  MAC000002   \n",
            "115      3.200   1019.29      0.78       3.08 2013-02-08  MAC000002   \n",
            "131      8.050   1033.95      0.89       1.35 2013-11-27  MAC000002   \n",
            "55       6.260   1018.89      0.80       5.44 2012-12-09  MAC000002   \n",
            "95       5.900   1015.79      0.79       2.61 2013-11-10  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "71       9.480   1037.15      0.85       4.28 2013-01-04  MAC000002   \n",
            "106      8.145   1005.53      0.90       2.00 2013-11-08  MAC000002   \n",
            "14       5.745   1014.66      0.82       0.72 2012-11-18  MAC000002   \n",
            "92       0.780   1020.50      0.63       4.79 2013-02-22  MAC000002   \n",
            "102      0.775   1008.99      0.85       3.28 2013-01-15  MAC000002   \n",
            "\n",
            "     energy_sum  \n",
            "135      10.586  \n",
            "115       9.635  \n",
            "131      11.003  \n",
            "55       11.355  \n",
            "95       25.556  \n",
            "..          ...  \n",
            "71        9.857  \n",
            "106      26.190  \n",
            "14       13.779  \n",
            "92       24.018  \n",
            "102       9.830  \n",
            "\n",
            "[165 rows x 7 columns]\n",
            "First Validation set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "196      4.965   1006.45      0.86       3.61 2013-01-26  MAC000002   \n",
            "170      7.380    991.64      0.82       2.94 2013-11-04  MAC000002   \n",
            "197      4.140   1014.89      0.84       3.07 2013-12-29  MAC000002   \n",
            "178      3.065   1026.97      0.83       4.80 2013-02-25  MAC000002   \n",
            "184      5.515   1004.26      0.78       4.83 2013-11-21  MAC000002   \n",
            "214      5.545   1031.32      0.91       1.35 2013-12-10  MAC000002   \n",
            "206     12.090    999.46      0.88       4.86 2013-11-06  MAC000002   \n",
            "191      6.540   1030.42      0.78       3.55 2013-11-24  MAC000002   \n",
            "208      9.230    986.81      0.75       7.86 2013-12-27  MAC000002   \n",
            "177      8.090   1026.88      0.92       3.27 2013-01-07  MAC000002   \n",
            "217      5.335    997.47      0.77       3.17 2013-12-28  MAC000002   \n",
            "168      8.165   1025.35      0.83       4.56 2013-12-08  MAC000002   \n",
            "198      4.220   1034.17      0.83       4.40 2013-02-27  MAC000002   \n",
            "199      5.945   1002.41      0.90       2.52 2013-03-09  MAC000002   \n",
            "173      9.495    998.64      0.74       5.30 2013-11-03  MAC000002   \n",
            "182      4.510   1006.64      0.75       6.04 2013-02-06  MAC000002   \n",
            "171      5.565   1013.74      0.78       3.66 2013-03-04  MAC000002   \n",
            "169      8.990   1034.64      0.87       1.23 2013-11-28  MAC000002   \n",
            "212      5.410   1024.21      0.82       3.37 2013-11-23  MAC000002   \n",
            "192     11.035   1006.38      0.83       2.89 2013-11-07  MAC000002   \n",
            "211      0.895   1014.52      0.80       3.20 2013-01-25  MAC000002   \n",
            "210      2.555   1003.95      0.85       3.41 2013-11-20  MAC000002   \n",
            "218      4.645   1016.63      0.90       3.25 2014-01-12  MAC000002   \n",
            "180      4.120   1006.74      0.86       2.37 2013-03-20  MAC000002   \n",
            "174     10.125   1024.17      0.87       3.71 2013-01-08  MAC000002   \n",
            "181      8.005   1033.95      0.79       1.36 2013-12-02  MAC000002   \n",
            "189      8.530   1012.39      0.74       7.30 2013-01-31  MAC000002   \n",
            "195      2.630   1018.79      0.86       0.68 2013-02-09  MAC000002   \n",
            "202      5.440   1029.50      0.72       0.96 2013-12-03  MAC000002   \n",
            "190      1.990   1009.85      0.92       6.42 2013-03-23  MAC000002   \n",
            "176      4.230   1029.46      0.93       1.45 2013-12-11  MAC000002   \n",
            "165      5.955   1027.30      0.73       3.68 2013-11-30  MAC000002   \n",
            "213     -0.405    994.61      0.90       1.22 2013-01-21  MAC000002   \n",
            "201      3.780   1010.59      0.73       3.65 2013-03-13  MAC000002   \n",
            "194      5.685   1023.72      0.73       6.12 2013-12-05  MAC000002   \n",
            "205      3.725   1015.96      0.75       4.73 2013-03-21  MAC000002   \n",
            "166     -0.455   1001.55      0.88       3.44 2013-01-22  MAC000002   \n",
            "186     -1.190   1013.33      0.91       0.93 2013-01-16  MAC000002   \n",
            "167     12.675    998.60      0.82       4.55 2013-11-02  MAC000002   \n",
            "215      7.400   1023.75      0.74       4.60 2013-11-14  MAC000002   \n",
            "204      4.300   1000.03      0.73       7.30 2013-02-05  MAC000002   \n",
            "200      8.310   1010.88      0.83       6.58 2013-12-30  MAC000002   \n",
            "188      1.765   1011.63      0.64       3.95 2013-03-27  MAC000002   \n",
            "209      7.760   1032.19      0.96       2.23 2013-01-06  MAC000002   \n",
            "175      7.330   1011.45      0.81       2.96 2013-02-14  MAC000002   \n",
            "187      8.925   1024.24      0.87       0.87 2013-11-17  MAC000002   \n",
            "183      8.370   1026.65      0.77       4.40 2013-11-29  MAC000002   \n",
            "219      5.385   1013.91      0.83       2.43 2014-01-24  MAC000002   \n",
            "185      8.680    979.52      0.81       6.67 2013-12-24  MAC000002   \n",
            "172      8.065   1004.27      0.85       4.72 2013-12-31  MAC000002   \n",
            "207     10.035   1006.93      0.77       7.85 2013-01-30  MAC000002   \n",
            "179     -0.490   1017.44      0.83       1.01 2013-01-17  MAC000002   \n",
            "193      6.780   1005.47      0.84       5.95 2013-01-28  MAC000002   \n",
            "216      0.965   1020.81      0.68       3.92 2013-02-23  MAC000002   \n",
            "203      9.250   1035.16      0.92       3.15 2013-01-05  MAC000002   \n",
            "\n",
            "     energy_sum  \n",
            "196      16.348  \n",
            "170       9.736  \n",
            "197      14.982  \n",
            "178      11.198  \n",
            "184      11.730  \n",
            "214      12.882  \n",
            "206      11.627  \n",
            "191      14.341  \n",
            "208       9.122  \n",
            "177       9.439  \n",
            "217      13.658  \n",
            "168      18.280  \n",
            "198      15.769  \n",
            "199      10.858  \n",
            "173      16.200  \n",
            "182      14.222  \n",
            "171      25.447  \n",
            "169      10.513  \n",
            "212      11.214  \n",
            "192      11.834  \n",
            "211      11.260  \n",
            "210      16.814  \n",
            "218      16.034  \n",
            "180      14.014  \n",
            "174      11.640  \n",
            "181       9.061  \n",
            "189       9.208  \n",
            "195      14.746  \n",
            "202      14.641  \n",
            "190      25.599  \n",
            "176      12.935  \n",
            "165       7.858  \n",
            "213      12.456  \n",
            "201      12.956  \n",
            "194      10.495  \n",
            "205      10.727  \n",
            "166      10.870  \n",
            "186      14.408  \n",
            "167      13.312  \n",
            "215      12.202  \n",
            "204      14.146  \n",
            "200      15.082  \n",
            "188       5.097  \n",
            "209      10.293  \n",
            "175      10.042  \n",
            "187       9.094  \n",
            "183      11.510  \n",
            "219      14.540  \n",
            "185       4.781  \n",
            "172      13.746  \n",
            "207      10.297  \n",
            "179       9.857  \n",
            "193       9.907  \n",
            "216      14.125  \n",
            "203      11.171  \n",
            "First Test set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "220      4.435   1004.33      0.81       2.50 2014-01-14  MAC000002   \n",
            "225      5.730    992.43      0.68       5.83 2014-02-13  MAC000002   \n",
            "253      8.965    993.32      0.86       7.20 2014-01-01  MAC000002   \n",
            "233      8.175   1009.09      0.71       4.77 2014-02-22  MAC000002   \n",
            "239      6.840   1000.02      0.87       4.40 2014-01-05  MAC000002   \n",
            "270      7.780   1007.80      0.83       4.07 2014-02-17  MAC000002   \n",
            "256      7.970    979.25      0.77       8.53 2014-02-08  MAC000002   \n",
            "246      8.725   1012.46      0.83       4.00 2014-01-25  MAC000002   \n",
            "264      6.870   1003.42      0.72       5.42 2014-02-21  MAC000002   \n",
            "232      8.995    990.50      0.83       5.01 2014-01-16  MAC000002   \n",
            "274      5.930    994.27      0.75       7.24 2014-02-12  MAC000002   \n",
            "223      7.075    985.33      0.83       4.32 2014-01-28  MAC000002   \n",
            "254      8.750    994.51      0.78       7.66 2014-01-03  MAC000002   \n",
            "250      6.550    996.87      0.75       5.30 2014-02-04  MAC000002   \n",
            "228      4.590    993.99      0.90       4.25 2014-01-29  MAC000002   \n",
            "237      7.590    982.20      0.77       7.86 2014-02-05  MAC000002   \n",
            "226      7.835    989.90      0.82       4.98 2014-02-06  MAC000002   \n",
            "224     11.110   1005.19      0.74       5.02 2014-02-24  MAC000002   \n",
            "261      6.820    993.71      0.84       4.46 2014-01-04  MAC000002   \n",
            "247      8.645    993.31      0.82       4.89 2014-01-02  MAC000002   \n",
            "267      4.870   1007.71      0.89       0.20 2014-01-20  MAC000002   \n",
            "266      7.730   1012.73      0.73       3.82 2014-02-26  MAC000002   \n",
            "272      6.145   1002.10      0.79       4.55 2014-01-26  MAC000002   \n",
            "235      7.275    990.08      0.76       6.97 2014-02-01  MAC000002   \n",
            "229      8.070   1006.71      0.87       2.13 2014-01-22  MAC000002   \n",
            "236      7.660   1011.57      0.87       3.29 2014-02-19  MAC000002   \n",
            "244      8.800    998.57      0.90       4.63 2014-01-15  MAC000002   \n",
            "251      5.895   1012.39      0.82       3.98 2014-01-23  MAC000002   \n",
            "273      7.120   1007.02      0.74       4.14 2014-02-27  MAC000002   \n",
            "268      7.545   1005.39      0.72       4.75 2014-02-02  MAC000002   \n",
            "245      7.745    990.31      0.81       6.60 2014-02-14  MAC000002   \n",
            "231      6.160   1006.70      0.76       3.34 2014-02-16  MAC000002   \n",
            "252      9.420   1001.54      0.84       5.38 2014-02-20  MAC000002   \n",
            "269      6.410   1014.26      0.85       3.56 2014-01-10  MAC000002   \n",
            "257      6.970    988.77      0.79       3.98 2014-02-07  MAC000002   \n",
            "249      7.990   1008.67      0.87       3.06 2014-02-18  MAC000002   \n",
            "260      8.785    991.61      0.83       4.34 2014-01-18  MAC000002   \n",
            "221     10.305   1010.37      0.76       7.10 2014-02-23  MAC000002   \n",
            "241      9.495   1011.94      0.85       4.15 2014-01-08  MAC000002   \n",
            "222      6.355    995.52      0.80       1.72 2014-01-19  MAC000002   \n",
            "263      6.480   1003.89      0.79       4.92 2014-02-03  MAC000002   \n",
            "259      4.815   1010.13      0.89       2.19 2014-01-21  MAC000002   \n",
            "255      8.435   1006.30      0.77       6.03 2014-01-09  MAC000002   \n",
            "243      7.570   1005.09      0.83       4.00 2014-01-13  MAC000002   \n",
            "265     10.180   1003.19      0.78       7.30 2014-01-07  MAC000002   \n",
            "230      5.400    998.51      0.91       4.74 2014-01-31  MAC000002   \n",
            "242      7.450    990.80      0.87       4.68 2014-01-17  MAC000002   \n",
            "238      6.480    984.71      0.66       8.05 2014-02-09  MAC000002   \n",
            "275      7.640    988.63      0.69       9.96 2014-02-15  MAC000002   \n",
            "240      6.430    992.84      0.84       1.66 2014-02-10  MAC000002   \n",
            "227      5.140    996.66      0.76       4.62 2014-02-11  MAC000002   \n",
            "262      9.110   1000.65      0.78       5.69 2014-02-25  MAC000002   \n",
            "234      5.255    989.87      0.79       5.29 2014-01-27  MAC000002   \n",
            "248     10.955    994.61      0.78       8.30 2014-01-06  MAC000002   \n",
            "271      4.470   1001.76      0.91       1.52 2014-01-30  MAC000002   \n",
            "258      4.805   1018.51      0.81       2.68 2014-01-11  MAC000002   \n",
            "\n",
            "     energy_sum  \n",
            "220      18.532  \n",
            "225      11.723  \n",
            "253      15.496  \n",
            "233      12.734  \n",
            "239      14.418  \n",
            "270      12.917  \n",
            "256      17.172  \n",
            "246      14.039  \n",
            "264      13.083  \n",
            "232      15.802  \n",
            "274      13.812  \n",
            "223      13.921  \n",
            "254      11.382  \n",
            "250      24.363  \n",
            "228      20.304  \n",
            "237      17.018  \n",
            "226      13.352  \n",
            "224      12.528  \n",
            "261      16.176  \n",
            "247      14.195  \n",
            "267      12.486  \n",
            "266      12.328  \n",
            "272      24.040  \n",
            "235      11.425  \n",
            "229      14.116  \n",
            "236      14.997  \n",
            "244      19.011  \n",
            "251      12.585  \n",
            "273      20.518  \n",
            "268      27.272  \n",
            "245      13.497  \n",
            "231      21.940  \n",
            "252      11.876  \n",
            "269      14.964  \n",
            "257      16.663  \n",
            "249      17.066  \n",
            "260      12.371  \n",
            "221      14.886  \n",
            "241      13.079  \n",
            "222      14.800  \n",
            "263      17.468  \n",
            "259      16.025  \n",
            "255      14.577  \n",
            "243      18.266  \n",
            "265      16.492  \n",
            "230      21.574  \n",
            "242      14.894  \n",
            "238      12.657  \n",
            "275      12.570  \n",
            "240      10.846  \n",
            "227      15.586  \n",
            "262      11.826  \n",
            "234      22.629  \n",
            "248      14.302  \n",
            "271      14.166  \n",
            "258      14.466  \n"
          ]
        }
      ],
      "source": [
        "# Display the first training set\n",
        "print(\"First Training set:\")\n",
        "print(FL_train_sets_list[0])\n",
        "print(\"First Validation set:\")\n",
        "print(FL_val_sets_list[0])\n",
        "print(\"First Test set:\")\n",
        "print(FL_test_sets_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Bc6V-SGNCaAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068fc3b4-a041-4f1e-db97-4f5c412321ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Training set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "345      1.605   1016.35      0.84       2.10 2012-12-02  MAC000246   \n",
            "482     10.680   1012.62      0.86       8.91 2013-12-21  MAC000246   \n",
            "456      5.400   1016.89      0.82       4.00 2013-01-01  MAC000246   \n",
            "285      4.840   1006.01      0.77       5.57 2011-12-05  MAC000246   \n",
            "403      9.935    999.12      0.91       4.28 2012-12-24  MAC000246   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "382     11.210   1003.59      0.86       5.51 2012-12-23  MAC000246   \n",
            "290      8.580   1010.84      0.81       7.01 2011-12-08  MAC000246   \n",
            "368      7.755    997.84      0.91       2.38 2012-11-10  MAC000246   \n",
            "455      4.835   1029.69      0.80       4.25 2013-02-28  MAC000246   \n",
            "378      4.815   1023.19      0.83       2.14 2012-12-08  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "345      27.061  \n",
            "482      10.588  \n",
            "456      36.512  \n",
            "285       6.020  \n",
            "403      26.337  \n",
            "..          ...  \n",
            "382      24.395  \n",
            "290      13.664  \n",
            "368      24.064  \n",
            "455      24.677  \n",
            "378      15.284  \n",
            "\n",
            "[234 rows x 7 columns]\n",
            "First Validation set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "543      2.340   1005.94      0.82       5.05 2013-03-10  MAC000246   \n",
            "510      4.405   1020.42      0.83       2.86 2013-02-18  MAC000246   \n",
            "544      6.915   1023.52      0.70       1.11 2013-02-16  MAC000246   \n",
            "522      8.050   1033.95      0.89       1.35 2013-11-27  MAC000246   \n",
            "520      5.255    995.06      0.92       2.27 2013-12-26  MAC000246   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "530      1.690   1015.13      0.76       2.87 2013-02-12  MAC000246   \n",
            "570     -0.490   1017.44      0.83       1.01 2013-01-17  MAC000246   \n",
            "581      1.990   1009.85      0.92       6.42 2013-03-23  MAC000246   \n",
            "524      6.025   1032.00      0.82       2.30 2013-11-13  MAC000246   \n",
            "561      7.380    991.64      0.82       2.94 2013-11-04  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "543      27.582  \n",
            "510      15.095  \n",
            "544      19.839  \n",
            "522      16.018  \n",
            "520      24.472  \n",
            "..          ...  \n",
            "530      22.875  \n",
            "570      25.718  \n",
            "581      16.612  \n",
            "524      22.343  \n",
            "561      12.910  \n",
            "\n",
            "[78 rows x 7 columns]\n",
            "First Test set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "618      5.140    996.66      0.76       4.62 2014-02-11  MAC000246   \n",
            "588      4.140   1014.89      0.84       3.07 2013-12-29  MAC000246   \n",
            "610      5.385   1013.91      0.83       2.43 2014-01-24  MAC000246   \n",
            "619      4.590    993.99      0.90       4.25 2014-01-29  MAC000246   \n",
            "606      7.400   1023.75      0.74       4.60 2013-11-14  MAC000246   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "608      5.335    997.47      0.77       3.17 2013-12-28  MAC000246   \n",
            "648      6.970    988.77      0.79       3.98 2014-02-07  MAC000246   \n",
            "659      7.545   1005.39      0.72       4.75 2014-02-02  MAC000246   \n",
            "602      0.895   1014.52      0.80       3.20 2013-01-25  MAC000246   \n",
            "639     10.955    994.61      0.78       8.30 2014-01-06  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "618      26.641  \n",
            "588      28.317  \n",
            "610      11.117  \n",
            "619      16.243  \n",
            "606      16.882  \n",
            "..          ...  \n",
            "608      21.253  \n",
            "648       9.560  \n",
            "659      31.426  \n",
            "602      21.594  \n",
            "639      16.022  \n",
            "\n",
            "[79 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Display the first training set\n",
        "print(\"First Training set:\")\n",
        "print(FL_train_sets_list[1])\n",
        "print(\"First Validation set:\")\n",
        "print(FL_val_sets_list[1])\n",
        "print(\"First Test set:\")\n",
        "print(FL_test_sets_list[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MApq3bu5sigc"
      },
      "source": [
        "### Centralised training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "YQOhoHUdtI7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd83226-cfe3-4635-9da3-562632894034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Central Training set size: 399\n",
            "Central Validation set size: 133\n",
            "Central Test set size: 135\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Copy FL training set to central training set\n",
        "Central_train_data = FL_train_set.copy()\n",
        "\n",
        "# Copy FL validation set to central validation set\n",
        "Central_val_data = FL_val_set.copy()\n",
        "\n",
        "# Copy FL test set to central test set\n",
        "Central_test_data = FL_test_set.copy()\n",
        "\n",
        "# Display the sizes of the sets\n",
        "print(\"Central Training set size:\", len(Central_train_data))\n",
        "print(\"Central Validation set size:\", len(Central_val_data))\n",
        "print(\"Central Test set size:\", len(Central_test_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXufv5y-tO1x"
      },
      "source": [
        "Displaying training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "gUNX3w0utFkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfcd5d4-649e-4813-881a-cbe7bc5e5ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        3.140   1012.97      0.83       4.76 2013-01-12  MAC000002   \n",
            "1        3.200   1019.29      0.78       3.08 2013-02-08  MAC000002   \n",
            "2        8.050   1033.95      0.89       1.35 2013-11-27  MAC000002   \n",
            "3        6.260   1018.89      0.80       5.44 2012-12-09  MAC000002   \n",
            "4        5.900   1015.79      0.79       2.61 2013-11-10  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "394     11.210   1003.59      0.86       5.51 2012-12-23  MAC000246   \n",
            "395      8.580   1010.84      0.81       7.01 2011-12-08  MAC000246   \n",
            "396      7.755    997.84      0.91       2.38 2012-11-10  MAC000246   \n",
            "397      4.835   1029.69      0.80       4.25 2013-02-28  MAC000246   \n",
            "398      4.815   1023.19      0.83       2.14 2012-12-08  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        10.586  \n",
            "1         9.635  \n",
            "2        11.003  \n",
            "3        11.355  \n",
            "4        25.556  \n",
            "..          ...  \n",
            "394      24.395  \n",
            "395      13.664  \n",
            "396      24.064  \n",
            "397      24.677  \n",
            "398      15.284  \n",
            "\n",
            "[399 rows x 7 columns]\n",
            "Validation set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        4.965   1006.45      0.86       3.61 2013-01-26  MAC000002   \n",
            "1        7.380    991.64      0.82       2.94 2013-11-04  MAC000002   \n",
            "2        4.140   1014.89      0.84       3.07 2013-12-29  MAC000002   \n",
            "3        3.065   1026.97      0.83       4.80 2013-02-25  MAC000002   \n",
            "4        5.515   1004.26      0.78       4.83 2013-11-21  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "128      1.690   1015.13      0.76       2.87 2013-02-12  MAC000246   \n",
            "129     -0.490   1017.44      0.83       1.01 2013-01-17  MAC000246   \n",
            "130      1.990   1009.85      0.92       6.42 2013-03-23  MAC000246   \n",
            "131      6.025   1032.00      0.82       2.30 2013-11-13  MAC000246   \n",
            "132      7.380    991.64      0.82       2.94 2013-11-04  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        16.348  \n",
            "1         9.736  \n",
            "2        14.982  \n",
            "3        11.198  \n",
            "4        11.730  \n",
            "..          ...  \n",
            "128      22.875  \n",
            "129      25.718  \n",
            "130      16.612  \n",
            "131      22.343  \n",
            "132      12.910  \n",
            "\n",
            "[133 rows x 7 columns]\n",
            "Test set:\n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        4.435   1004.33      0.81       2.50 2014-01-14  MAC000002   \n",
            "1        5.730    992.43      0.68       5.83 2014-02-13  MAC000002   \n",
            "2        8.965    993.32      0.86       7.20 2014-01-01  MAC000002   \n",
            "3        8.175   1009.09      0.71       4.77 2014-02-22  MAC000002   \n",
            "4        6.840   1000.02      0.87       4.40 2014-01-05  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "130      5.335    997.47      0.77       3.17 2013-12-28  MAC000246   \n",
            "131      6.970    988.77      0.79       3.98 2014-02-07  MAC000246   \n",
            "132      7.545   1005.39      0.72       4.75 2014-02-02  MAC000246   \n",
            "133      0.895   1014.52      0.80       3.20 2013-01-25  MAC000246   \n",
            "134     10.955    994.61      0.78       8.30 2014-01-06  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        18.532  \n",
            "1        11.723  \n",
            "2        15.496  \n",
            "3        12.734  \n",
            "4        14.418  \n",
            "..          ...  \n",
            "130      21.253  \n",
            "131       9.560  \n",
            "132      31.426  \n",
            "133      21.594  \n",
            "134      16.022  \n",
            "\n",
            "[135 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Display the training and test sets\n",
        "print(\"Training set:\")\n",
        "print(Central_train_data)\n",
        "print(\"Validation set:\")\n",
        "print(Central_val_data)\n",
        "print(\"Test set:\")\n",
        "print(Central_test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpGqOqfnfFW"
      },
      "source": [
        "# Centralised Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe8CeDeXpYMQ"
      },
      "source": [
        "## Defining features and target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "aPxOdMTSpORn"
      },
      "outputs": [],
      "source": [
        "# Separate features and target variable\n",
        "Central_X_train = Central_train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_train = Central_train_data['energy_sum']\n",
        "\n",
        "# Separate features and target variable\n",
        "Central_X_val = Central_val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_val = Central_val_data['energy_sum']\n",
        "\n",
        "# Separate features and target variable\n",
        "Central_X_test = Central_test_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_test = Central_test_data['energy_sum']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrtqxMEqk_2"
      },
      "source": [
        "## Preprocessing (scaler and imputer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "VNGXlzX_sH77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effdbcac-0d8d-463b-ea61-b998dcb0042c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Central Training Dataset:\n",
            "Original Shape: (399, 4)\n",
            "Preprocessed Shape: (399, 4)\n",
            "\n",
            "Central Validation Dataset:\n",
            "Original Shape: (133, 4)\n",
            "Preprocessed Shape: (133, 4)\n",
            "\n",
            "Central Test Dataset:\n",
            "Original Shape: (135, 4)\n",
            "Preprocessed Shape: (135, 4)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the datasets\n",
        "datasets = [\n",
        "    (Central_X_train, Central_y_train, \"Central Training\"),\n",
        "    (Central_X_val, Central_y_val, \"Central Validation\"),\n",
        "    (Central_X_test, Central_y_test, \"Central Test\")\n",
        "]\n",
        "\n",
        "# Preprocess each dataset\n",
        "for X, y, name in datasets:\n",
        "    # Impute missing values and scale features\n",
        "    X_scaled = StandardScaler().fit_transform(SimpleImputer(strategy='mean').fit_transform(X))\n",
        "\n",
        "    # Display dataset name and shape\n",
        "    print(f\"{name} Dataset:\")\n",
        "    print(\"Original Shape:\", X.shape)\n",
        "    print(\"Preprocessed Shape:\", X_scaled.shape)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "PqoTpCpfHd8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fb7b11-3ba6-45c1-9fae-e441df6b85c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "mean_temp    0\n",
            "pressure     0\n",
            "humidity     0\n",
            "windSpeed    0\n",
            "dtype: int64\n",
            "0\n",
            "mean_temp    0\n",
            "pressure     0\n",
            "humidity     0\n",
            "windSpeed    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-105-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
            "<ipython-input-105-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
            "<ipython-input-105-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
            "<ipython-input-105-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(Central_y_train.isnull().sum())\n",
        "print(Central_X_train.isnull().sum())\n",
        "\n",
        "# Replace missing values with mean (you can choose another strategy)\n",
        "Central_y_train.fillna(Central_y_train.mean(), inplace=True)\n",
        "Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(Central_y_train.isnull().sum())\n",
        "print(Central_X_train.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDqGsD-tGiAH"
      },
      "source": [
        "## Model selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "w9syWdcgGg9Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # List of models to train\n",
        "# models = [\n",
        "#     LinearRegression(),\n",
        "#     DecisionTreeRegressor(),\n",
        "#     RandomForestRegressor(),\n",
        "#     GradientBoostingRegressor(),\n",
        "#     XGBRegressor(),\n",
        "#     LGBMRegressor(),\n",
        "#     CatBoostRegressor(),\n",
        "#     SVR(),\n",
        "#     MLPRegressor(),\n",
        "#     KNeighborsRegressor()\n",
        "# ]\n",
        "\n",
        "# # Dictionary to store model performances\n",
        "# model_performances = {}\n",
        "\n",
        "# # Train and evaluate each model on the validation set\n",
        "# for model in models:\n",
        "#     model_name = model.__class__.__name__\n",
        "#     print(f\"Training {model_name}...\")\n",
        "#     model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "#     # Predict on the validation set\n",
        "#     y_pred_val = model.predict(Central_X_val)\n",
        "#     # print(y_pred_val)\n",
        "\n",
        "#     # Calculate Mean Squared Error\n",
        "#     # mse_val = mean_squared_error(Central_y_val, y_pred_val)\n",
        "#     model_performances[model_name] = (Central_y_val-y_pred_val)\n",
        "#     print(model_performances[model_name])\n",
        "\n",
        "# # Find the best-performing model\n",
        "# best_model = min(model_performances, key=model_performances.get)\n",
        "# print(f\"Best-performing model: {best_model} (MSE: {model_performances[best_model]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "xZDeUas4R_tS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "6ad2df48-020e-4c49-cd6f-f39507cd93c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define models\n",
        "linear_model = LinearRegression()\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, objective='reg:squarederror', random_state=0)\n",
        "\n",
        "# Remove rows with missing values from both features and target variables\n",
        "Central_train_data_cleaned = Central_train_data.dropna()\n",
        "Central_X_train = Central_train_data_cleaned[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_train = Central_train_data_cleaned['energy_sum']\n",
        "\n",
        "# Drop rows with NaN values in the target variable in the validation set\n",
        "Central_val_data.dropna(subset=['energy_sum'], inplace=True)\n",
        "\n",
        "# Train linear model\n",
        "linear_model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Train random forest model\n",
        "rf_model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model.fit(Central_X_train, Central_y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "GOApr5TMrh6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22979723-91fc-41c2-da6c-0b8f81358467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 2s 36ms/step - loss: 4676.6846 - mse: 4676.6846 - val_loss: 548.4885 - val_mse: 548.4885\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 959.1257 - mse: 959.1257 - val_loss: 279.4042 - val_mse: 279.4042\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 147.4260 - mse: 147.4260 - val_loss: 162.9514 - val_mse: 162.9514\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 106.1415 - mse: 106.1415 - val_loss: 88.6135 - val_mse: 88.6135\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 74.3914 - mse: 74.3914 - val_loss: 40.6638 - val_mse: 40.6638\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 55.8731 - mse: 55.8731 - val_loss: 43.1126 - val_mse: 43.1126\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 55.1737 - mse: 55.1737 - val_loss: 45.7638 - val_mse: 45.7638\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 24ms/step - loss: 53.4443 - mse: 53.4443 - val_loss: 40.8183 - val_mse: 40.8183\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 52.9154 - mse: 52.9154 - val_loss: 44.0280 - val_mse: 44.0280\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 53.3363 - mse: 53.3363 - val_loss: 41.4289 - val_mse: 41.4289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x783ab80a5ba0>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "# Build a simple feedforward neural network(FNN)\n",
        "fnn_model = keras.Sequential([\n",
        "    layers.Input(shape=(4,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "fnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "fnn_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_data=(Central_X_val, Central_y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "2BMpFLFqfXsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10b8299-b256-47d8-def0-48c6bb6cb039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 7s 99ms/step - loss: 3413.9390 - mse: 3413.9390 - val_loss: 289.1445 - val_mse: 289.1445\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 464.8233 - mse: 464.8233 - val_loss: 56.9872 - val_mse: 56.9872\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 24ms/step - loss: 164.6505 - mse: 164.6505 - val_loss: 61.9965 - val_mse: 61.9965\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 87.1530 - mse: 87.1530 - val_loss: 43.7248 - val_mse: 43.7248\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 23ms/step - loss: 61.6928 - mse: 61.6928 - val_loss: 53.7112 - val_mse: 53.7112\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 55.3522 - mse: 55.3522 - val_loss: 43.2451 - val_mse: 43.2451\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 54.5132 - mse: 54.5132 - val_loss: 39.1457 - val_mse: 39.1457\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 28ms/step - loss: 54.4402 - mse: 54.4402 - val_loss: 40.5990 - val_mse: 40.5990\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 53.4334 - mse: 53.4334 - val_loss: 41.3424 - val_mse: 41.3424\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 53.4605 - mse: 53.4605 - val_loss: 40.2521 - val_mse: 40.2521\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7839b51e6a10>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "# Building a simple RNN model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Create an RNN model\n",
        "simple_rnn_model = Sequential()\n",
        "simple_rnn_model.add(SimpleRNN(64, activation='relu', input_shape=(Central_X_train.shape[1], 1)))\n",
        "simple_rnn_model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "simple_rnn_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "simple_rnn_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_data=(Central_X_val, Central_y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "VK9tFvolsTkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad73ed7-8dc5-4e10-8162-a1ad8649ecfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 6s 111ms/step - loss: 230.7830 - val_loss: 90.9033\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 28ms/step - loss: 59.1827 - val_loss: 40.0653\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 54.3696 - val_loss: 39.4608\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 53.8733 - val_loss: 41.0915\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 53.3361 - val_loss: 43.2130\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 27ms/step - loss: 53.2743 - val_loss: 42.5950\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 26ms/step - loss: 53.2543 - val_loss: 41.9000\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 25ms/step - loss: 53.2170 - val_loss: 41.9105\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 25ms/step - loss: 53.3278 - val_loss: 41.5798\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 27ms/step - loss: 53.2002 - val_loss: 43.0868\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x783ab84206d0>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Reshape data for LSTM input (samples, timesteps, features)\n",
        "num_timesteps = 1\n",
        "num_features = Central_X_train.shape[1]\n",
        "\n",
        "X_train_reshaped = Central_X_train.values.reshape((Central_X_train.shape[0], num_timesteps, num_features))\n",
        "X_val_reshaped = Central_X_val.values.reshape((Central_X_val.shape[0], num_timesteps, num_features))\n",
        "\n",
        "# Create an LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, activation='relu', input_shape=(num_timesteps, num_features)))\n",
        "lstm_model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_reshaped, Central_y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, Central_y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq6pYV2vajNg"
      },
      "source": [
        "### Defining the Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Nv4rwuSLaikH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Store the evaluation results for each model\n",
        "eval_results = []\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    predictions = model.predict(X)\n",
        "    mse = mean_squared_error(y, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y, predictions)\n",
        "    r2 = r2_score(y, predictions)\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(\"Mean Squared Error:\", mse)\n",
        "    print(\"Root Mean Squared Error:\", rmse)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    print(\"R-squared:\", r2)\n",
        "    print(\"R-squared variance:\", r2*100)\n",
        "    print()\n",
        "    return mse, rmse, mae, r2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOuTnMTmalw5"
      },
      "source": [
        "### Using the Evalution Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "ALw9fSQTaTlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e93bd737-f18d-474c-e1f7-6be1847057fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: LinearRegression\n",
            "Mean Squared Error: 41.17150212588838\n",
            "Root Mean Squared Error: 6.416502328051349\n",
            "Mean Absolute Error: 4.8898272950595505\n",
            "R-squared: -0.0664766567428332\n",
            "R-squared variance: -6.647665674283321\n",
            "\n",
            "Model: RandomForestRegressor\n",
            "Mean Squared Error: 57.35830567047333\n",
            "Root Mean Squared Error: 7.573526633641247\n",
            "Mean Absolute Error: 5.807958722543663\n",
            "R-squared: -0.4857678469160289\n",
            "R-squared variance: -48.576784691602896\n",
            "\n",
            "Model: XGBRegressor\n",
            "Mean Squared Error: 74.19725545563229\n",
            "Root Mean Squared Error: 8.61378287720513\n",
            "Mean Absolute Error: 6.62705374229507\n",
            "R-squared: -0.9219517591528545\n",
            "R-squared variance: -92.19517591528546\n",
            "\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "Model: Sequential\n",
            "Mean Squared Error: 41.428910324657714\n",
            "Root Mean Squared Error: 6.436529369517218\n",
            "Mean Absolute Error: 4.96584446852752\n",
            "R-squared: -0.07314437157146214\n",
            "R-squared variance: -7.314437157146214\n",
            "\n",
            "5/5 [==============================] - 1s 6ms/step\n",
            "Model: Sequential\n",
            "Mean Squared Error: 40.25208438573107\n",
            "Root Mean Squared Error: 6.344453040706588\n",
            "Mean Absolute Error: 4.920394682678315\n",
            "R-squared: -0.0426607280775424\n",
            "R-squared variance: -4.26607280775424\n",
            "\n",
            "5/5 [==============================] - 1s 4ms/step\n",
            "Model: Sequential\n",
            "Mean Squared Error: 43.08678937414386\n",
            "Root Mean Squared Error: 6.564052816221382\n",
            "Mean Absolute Error: 4.974513572336389\n",
            "R-squared: -0.11608886508480931\n",
            "R-squared variance: -11.608886508480932\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define the models as trained earlier\n",
        "models = [\n",
        "    ('Linear Regression', linear_model),\n",
        "    ('Random Forest', rf_model),\n",
        "    ('XGBoost', xgb_model),\n",
        "    ('Neural Network', fnn_model),\n",
        "    ('RNN',simple_rnn_model),\n",
        "    ('LSTM',lstm_model)\n",
        "]\n",
        "\n",
        "# Store the evaluation results for each model\n",
        "eval_results = []\n",
        "\n",
        "# Evaluate each model and store results\n",
        "for model_name, model in models:\n",
        "    if model_name == 'LSTM':\n",
        "        X_val_reshaped = Central_X_val.values.reshape((Central_X_val.shape[0], num_timesteps, num_features))\n",
        "        mse, rmse, mae, r2 = evaluate_model(model, X_val_reshaped, Central_y_val)\n",
        "    else:\n",
        "        mse, rmse, mae, r2 = evaluate_model(model, Central_X_val, Central_y_val)\n",
        "\n",
        "    eval_results.append((model_name, mse, rmse, mae, r2))\n",
        "\n",
        "# Convert eval_results to a numpy array for plotting\n",
        "eval_results = np.array(eval_results)\n",
        "\n",
        "# Plot the evaluation metrics for each model\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(models))\n",
        "metrics = ['MSE', 'RMSE', 'MAE', 'R-squared']\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.bar(x, eval_results[:, i + 1].astype(float), tick_label=eval_results[:, 0])\n",
        "    plt.title(metric)\n",
        "    plt.xticks(rotation=15)\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1rDcfEIXR2Q"
      },
      "source": [
        "### Hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "ll_KLWBJv55X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae94ac0-5121-435a-c079-ae3a6d5b3388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.1s\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Create XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_xgb_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best_xgb_model.fit(FL_val_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']],FL_val_set['energy_sum'])\n",
        "best_xgb_model.fit(Central_X_train, Central_y_train)"
      ],
      "metadata": {
        "id": "EXsPBLka-Xqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "8b6ebc28-e458-44b1-a277-8bec7d8d08a3"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=0.8, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=0.8, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=0.8, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxA3oa-qR9NI"
      },
      "source": [
        "# Federated Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing more modules"
      ],
      "metadata": {
        "id": "9Ri8FEKkbpXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import bz2\n",
        "import shutil\n",
        "\n",
        "!nvidia-smi\n",
        "!pip install matplotlib scikit-learn tqdm torch torchmetrics torchsummary xgboost\n",
        "!pip install -U \"flwr-nightly[simulation]\"\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchmetrics import Accuracy, MeanSquaredError\n",
        "from tqdm import trange, tqdm\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Flower related modules for federated XGBoost\n",
        "import flwr as fl\n",
        "from flwr.common.typing import Parameters\n",
        "from collections import OrderedDict\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from flwr.common import NDArray, NDArrays\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "ulFjWJZEGep9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7034aafa-0509-4ca1-eab6-da96fc742d2f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 20 15:14:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    34W /  70W |   1153MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: flwr-nightly[simulation] in /usr/local/lib/python3.10/dist-packages (1.5.0.dev20230818)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (41.0.3)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (1.51.3)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (3.20.3)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (3.18.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (1.10.12)\n",
            "Requirement already satisfied: ray[default]==2.5.1 in /usr/local/lib/python3.10/dist-packages (from flwr-nightly[simulation]) (2.5.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (8.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (4.19.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (1.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (23.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (2.31.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (3.8.5)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (0.5.5)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (0.3.14)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (1.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (0.11.2)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (0.17.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (6.3.0)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.5.1->flwr-nightly[simulation]) (20.21.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr-nightly[simulation]) (1.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->flwr-nightly[simulation]) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.5.1->flwr-nightly[simulation]) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.5.1->flwr-nightly[simulation]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.5.1->flwr-nightly[simulation]) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.5.1->flwr-nightly[simulation]) (1.9.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr-nightly[simulation]) (2.21)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]==2.5.1->flwr-nightly[simulation]) (12.535.77)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]==2.5.1->flwr-nightly[simulation]) (5.9.5)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]==2.5.1->flwr-nightly[simulation]) (1.20.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]==2.5.1->flwr-nightly[simulation]) (0.3.7)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]==2.5.1->flwr-nightly[simulation]) (3.10.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]==2.5.1->flwr-nightly[simulation]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]==2.5.1->flwr-nightly[simulation]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]==2.5.1->flwr-nightly[simulation]) (0.9.2)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (2.11.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]==2.5.1->flwr-nightly[simulation]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]==2.5.1->flwr-nightly[simulation]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]==2.5.1->flwr-nightly[simulation]) (2023.7.22)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]==2.5.1->flwr-nightly[simulation]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]==2.5.1->flwr-nightly[simulation]) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.5.1->flwr-nightly[simulation]) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_xgbtree(tree: XGBRegressor, n_tree: int) -> None:\n",
        "    \"\"\"Visualize the built xgboost tree.\"\"\"\n",
        "    xgb.plot_tree(tree, num_trees=n_tree)\n",
        "    plt.rcParams[\"figure.figsize\"] = [50, 10]\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def construct_tree(\n",
        "    dataset: Dataset, label: NDArray, n_estimators: int, tree_type: str\n",
        ") -> XGBRegressor:\n",
        "    \"\"\"Construct a xgboost tree form tabular dataset.\"\"\"\n",
        "    if tree_type == \"REG\":\n",
        "      tree = xgb.XGBRegressor(\n",
        "          objective=\"reg:squarederror\",\n",
        "          learning_rate=0.1,\n",
        "          max_depth=8,\n",
        "          n_estimators=n_estimators,\n",
        "          subsample=0.8,\n",
        "          colsample_bylevel=1,\n",
        "          colsample_bynode=1,\n",
        "          colsample_bytree=1,\n",
        "          alpha=5,\n",
        "          gamma=5,\n",
        "          num_parallel_tree=1,\n",
        "          min_child_weight=1,\n",
        "      )\n",
        "\n",
        "    tree.fit(dataset, label)\n",
        "    return tree\n",
        "\n",
        "def construct_tree_from_loader(\n",
        "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
        ") -> XGBRegressor:\n",
        "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
        "    for dataset in dataset_loader:\n",
        "        data, label = dataset[0], dataset[1]\n",
        "    return construct_tree(data, label, n_estimators, tree_type)\n",
        "\n",
        "def single_tree_prediction(\n",
        "    tree: XGBRegressor, n_tree: int, dataset: NDArray\n",
        ") -> Optional[NDArray]:\n",
        "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
        "    ensemble.\"\"\"\n",
        "    num_t = len(tree.get_booster().get_dump())\n",
        "    if n_tree > num_t:\n",
        "        print(\n",
        "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    return tree.predict(\n",
        "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
        "    )\n",
        "\n",
        "def tree_encoding(\n",
        "    trainloader: DataLoader,\n",
        "    client_trees: Union[\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Tuple[XGBRegressor, int]],\n",
        "    ],\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> Optional[Tuple[NDArray, NDArray]]:\n",
        "    \"\"\"Transform the tabular dataset into prediction results using the\n",
        "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
        "    if trainloader is None:\n",
        "        return None\n",
        "\n",
        "    for local_dataset in trainloader:\n",
        "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
        "\n",
        "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
        "    x_train_enc = np.array(x_train_enc, copy=True)\n",
        "\n",
        "    temp_trees: Any = None\n",
        "    if isinstance(client_trees, list) is False:\n",
        "        temp_trees = [client_trees[0]] * client_num\n",
        "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
        "        temp_trees = [client_trees[0][0]] * client_num\n",
        "    else:\n",
        "        cids = []\n",
        "        temp_trees = []\n",
        "        for i, _ in enumerate(client_trees):\n",
        "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
        "            cids.append(client_trees[i][1])  # type: ignore\n",
        "        sorted_index = np.argsort(np.asarray(cids))\n",
        "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
        "\n",
        "    for i, _ in enumerate(temp_trees):\n",
        "        for j in range(client_tree_num):\n",
        "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
        "                temp_trees[i], j, x_train\n",
        "            )\n",
        "\n",
        "    x_train_enc32: Any = np.float32(x_train_enc)\n",
        "    y_train32: Any = np.float32(y_train)\n",
        "\n",
        "    x_train_enc32, y_train32 = torch.from_numpy(\n",
        "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
        "    ), torch.from_numpy(\n",
        "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
        "    )\n",
        "    return x_train_enc32, y_train32"
      ],
      "metadata": {
        "id": "rYBOwdF7GwR2"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_train = FL_train_set\n",
        "reg_test = FL_test_set\n",
        "task_type = \"REG\"\n",
        "\n",
        "# Select the downloaded training and test dataset\n",
        "if task_type == \"REG\":\n",
        "    train = reg_train\n",
        "    test = reg_test\n",
        "    data_train = reg_train\n",
        "    data_test = reg_test\n",
        "\n",
        "print(\"Task type selected is: \" + task_type)\n",
        "print(\"Training dataset is: \")\n",
        "print(train)\n",
        "print(\"Test dataset is: \")\n",
        "print(test)"
      ],
      "metadata": {
        "id": "x4tqBX7BHSXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4465e9f-299e-4d0b-80ce-a3b0cac97d79"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task type selected is: REG\n",
            "Training dataset is: \n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        3.140   1012.97      0.83       4.76 2013-01-12  MAC000002   \n",
            "1        3.200   1019.29      0.78       3.08 2013-02-08  MAC000002   \n",
            "2        8.050   1033.95      0.89       1.35 2013-11-27  MAC000002   \n",
            "3        6.260   1018.89      0.80       5.44 2012-12-09  MAC000002   \n",
            "4        5.900   1015.79      0.79       2.61 2013-11-10  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "394     11.210   1003.59      0.86       5.51 2012-12-23  MAC000246   \n",
            "395      8.580   1010.84      0.81       7.01 2011-12-08  MAC000246   \n",
            "396      7.755    997.84      0.91       2.38 2012-11-10  MAC000246   \n",
            "397      4.835   1029.69      0.80       4.25 2013-02-28  MAC000246   \n",
            "398      4.815   1023.19      0.83       2.14 2012-12-08  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        10.586  \n",
            "1         9.635  \n",
            "2        11.003  \n",
            "3        11.355  \n",
            "4        25.556  \n",
            "..          ...  \n",
            "394      24.395  \n",
            "395      13.664  \n",
            "396      24.064  \n",
            "397      24.677  \n",
            "398      15.284  \n",
            "\n",
            "[399 rows x 7 columns]\n",
            "Test dataset is: \n",
            "     mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0        4.435   1004.33      0.81       2.50 2014-01-14  MAC000002   \n",
            "1        5.730    992.43      0.68       5.83 2014-02-13  MAC000002   \n",
            "2        8.965    993.32      0.86       7.20 2014-01-01  MAC000002   \n",
            "3        8.175   1009.09      0.71       4.77 2014-02-22  MAC000002   \n",
            "4        6.840   1000.02      0.87       4.40 2014-01-05  MAC000002   \n",
            "..         ...       ...       ...        ...        ...        ...   \n",
            "130      5.335    997.47      0.77       3.17 2013-12-28  MAC000246   \n",
            "131      6.970    988.77      0.79       3.98 2014-02-07  MAC000246   \n",
            "132      7.545   1005.39      0.72       4.75 2014-02-02  MAC000246   \n",
            "133      0.895   1014.52      0.80       3.20 2013-01-25  MAC000246   \n",
            "134     10.955    994.61      0.78       8.30 2014-01-06  MAC000246   \n",
            "\n",
            "     energy_sum  \n",
            "0        18.532  \n",
            "1        11.723  \n",
            "2        15.496  \n",
            "3        12.734  \n",
            "4        14.418  \n",
            "..          ...  \n",
            "130      21.253  \n",
            "131       9.560  \n",
            "132      31.426  \n",
            "133      21.594  \n",
            "134      16.022  \n",
            "\n",
            "[135 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeDataset(Dataset):\n",
        "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
        "        self.labels = labels\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
        "        label = self.labels[idx]\n",
        "        data = self.data[idx, :]\n",
        "        sample = {0: data, 1: label}\n",
        "        return sample\n",
        "\n",
        "task_type = \"REG\"\n",
        "\n",
        "# Load and preprocess regression data\n",
        "X_train = FL_train_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "y_train = FL_train_set['energy_sum'].values\n",
        "\n",
        "X_test = FL_test_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "y_test = FL_test_set['energy_sum'].values\n",
        "\n",
        "X_train.flags.writeable = True\n",
        "y_train.flags.writeable = True\n",
        "\n",
        "X_test.flags.writeable = True\n",
        "y_test.flags.writeable = True\n",
        "\n",
        "print(\"Feature dimension of the dataset:\", X_train.shape[1])\n",
        "print(\"Size of the trainset:\", X_train.shape[0])\n",
        "print(\"Size of the testset:\", X_test.shape[0])\n",
        "assert X_train.shape[1] == X_test.shape[1]\n",
        "\n",
        "trainset = TreeDataset(np.array(X_train, copy=True), np.array(y_train, copy=True))\n",
        "testset = TreeDataset(np.array(X_test, copy=True), np.array(y_test, copy=True))\n",
        "print(trainset)\n",
        "print(testset)\n"
      ],
      "metadata": {
        "id": "gOilCVu1HVja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4880672c-f5db-42dd-dc8d-d75cc0ac42f6"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature dimension of the dataset: 4\n",
            "Size of the trainset: 399\n",
            "Size of the testset: 135\n",
            "<__main__.TreeDataset object at 0x783d1de0be50>\n",
            "<__main__.TreeDataset object at 0x783d1de0af80>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(\n",
        "    dataset: Dataset, partition: str, batch_size: Union[int, str]\n",
        ") -> DataLoader:\n",
        "    if batch_size == \"whole\":\n",
        "        batch_size = len(dataset)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, pin_memory=True, shuffle=(partition == \"train\")\n",
        "    )\n",
        "\n",
        "def do_fl_partitioning(\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    pool_size: int,\n",
        "    batch_size: Union[int, str],\n",
        "    val_ratio: float = 0.0,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
        "    partition_size = len(trainset) // pool_size\n",
        "    lengths = [partition_size] * pool_size\n",
        "    if sum(lengths) != len(trainset):\n",
        "        lengths[-1] = len(trainset) - sum(lengths[0:-1])\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(0))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = int(len(ds) * val_ratio)\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(0))\n",
        "        trainloaders.append(get_dataloader(ds_train, \"train\", batch_size))\n",
        "        if len_val != 0:\n",
        "            valloaders.append(get_dataloader(ds_val, \"val\", batch_size))\n",
        "        else:\n",
        "            valloaders = None\n",
        "    testloader = get_dataloader(testset, \"test\", batch_size)\n",
        "\n",
        "    # columns_to_extract = ['mean_temp', 'pressure', 'humidity', 'windSpeed', 'energy_sum']\n",
        "    # trainloaders = [get_dataloader(dataset, \"train\", batch_size) for dataset in FL_train_sets_list]\n",
        "\n",
        "    # trainloaders=FL_train_sets_list[['mean_temp', 'pressure', 'humidity', 'windSpeed','energy_sum']]\n",
        "    # valloaders=FL_val_sets_list[['mean_temp', 'pressure', 'humidity', 'windSpeed','energy_sum']]\n",
        "    # testloader=FL_test_sets_list[['mean_temp', 'pressure', 'humidity', 'windSpeed','energy_sum']]\n",
        "\n",
        "\n",
        "    return trainloaders, valloaders, testloader"
      ],
      "metadata": {
        "id": "Wgf71XTUH0l6"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of clients participated in the federated learning\n",
        "client_num = 2\n",
        "\n",
        "# The number of XGBoost trees in the tree ensemble that will be built for each client\n",
        "client_tree_num = 500 // client_num"
      ],
      "metadata": {
        "id": "108M9aycH49T"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(X_train)\n",
        "print(FL_train_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']])\n",
        "global_tree = construct_tree(X_train, y_train, client_tree_num, task_type)\n",
        "preds_train = global_tree.predict(X_train)\n",
        "preds_test = global_tree.predict(X_test)\n",
        "\n",
        "result_train = mean_squared_error(y_train, preds_train)\n",
        "result_test = mean_squared_error(y_test, preds_test)\n",
        "print(\"Global XGBoost Training MSE: %f\" % (result_train))\n",
        "print(\"Global XGBoost Testing MSE: %f\" % (result_test))\n",
        "# print(y_train)\n",
        "# print(y_test)\n",
        "# print(X_train)\n",
        "\n",
        "print(global_tree)\n"
      ],
      "metadata": {
        "id": "-HM8v1lYH8V7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15e81c7-4afd-4e39-f377-cdb10cc354b7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.14000e+00 1.01297e+03 8.30000e-01 4.76000e+00]\n",
            " [3.20000e+00 1.01929e+03 7.80000e-01 3.08000e+00]\n",
            " [8.05000e+00 1.03395e+03 8.90000e-01 1.35000e+00]\n",
            " ...\n",
            " [7.75500e+00 9.97840e+02 9.10000e-01 2.38000e+00]\n",
            " [4.83500e+00 1.02969e+03 8.00000e-01 4.25000e+00]\n",
            " [4.81500e+00 1.02319e+03 8.30000e-01 2.14000e+00]]\n",
            "     mean_temp  pressure  humidity  windSpeed\n",
            "0        3.140   1012.97      0.83       4.76\n",
            "1        3.200   1019.29      0.78       3.08\n",
            "2        8.050   1033.95      0.89       1.35\n",
            "3        6.260   1018.89      0.80       5.44\n",
            "4        5.900   1015.79      0.79       2.61\n",
            "..         ...       ...       ...        ...\n",
            "394     11.210   1003.59      0.86       5.51\n",
            "395      8.580   1010.84      0.81       7.01\n",
            "396      7.755    997.84      0.91       2.38\n",
            "397      4.835   1029.69      0.80       4.25\n",
            "398      4.815   1023.19      0.83       2.14\n",
            "\n",
            "[399 rows x 4 columns]\n",
            "Global XGBoost Training MSE: 25.624486\n",
            "Global XGBoost Testing MSE: 58.950050\n",
            "XGBRegressor(alpha=5, base_score=None, booster=None, callbacks=None,\n",
            "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
            "             early_stopping_rounds=None, enable_categorical=False,\n",
            "             eval_metric=None, feature_types=None, gamma=5, gpu_id=None,\n",
            "             grow_policy=None, importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
            "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
            "             min_child_weight=1, missing=nan, monotone_constraints=None,\n",
            "             n_estimators=250, n_jobs=None, num_parallel_tree=1, predictor=None, ...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client_trees_comparison = []\n",
        "trainloaders, _, testloader = do_fl_partitioning(\n",
        "    trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
        ")\n",
        "for i, trainloader in enumerate(trainloaders):\n",
        "    for local_dataset in trainloader:\n",
        "        local_X_train, local_y_train = local_dataset[0], local_dataset[1]\n",
        "        tree = construct_tree(local_X_train, local_y_train, client_tree_num, task_type)\n",
        "        client_trees_comparison.append(tree)\n",
        "\n",
        "        preds_train = client_trees_comparison[-1].predict(local_X_train)\n",
        "        preds_test = client_trees_comparison[-1].predict(X_test)\n",
        "\n",
        "        result_train = mean_squared_error(local_y_train, preds_train)\n",
        "        result_test = mean_squared_error(y_test, preds_test)\n",
        "        print(\"Local Client %d XGBoost Training MSE: %f\" % (i, result_train))\n",
        "        print(\"Local Client %d XGBoost Testing MSE: %f\" % (i, result_test))\n"
      ],
      "metadata": {
        "id": "6Hki4Uc_H-xZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e78159-b8c4-4d79-ac08-ea10e9163c3f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Client 0 XGBoost Training MSE: 11.130015\n",
            "Local Client 0 XGBoost Testing MSE: 57.808534\n",
            "Local Client 1 XGBoost Training MSE: 11.786690\n",
            "Local Client 1 XGBoost Testing MSE: 45.360228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_channel: int = 64) -> None:\n",
        "        super(CNN, self).__init__()\n",
        "        n_out = 1\n",
        "        self.task_type = task_type\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            1, n_channel, kernel_size=client_tree_num, stride=client_tree_num, padding=0\n",
        "        )\n",
        "        self.layer_direct = nn.Linear(n_channel * client_num, n_out)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()\n",
        "        self.Identity = nn.Identity()\n",
        "\n",
        "        # Add weight initialization\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ReLU(self.conv1d(x))\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.Identity(self.layer_direct(x))\n",
        "        return x\n",
        "\n",
        "    def get_weights(self) -> fl.common.NDArrays:\n",
        "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
        "        return [\n",
        "            np.array(val.cpu().numpy(), copy=True)\n",
        "            for _, val in self.state_dict().items()\n",
        "        ]\n",
        "\n",
        "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
        "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "        layer_dict = {}\n",
        "        for k, v in zip(self.state_dict().keys(), weights):\n",
        "            if v.ndim != 0:\n",
        "                layer_dict[k] = torch.Tensor(np.array(v, copy=True))\n",
        "        state_dict = OrderedDict(layer_dict)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(\n",
        "    task_type: str,\n",
        "    net: CNN,\n",
        "    trainloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_iterations: int,\n",
        "    log_progress: bool = True,\n",
        ") -> Tuple[float, float, int]:\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    # optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-6)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
        "\n",
        "    def cycle(iterable):\n",
        "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
        "        while True:\n",
        "            for x in iterable:\n",
        "                yield x\n",
        "\n",
        "    # Train the network\n",
        "    net.train()\n",
        "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
        "    pbar = (\n",
        "        tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f\"TRAIN\")\n",
        "        if log_progress\n",
        "        else iter(cycle(trainloader))\n",
        "    )\n",
        "\n",
        "    # Unusually, this training is formulated in terms of number of updates/iterations/batches processed\n",
        "    # by the network. This will be helpful later on, when partitioning the data across clients: resulting\n",
        "    # in differences between dataset sizes and hence inconsistent numbers of updates per 'epoch'.\n",
        "    for i, data in zip(range(num_iterations), pbar):\n",
        "        tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(tree_outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collected training loss and accuracy statistics\n",
        "        total_loss += loss.item()\n",
        "        n_samples += labels.size(0)\n",
        "\n",
        "        if task_type == \"REG\":\n",
        "            mse = MeanSquaredError()(outputs, labels.type(torch.int))\n",
        "            total_result += mse * labels.size(0)\n",
        "\n",
        "        if log_progress:\n",
        "             if task_type == \"REG\":\n",
        "                pbar.set_postfix(\n",
        "                    {\n",
        "                        \"train_loss\": total_loss / n_samples,\n",
        "                        \"train_mse\": total_result / n_samples,\n",
        "                    }\n",
        "                )\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return total_loss / n_samples, total_result / n_samples, n_samples\n",
        "\n",
        "\n",
        "def test(\n",
        "    task_type: str,\n",
        "    net: CNN,\n",
        "    testloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    log_progress: bool = True,\n",
        ") -> Tuple[float, float, int]:\n",
        "    \"\"\"Evaluates the network on test data.\"\"\"\n",
        "    if task_type == \"REG\":\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
        "        for data in pbar:\n",
        "            tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(tree_outputs)\n",
        "\n",
        "            # Collected testing loss and accuracy statistics\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            n_samples += labels.size(0)\n",
        "\n",
        "            if task_type == \"REG\":\n",
        "                mse = MeanSquaredError()(outputs.cpu(), labels.type(torch.int).cpu())\n",
        "                total_result += mse * labels.size(0)\n",
        "\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return total_loss / n_samples, total_result / n_samples, n_samples"
      ],
      "metadata": {
        "id": "ZxnV5c3IIBUQ"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower client\n",
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "    FitRes,\n",
        "    GetPropertiesIns,\n",
        "    GetPropertiesRes,\n",
        "    GetParametersIns,\n",
        "    GetParametersRes,\n",
        "    Status,\n",
        "    Code,\n",
        "    parameters_to_ndarrays,\n",
        "    ndarrays_to_parameters,\n",
        ")"
      ],
      "metadata": {
        "id": "siMTvAMaIFaJ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_encoding_loader(\n",
        "    dataloader: DataLoader,\n",
        "    batch_size: int,\n",
        "    client_trees: Union[\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Tuple[XGBRegressor, int]],\n",
        "    ],\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> DataLoader:\n",
        "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
        "    if encoding is None:\n",
        "        return None\n",
        "    data, labels = encoding\n",
        "    tree_dataset = TreeDataset(data, labels)\n",
        "    return get_dataloader(tree_dataset, \"tree\", batch_size)\n",
        "\n",
        "class FL_Client(fl.client.Client):\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_type: str,\n",
        "        trainloader: DataLoader,\n",
        "        valloader: DataLoader,\n",
        "        client_tree_num: int,\n",
        "        client_num: int,\n",
        "        cid: str,\n",
        "        log_progress: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a client for training `network.Net` on tabular dataset.\n",
        "        \"\"\"\n",
        "        self.task_type = task_type\n",
        "        self.cid = cid\n",
        "        self.tree = construct_tree_from_loader(trainloader, client_tree_num, task_type)\n",
        "        self.trainloader_original = trainloader\n",
        "        self.valloader_original = valloader\n",
        "        self.trainloader = None\n",
        "        self.valloader = None\n",
        "        self.client_tree_num = client_tree_num\n",
        "        self.client_num = client_num\n",
        "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "        self.log_progress = log_progress\n",
        "\n",
        "        # instantiate model\n",
        "        self.net = CNN()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_properties(self, ins: GetPropertiesIns) -> GetPropertiesRes:\n",
        "        return GetPropertiesRes(properties=self.properties)\n",
        "\n",
        "    def get_parameters(\n",
        "        self, ins: GetParametersIns\n",
        "    ) -> Tuple[\n",
        "        GetParametersRes,  Tuple[XGBRegressor, int]\n",
        "    ]:\n",
        "        return [\n",
        "            GetParametersRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=ndarrays_to_parameters(self.net.get_weights()),\n",
        "            ),\n",
        "            (self.tree, int(self.cid)),\n",
        "        ]\n",
        "\n",
        "    def set_parameters(\n",
        "        self,\n",
        "        parameters: Tuple[\n",
        "            Parameters,\n",
        "            Union[\n",
        "                Tuple[XGBRegressor, int],\n",
        "                List[Tuple[XGBRegressor, int]],\n",
        "            ],\n",
        "        ],\n",
        "    ) ->  Union[\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Tuple[XGBRegressor, int]],\n",
        "    ]:\n",
        "        self.net.set_weights(parameters_to_ndarrays(parameters[0]))\n",
        "        return parameters[1]\n",
        "\n",
        "    def fit(self, fit_params: FitIns) -> FitRes:\n",
        "        # Process incoming request to train\n",
        "        num_iterations = fit_params.config[\"num_iterations\"]\n",
        "        batch_size = fit_params.config[\"batch_size\"]\n",
        "        aggregated_trees = self.set_parameters(fit_params.parameters)\n",
        "\n",
        "        if type(aggregated_trees) is list:\n",
        "            print(\"Client \" + self.cid + \": received\", len(aggregated_trees), \"trees\")\n",
        "        else:\n",
        "            print(\"Client \" + self.cid + \": only had its own tree\")\n",
        "        self.trainloader = tree_encoding_loader(\n",
        "            self.trainloader_original,\n",
        "            batch_size,\n",
        "            aggregated_trees,\n",
        "            self.client_tree_num,\n",
        "            self.client_num,\n",
        "        )\n",
        "        self.valloader = tree_encoding_loader(\n",
        "            self.valloader_original,\n",
        "            batch_size,\n",
        "            aggregated_trees,\n",
        "            self.client_tree_num,\n",
        "            self.client_num,\n",
        "        )\n",
        "\n",
        "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
        "        num_iterations = num_iterations or len(self.trainloader)\n",
        "\n",
        "        # Train the model\n",
        "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
        "        self.net.to(self.device)\n",
        "        train_loss, train_result, num_examples = train(\n",
        "            self.task_type,\n",
        "            self.net,\n",
        "            self.trainloader,\n",
        "            device=self.device,\n",
        "            num_iterations=num_iterations,\n",
        "            log_progress=self.log_progress,\n",
        "        )\n",
        "        print(\n",
        "            f\"Client {self.cid}: training round complete, {num_examples} examples processed\"\n",
        "        )\n",
        "\n",
        "        # Return training information: model, number of examples processed and metrics\n",
        "        if self.task_type == \"REG\":\n",
        "            return FitRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=self.get_parameters(fit_params.config),\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"loss\": train_loss, \"mse\": train_result},\n",
        "            )\n",
        "\n",
        "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
        "        # Process incoming request to evaluate\n",
        "        self.set_parameters(eval_params.parameters)\n",
        "\n",
        "        # Evaluate the model\n",
        "        self.net.to(self.device)\n",
        "        loss, result, num_examples = test(\n",
        "            self.task_type,\n",
        "            self.net,\n",
        "            self.valloader,\n",
        "            device=self.device,\n",
        "            log_progress=self.log_progress,\n",
        "        )\n",
        "\n",
        "        # Return evaluation information\n",
        "        if self.task_type == \"REG\":\n",
        "            print(\n",
        "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, mse={result:.4f}\"\n",
        "            )\n",
        "            return EvaluateRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                loss=loss,\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"mse\": result},\n",
        "            )\n"
      ],
      "metadata": {
        "id": "JDFHpSFmIpDk"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower server\n",
        "import functools\n",
        "from flwr.server.strategy import FedXgbNnAvg\n",
        "from flwr.server.app import ServerConfig\n",
        "\n",
        "import timeit\n",
        "from logging import DEBUG, INFO\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from flwr.common import DisconnectRes, Parameters, ReconnectIns, Scalar\n",
        "from flwr.common.logger import log\n",
        "from flwr.common.typing import GetParametersIns\n",
        "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.history import History\n",
        "from flwr.server.strategy import Strategy\n",
        "from flwr.server.server import (\n",
        "    reconnect_clients,\n",
        "    reconnect_client,\n",
        "    fit_clients,\n",
        "    fit_client,\n",
        "    _handle_finished_future_after_fit,\n",
        "    evaluate_clients,\n",
        "    evaluate_client,\n",
        "    _handle_finished_future_after_evaluate,\n",
        ")\n",
        "\n",
        "FitResultsAndFailures = Tuple[\n",
        "    List[Tuple[ClientProxy, FitRes]],\n",
        "    List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "]\n",
        "EvaluateResultsAndFailures = Tuple[\n",
        "    List[Tuple[ClientProxy, EvaluateRes]],\n",
        "    List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
        "]"
      ],
      "metadata": {
        "id": "NS0N-9TeIrCl"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FL_Server(fl.server.Server):\n",
        "    \"\"\"Flower server.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, *, client_manager: ClientManager, strategy: Optional[Strategy] = None\n",
        "    ) -> None:\n",
        "        self._client_manager: ClientManager = client_manager\n",
        "        self.parameters: Parameters = Parameters(\n",
        "            tensors=[], tensor_type=\"numpy.ndarray\"\n",
        "        )\n",
        "        self.strategy: Strategy = strategy\n",
        "        self.max_workers: Optional[int] = None\n",
        "\n",
        "\n",
        "    def fit(self, num_rounds: int, timeout: Optional[float]) -> History:\n",
        "        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n",
        "        history = History()\n",
        "\n",
        "        # Initialize parameters\n",
        "        log(INFO, \"Initializing global parameters\")\n",
        "        self.parameters = self._get_initial_parameters(timeout=timeout)\n",
        "\n",
        "        log(INFO, \"Evaluating initial parameters\")\n",
        "        res = self.strategy.evaluate(0, parameters=self.parameters)\n",
        "        if res is not None:\n",
        "            log(\n",
        "                INFO,\n",
        "                \"initial parameters (loss, other metrics): %s, %s\",\n",
        "                res,\n",
        "            )\n",
        "            history.add_loss_centralized(server_round=0, loss=res[0])\n",
        "            history.add_metrics_centralized(server_round=0, metrics=res[1])\n",
        "\n",
        "        # Run federated learning for num_rounds\n",
        "        log(INFO, \"FL starting\")\n",
        "        start_time = timeit.default_timer()\n",
        "\n",
        "        for current_round in range(1, num_rounds + 1):\n",
        "            # Train model and replace previous global model\n",
        "            res_fit = self.fit_round(server_round=current_round, timeout=timeout)\n",
        "            if res_fit:\n",
        "                parameters_prime, _, _ = res_fit  # fit_metrics_aggregated\n",
        "                if parameters_prime:\n",
        "                    self.parameters = parameters_prime\n",
        "\n",
        "            # Evaluate model using strategy implementation\n",
        "            res_cen = self.strategy.evaluate(current_round, parameters=self.parameters)\n",
        "            if res_cen is not None:\n",
        "                loss_cen, metrics_cen = res_cen\n",
        "                log(\n",
        "                    INFO,\n",
        "                    \"fit progress: (%s, %s, %s, %s)\",\n",
        "                    current_round,\n",
        "                    loss_cen,\n",
        "                    metrics_cen,\n",
        "                    timeit.default_timer() - start_time,\n",
        "                )\n",
        "                history.add_loss_centralized(server_round=current_round, loss=loss_cen)\n",
        "                history.add_metrics_centralized(\n",
        "                    server_round=current_round, metrics=metrics_cen\n",
        "                )\n",
        "\n",
        "            # Evaluate model on a sample of available clients\n",
        "            res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
        "            if res_fed:\n",
        "                loss_fed, evaluate_metrics_fed, _ = res_fed\n",
        "                if loss_fed:\n",
        "                    history.add_loss_distributed(\n",
        "                        server_round=current_round, loss=loss_fed\n",
        "                    )\n",
        "                    history.add_metrics_distributed(\n",
        "                        server_round=current_round, metrics=evaluate_metrics_fed\n",
        "                    )\n",
        "\n",
        "        # Bookkeeping\n",
        "        end_time = timeit.default_timer()\n",
        "        elapsed = end_time - start_time\n",
        "        log(INFO, \"FL finished in %s\", elapsed)\n",
        "        return history\n",
        "\n",
        "    def evaluate_round(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        timeout: Optional[float],\n",
        "    ) -> Optional[\n",
        "        Tuple[Optional[float], Dict[str, Scalar], EvaluateResultsAndFailures]\n",
        "    ]:\n",
        "        \"\"\"Validate current global model on a number of clients.\"\"\"\n",
        "\n",
        "        # Get clients and their respective instructions from strategy\n",
        "        client_instructions = self.strategy.configure_evaluate(\n",
        "            server_round=server_round,\n",
        "            parameters=self.parameters,\n",
        "            client_manager=self._client_manager,\n",
        "        )\n",
        "        if not client_instructions:\n",
        "            log(INFO, \"evaluate_round %s: no clients selected, cancel\", server_round)\n",
        "            return None\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"evaluate_round %s: strategy sampled %s clients (out of %s)\",\n",
        "            server_round,\n",
        "            len(client_instructions),\n",
        "            self._client_manager.num_available(),\n",
        "        )\n",
        "\n",
        "        # Collect `evaluate` results from all clients participating in this round\n",
        "        results, failures = evaluate_clients(\n",
        "            client_instructions,\n",
        "            max_workers=self.max_workers,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"evaluate_round %s received %s results and %s failures\",\n",
        "            server_round,\n",
        "            len(results),\n",
        "            len(failures),\n",
        "        )\n",
        "\n",
        "        # Aggregate the evaluation results\n",
        "        aggregated_result: Tuple[\n",
        "            Optional[float],\n",
        "            Dict[str, Scalar],\n",
        "        ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
        "\n",
        "        loss_aggregated, metrics_aggregated = aggregated_result\n",
        "        return loss_aggregated, metrics_aggregated, (results, failures)\n",
        "\n",
        "    def fit_round(\n",
        "          self,\n",
        "          server_round: int,\n",
        "          timeout: Optional[float],\n",
        "      ) -> Optional[\n",
        "          Tuple[\n",
        "              Optional[\n",
        "                  Tuple[\n",
        "                      Parameters,\n",
        "                      Union[\n",
        "                          Tuple[XGBRegressor, int],\n",
        "                          List[\n",
        "                              Tuple[XGBRegressor, int]\n",
        "                          ],\n",
        "                      ],\n",
        "                  ]\n",
        "              ],\n",
        "              Dict[str, Scalar],\n",
        "              FitResultsAndFailures,\n",
        "          ]\n",
        "      ]:\n",
        "        \"\"\"Perform a single round of federated averaging.\"\"\"\n",
        "\n",
        "        # Get clients and their respective instructions from strategy\n",
        "        client_instructions = self.strategy.configure_fit(\n",
        "            server_round=server_round,\n",
        "            parameters=self.parameters,\n",
        "            client_manager=self._client_manager,\n",
        "        )\n",
        "\n",
        "        if not client_instructions:\n",
        "            log(INFO, \"fit_round %s: no clients selected, cancel\", server_round)\n",
        "            return None\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"fit_round %s: strategy sampled %s clients (out of %s)\",\n",
        "            server_round,\n",
        "            len(client_instructions),\n",
        "            self._client_manager.num_available(),\n",
        "        )\n",
        "\n",
        "        # Collect `fit` results from all clients participating in this round\n",
        "        results, failures = fit_clients(\n",
        "            client_instructions=client_instructions,\n",
        "            max_workers=self.max_workers,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"fit_round %s received %s results and %s failures\",\n",
        "            server_round,\n",
        "            len(results),\n",
        "            len(failures),\n",
        "        )\n",
        "\n",
        "        # Aggregate training results\n",
        "        NN_aggregated: Parameters\n",
        "        trees_aggregated: Union[\n",
        "            Tuple[XGBRegressor, int],\n",
        "            List[ Tuple[XGBRegressor, int]],\n",
        "        ]\n",
        "        metrics_aggregated: Dict[str, Scalar]\n",
        "        aggregated, metrics_aggregated = self.strategy.aggregate_fit(\n",
        "            server_round, results, failures\n",
        "        )\n",
        "        NN_aggregated, trees_aggregated = aggregated[0], aggregated[1]\n",
        "\n",
        "        if type(trees_aggregated) is list:\n",
        "            print(\"Server side aggregated\", len(trees_aggregated), \"trees.\")\n",
        "        else:\n",
        "            print(\"Server side did not aggregate trees.\")\n",
        "\n",
        "        return (\n",
        "            [NN_aggregated, trees_aggregated],\n",
        "            metrics_aggregated,\n",
        "            (results, failures),\n",
        "        )\n",
        "\n",
        "    def _get_initial_parameters(\n",
        "        self, timeout: Optional[float]\n",
        "      ) -> Tuple[Parameters, Tuple[XGBRegressor, int]]:\n",
        "        \"\"\"Get initial parameters from one of the available clients.\"\"\"\n",
        "\n",
        "        # Server-side parameter initialization\n",
        "        parameters: Optional[Parameters] = self.strategy.initialize_parameters(\n",
        "            client_manager=self._client_manager\n",
        "        )\n",
        "        if parameters is not None:\n",
        "            log(INFO, \"Using initial parameters provided by strategy\")\n",
        "            return parameters\n",
        "\n",
        "        # Get initial parameters from one of the clients\n",
        "        log(INFO, \"Requesting initial parameters from one random client\")\n",
        "        random_client = self._client_manager.sample(1)[0]\n",
        "        ins = GetParametersIns(config={})\n",
        "        get_parameters_res_tree = random_client.get_parameters(ins=ins, timeout=timeout)\n",
        "        parameters = [get_parameters_res_tree[0].parameters, get_parameters_res_tree[1]]\n",
        "        log(INFO, \"Received initial parameters from one random client\")\n",
        "\n",
        "        return parameters\n"
      ],
      "metadata": {
        "id": "pbGqqV8cIuMi"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_layers(model: nn.Module) -> None:\n",
        "    print(model)\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "\n",
        "def serverside_eval(\n",
        "    server_round: int,\n",
        "    parameters: Tuple[\n",
        "        Parameters,\n",
        "        Union[\n",
        "            Tuple[XGBRegressor, int],\n",
        "            List[Tuple[XGBRegressor, int]],\n",
        "        ],\n",
        "    ],\n",
        "    config: Dict[str, Scalar],\n",
        "    task_type: str,\n",
        "    testloader: DataLoader,\n",
        "    batch_size: int,\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"An evaluation function for centralized/serverside evaluation over the entire test set.\"\"\"\n",
        "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = \"cpu\"\n",
        "    model = CNN()\n",
        "    # print_model_layers(model)\n",
        "\n",
        "    model.set_weights(parameters_to_ndarrays(parameters[0]))\n",
        "    model.to(device)\n",
        "\n",
        "    trees_aggregated = parameters[1]\n",
        "    testloader = tree_encoding_loader(\n",
        "        testloader, batch_size, trees_aggregated, client_tree_num, client_num\n",
        "    )\n",
        "    loss, result, _ = test(\n",
        "        task_type, model, testloader, device=device, log_progress=False\n",
        "    )\n",
        "\n",
        "    if task_type == \"REG\":\n",
        "        print(f\"Evaluation on the server: test_loss={loss:.4f}, test_mse={result:.4f}\")\n",
        "        return loss, {\"mse\": result}\n",
        "\n",
        "\n",
        "def start_experiment(\n",
        "    task_type: str,\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    num_rounds: int = 5,\n",
        "    client_tree_num: int = 50,\n",
        "    client_pool_size: int = 5,\n",
        "    num_iterations: int = 100,\n",
        "    fraction_fit: float = 1.0,\n",
        "    min_fit_clients: int = 2,\n",
        "    batch_size: int = 32,\n",
        "    val_ratio: float = 0.1,\n",
        ") -> History:\n",
        "    client_resources = {\"num_cpus\": 0.5}  # 2 clients per CPU\n",
        "\n",
        "    # Partition the dataset into subsets reserved for each client.\n",
        "    # - 'val_ratio' controls the proportion of the (local) client reserved as a local test set\n",
        "    # (good for testing how the final model performs on the client's local unseen data)\n",
        "    trainloaders, valloaders, testloader = do_fl_partitioning(\n",
        "        trainset,\n",
        "        testset,\n",
        "        batch_size=\"whole\",\n",
        "        pool_size=client_pool_size,\n",
        "        val_ratio=val_ratio,\n",
        "    )\n",
        "    print(\n",
        "        f\"Data partitioned across {client_pool_size} clients\"\n",
        "        f\" and {val_ratio} of local dataset reserved for validation.\"\n",
        "    )\n",
        "\n",
        "    # Configure the strategy\n",
        "    def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
        "        print(f\"Configuring round {server_round}\")\n",
        "        return {\n",
        "            \"num_iterations\": num_iterations,\n",
        "            \"batch_size\": batch_size,\n",
        "        }\n",
        "\n",
        "    # FedXgbNnAvg\n",
        "    strategy = FedXgbNnAvg(\n",
        "        fraction_fit=fraction_fit,\n",
        "        fraction_evaluate=fraction_fit if val_ratio > 0.0 else 0.0,\n",
        "        min_fit_clients=min_fit_clients,\n",
        "        min_evaluate_clients=min_fit_clients,\n",
        "        min_available_clients=client_pool_size,  # all clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        on_evaluate_config_fn=(lambda r: {\"batch_size\": batch_size}),\n",
        "        evaluate_fn=functools.partial(\n",
        "            serverside_eval,\n",
        "            task_type=task_type,\n",
        "            testloader=testloader,\n",
        "            batch_size=batch_size,\n",
        "            client_tree_num=client_tree_num,\n",
        "            client_num=client_pool_size,\n",
        "        ),\n",
        "        accept_failures=False,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"FL experiment configured for {num_rounds} rounds with {client_pool_size} clients in the pool.\"\n",
        "    )\n",
        "    print(\n",
        "        f\"FL round will proceed with {fraction_fit * 100}% of clients sampled, at least {min_fit_clients}.\"\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str) -> fl.client.Client:\n",
        "        \"\"\"Creates a federated learning client\"\"\"\n",
        "        if val_ratio > 0.0 and val_ratio <= 1.0:\n",
        "            return FL_Client(\n",
        "                task_type,\n",
        "                trainloaders[int(cid)],\n",
        "                valloaders[int(cid)],\n",
        "                client_tree_num,\n",
        "                client_pool_size,\n",
        "                cid,\n",
        "                log_progress=False,\n",
        "            )\n",
        "        else:\n",
        "            return FL_Client(\n",
        "                task_type,\n",
        "                trainloaders[int(cid)],\n",
        "                None,\n",
        "                client_tree_num,\n",
        "                client_pool_size,\n",
        "                cid,\n",
        "                log_progress=False,\n",
        "            )\n",
        "\n",
        "    # Start the simulation\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        server=FL_Server(client_manager=SimpleClientManager(), strategy=strategy),\n",
        "        num_clients=client_pool_size,\n",
        "        client_resources=client_resources,\n",
        "        config=ServerConfig(num_rounds=num_rounds),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "    # print(history)\n",
        "\n",
        "    return history\n",
        "\n",
        "    # # Extract evaluation results from history\n",
        "    # fl_eval_results = []\n",
        "    # for r in range(1, num_rounds + 1):\n",
        "    #     eval_results = history.metrics_centralized(r)\n",
        "    #     fl_eval_results.append(eval_results)\n",
        "\n",
        "    # return fl_eval_results\n"
      ],
      "metadata": {
        "id": "xJPjGwB0IyIq"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FL_eval_results=[]\n",
        "# FL_eval_results=\n",
        "start_experiment(\n",
        "    task_type=task_type,\n",
        "    trainset=trainset,\n",
        "    testset=testset,\n",
        "    num_rounds=2,\n",
        "    client_tree_num=client_tree_num,\n",
        "    client_pool_size=client_num,\n",
        "    num_iterations=100,\n",
        "    batch_size=64,\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=1,\n",
        "    val_ratio=0.0,\n",
        ")"
      ],
      "metadata": {
        "id": "mfHdv8WBI2Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869ffe01-a3dd-42c1-fc9d-ca28757caaf0"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING flwr 2023-08-20 15:16:37,446 | app.py:210 | Both server and strategy were provided, ignoring strategy\n",
            "WARNING:flwr:Both server and strategy were provided, ignoring strategy\n",
            "INFO flwr 2023-08-20 15:16:37,461 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data partitioned across 2 clients and 0.0 of local dataset reserved for validation.\n",
            "FL experiment configured for 2 rounds with 2 clients in the pool.\n",
            "FL round will proceed with 100.0% of clients sampled, at least 1.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41195)\u001b[0m Client 1: received 2 trees\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-20 15:16:44,281\tINFO worker.py:1636 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-20 15:16:45,880 | app.py:179 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3869373235.0, 'memory': 7738746471.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3869373235.0, 'memory': 7738746471.0}\n",
            "INFO flwr 2023-08-20 15:16:45,890 | <ipython-input-127-de93d89d8e39>:20 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-20 15:16:45,897 | <ipython-input-127-de93d89d8e39>:223 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=41889)\u001b[0m 2023-08-20 15:16:47.343681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=41889)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:171: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=41889)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
            "INFO flwr 2023-08-20 15:16:53,760 | <ipython-input-127-de93d89d8e39>:228 | Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "INFO flwr 2023-08-20 15:16:53,766 | <ipython-input-127-de93d89d8e39>:23 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not enough arguments for format string\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-132-904c4d402dde>\", line 3, in <cell line: 3>\n",
            "    start_experiment(\n",
            "  File \"<ipython-input-130-8ec04e7fd87c>\", line 134, in start_experiment\n",
            "    history = fl.simulation.start_simulation(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\", line 198, in start_simulation\n",
            "    hist = run_fl(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/app.py\", line 224, in run_fl\n",
            "    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)\n",
            "  File \"<ipython-input-127-de93d89d8e39>\", line 26, in fit\n",
            "    log(\n",
            "Message: 'initial parameters (loss, other metrics): %s, %s'\n",
            "Arguments: ((9.174614574291088, {'mse': tensor(355.6506)}),)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not enough arguments for format string\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-132-904c4d402dde>\", line 3, in <cell line: 3>\n",
            "    start_experiment(\n",
            "  File \"<ipython-input-130-8ec04e7fd87c>\", line 134, in start_experiment\n",
            "    history = fl.simulation.start_simulation(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\", line 198, in start_simulation\n",
            "    hist = run_fl(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/app.py\", line 224, in run_fl\n",
            "    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)\n",
            "  File \"<ipython-input-127-de93d89d8e39>\", line 26, in fit\n",
            "    log(\n",
            "Message: 'initial parameters (loss, other metrics): %s, %s'\n",
            "Arguments: ((9.174614574291088, {'mse': tensor(355.6506)}),)\n",
            "INFO flwr 2023-08-20 15:17:04,759 | <ipython-input-127-de93d89d8e39>:35 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-08-20 15:17:04,767 | <ipython-input-127-de93d89d8e39>:163 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on the server: test_loss=9.1746, test_mse=355.6506\n",
            "Configuring round 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41889)\u001b[0m Client 0: only had its own tree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=41890)\u001b[0m 2023-08-20 15:17:07.887151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41890)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:171: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41890)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=41890)\u001b[0m Client 1: only had its own tree\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41889)\u001b[0m Client 0: training for 100 iterations/updates\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41889)\u001b[0m Client 0: training round complete, 4975 examples processed\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41890)\u001b[0m Client 1: training for 100 iterations/updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2023-08-20 15:17:37,221 | <ipython-input-127-de93d89d8e39>:178 | fit_round 1 received 2 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 2 results and 0 failures\n",
            "WARNING flwr 2023-08-20 15:17:37,231 | fedxgb_nn_avg.py:88 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server side aggregated 2 trees.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41890)\u001b[0m Client 1: training round complete, 5000 examples processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2023-08-20 15:17:50,153 | <ipython-input-127-de93d89d8e39>:50 | fit progress: (1, 4.927996034975405, {'mse': tensor(180.7182)}, 45.38615491799919)\n",
            "INFO:flwr:fit progress: (1, 4.927996034975405, {'mse': tensor(180.7182)}, 45.38615491799919)\n",
            "INFO flwr 2023-08-20 15:17:50,158 | <ipython-input-127-de93d89d8e39>:97 | evaluate_round 1: no clients selected, cancel\n",
            "INFO:flwr:evaluate_round 1: no clients selected, cancel\n",
            "DEBUG flwr 2023-08-20 15:17:50,163 | <ipython-input-127-de93d89d8e39>:163 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on the server: test_loss=4.9280, test_mse=180.7182\n",
            "Configuring round 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41889)\u001b[0m Client 1: received 2 trees\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41889)\u001b[0m Client 1: training for 100 iterations/updates\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=41890)\u001b[0m Client 1: training for 100 iterations/updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2023-08-20 15:18:16,159 | <ipython-input-127-de93d89d8e39>:178 | fit_round 2 received 2 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 2 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=41889)\u001b[0m Client 1: training round complete, 5000 examples processed\n",
            "Server side aggregated 2 trees.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2023-08-20 15:18:29,792 | <ipython-input-127-de93d89d8e39>:50 | fit progress: (2, 1.9719198721426505, {'mse': tensor(65.4659)}, 85.02550243400037)\n",
            "INFO:flwr:fit progress: (2, 1.9719198721426505, {'mse': tensor(65.4659)}, 85.02550243400037)\n",
            "INFO flwr 2023-08-20 15:18:29,798 | <ipython-input-127-de93d89d8e39>:97 | evaluate_round 2: no clients selected, cancel\n",
            "INFO:flwr:evaluate_round 2: no clients selected, cancel\n",
            "INFO flwr 2023-08-20 15:18:29,803 | <ipython-input-127-de93d89d8e39>:78 | FL finished in 85.03677250200053\n",
            "INFO:flwr:FL finished in 85.03677250200053\n",
            "INFO flwr 2023-08-20 15:18:29,807 | app.py:225 | app_fit: losses_distributed []\n",
            "INFO:flwr:app_fit: losses_distributed []\n",
            "INFO flwr 2023-08-20 15:18:29,811 | app.py:226 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-08-20 15:18:29,813 | app.py:227 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-08-20 15:18:29,817 | app.py:228 | app_fit: losses_centralized [(0, 9.174614574291088), (1, 4.927996034975405), (2, 1.9719198721426505)]\n",
            "INFO:flwr:app_fit: losses_centralized [(0, 9.174614574291088), (1, 4.927996034975405), (2, 1.9719198721426505)]\n",
            "INFO flwr 2023-08-20 15:18:29,822 | app.py:229 | app_fit: metrics_centralized {'mse': [(0, tensor(355.6506)), (1, tensor(180.7182)), (2, tensor(65.4659))]}\n",
            "INFO:flwr:app_fit: metrics_centralized {'mse': [(0, tensor(355.6506)), (1, tensor(180.7182)), (2, tensor(65.4659))]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on the server: test_loss=1.9719, test_mse=65.4659\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "History (loss, centralized):\n",
              "\tround 0: 9.174614574291088\n",
              "\tround 1: 4.927996034975405\n",
              "\tround 2: 1.9719198721426505\n",
              "History (metrics, centralized):\n",
              "{'mse': [(0, tensor(355.6506)), (1, tensor(180.7182)), (2, tensor(65.4659))]}"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing evaluation results"
      ],
      "metadata": {
        "id": "n_Ixyxh1I3L6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNLXgIZgKJLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert eval_results to a numpy array for plotting\n",
        "eval_results = np.array(eval_results)\n",
        "fl_eval_results = np.array(fl_eval_results)\n",
        "\n",
        "# Plot the evaluation metrics for each model and each round\n",
        "plt.figure(figsize=(15, 10))\n",
        "x = np.arange(len(models))\n",
        "metrics = ['MSE', 'RMSE', 'MAE', 'R-squared']\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "\n",
        "    # Plot central model results\n",
        "    plt.bar(x - 0.2, eval_results[:, i + 1].astype(float), width=0.4, label='Central Model')\n",
        "\n",
        "    # Plot FL model results\n",
        "    plt.bar(x + 0.2, fl_eval_results[:, i + 1].astype(float), width=0.4, label='FL Model')\n",
        "\n",
        "    plt.title(metric)\n",
        "    plt.xticks(x, eval_results[:, 0], rotation=15)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VsHauYLlJFrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Personal Experiments"
      ],
      "metadata": {
        "id": "AcAwekLwJOSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBKw06SHQohf"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from types import SimpleNamespace\n",
        "\n",
        "# # Define the XGBoost model creation function\n",
        "# def define_xgboost_model(hyperparameters):\n",
        "#     return xgb.XGBRegressor(**hyperparameters)\n",
        "\n",
        "\n",
        "# # Helper function to update the local model parameters\n",
        "# def set_parameters(model, parameters):\n",
        "#     model.set_params(**parameters)\n",
        "\n",
        "# # Helper function to get the local model parameters\n",
        "# def get_parameters(model):\n",
        "#     return model.get_xgb_params()\n",
        "\n",
        "# # Flower Client class\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, xgb_model, train_data, val_data):\n",
        "#         self.xgb_model = xgb_model\n",
        "#         self.train_data = train_data\n",
        "#         self.val_data = val_data\n",
        "\n",
        "#     def get_parameters(self, config):\n",
        "#         return self.xgb_model.get_booster().save_raw()\n",
        "\n",
        "#     def set_parameters(self, parameters):\n",
        "#         booster = self.xgb_model.get_booster()\n",
        "#         booster.load_model(parameters)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Train the XGBoost model on the client's data\n",
        "#         features = self.train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.train_data['energy_sum'].values\n",
        "#         self.xgb_model.fit(features, target)\n",
        "\n",
        "#         # Return the updated model parameters\n",
        "#         return self.get_parameters(config)  # Pass config argument here\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Evaluate the XGBoost model on the client's validation data\n",
        "#         features = self.val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.val_data['energy_sum'].values\n",
        "#         predictions = self.xgb_model.predict(features)\n",
        "#         loss = mean_squared_error(target, predictions)\n",
        "\n",
        "#         return loss, len(self.val_data), {}\n",
        "\n",
        "# # Create an instance of XGBoost model\n",
        "# hyperparameters = {'objective': 'reg:squarederror', 'learning_rate': 0.1, 'max_depth': 3}\n",
        "# xgboost_model = define_xgboost_model(hyperparameters)\n",
        "\n",
        "# # Fit the xgboost_model using training data\n",
        "# xgboost_model.fit(\n",
        "#     FL_train_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']],\n",
        "#     FL_train_set['energy_sum']\n",
        "# )\n",
        "\n",
        "# # Create FlowerClient instances\n",
        "# NUM_CLIENTS = len(FL_train_sets_list)\n",
        "# clients = [client_fn(cid) for cid in range(NUM_CLIENTS)]\n",
        "\n",
        "# # Convert XGBoost model parameters to a dictionary\n",
        "# model_params = xgboost_model.get_xgb_params()\n",
        "\n",
        "# # Before starting the simulation loop, call fit on each client\n",
        "# for client in clients:\n",
        "#     # Fit each client using the XGBoost model parameters\n",
        "#     client.fit(model_params, config)\n",
        "\n",
        "# # Start simulation\n",
        "# fl.simulation.start_simulation(\n",
        "#     client_fn=lambda cid: clients[int(cid)],\n",
        "#     num_clients=NUM_CLIENTS,\n",
        "#     config=fl.server.ServerConfig(num_rounds=5),\n",
        "#     strategy=strategy,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWRIPsKl7D5m"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# # Define the XGBoost model creation function\n",
        "# def define_xgboost_model(hyperparameters):\n",
        "#     return xgb.XGBRegressor(**hyperparameters)\n",
        "\n",
        "# # Helper function to update the local model parameters\n",
        "# def set_parameters(model, parameters):\n",
        "#     booster = model.get_booster()\n",
        "#     booster.load_model(parameters)\n",
        "\n",
        "# # Helper function to get the local model parameters\n",
        "# def get_parameters(model):\n",
        "#     return model.get_booster().save_raw()\n",
        "\n",
        "# # Flower Client class\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, xgb_model, train_data, val_data):\n",
        "#         self.xgb_model = xgb_model\n",
        "#         self.train_data = train_data\n",
        "#         self.val_data = val_data\n",
        "\n",
        "#     def get_parameters(self):\n",
        "#         return get_parameters(self.xgb_model)\n",
        "\n",
        "#     def set_parameters(self, parameters):\n",
        "#         set_parameters(self.xgb_model, parameters)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Train the XGBoost model on the client's data\n",
        "#         features = self.train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.train_data['energy_sum'].values\n",
        "#         self.xgb_model.fit(features, target)\n",
        "\n",
        "#         # Return the updated model parameters\n",
        "#         return self.get_parameters()\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Evaluate the XGBoost model on the client's validation data\n",
        "#         features = self.val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.val_data['energy_sum'].values\n",
        "#         predictions = self.xgb_model.predict(features)\n",
        "#         loss = mean_squared_error(target, predictions)\n",
        "\n",
        "#         return loss, len(self.val_data), {}\n",
        "\n",
        "# # Create an instance of XGBoost model\n",
        "# hyperparameters = {'objective': 'reg:squarederror', 'learning_rate': 0.1, 'max_depth': 3}\n",
        "# xgboost_model = define_xgboost_model(hyperparameters)\n",
        "\n",
        "\n",
        "# # Create FlowerClient instances\n",
        "# NUM_CLIENTS = 5  # Adjust the number of clients as needed\n",
        "# clients = [XGBoostClient(xgboost_model, FL_train_set, FL_val_set) for _ in range(NUM_CLIENTS)]\n",
        "\n",
        "# # Before starting the simulation loop, call fit on each client\n",
        "# for client in clients:\n",
        "#     # Fit each client using the XGBoost model parameters\n",
        "#     client.fit(None, None)  # Initial fit with None parameters\n",
        "\n",
        "# # Start simulation\n",
        "# fl.simulation.start_simulation(\n",
        "#     client_fn=lambda cid: clients[cid],\n",
        "#     num_clients=NUM_CLIENTS,\n",
        "#     config=fl.server.ServerConfig(num_rounds=5),\n",
        "#     strategy=strategy,  # Define your strategy here\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yqTp44RAAZ-"
      },
      "outputs": [],
      "source": [
        "# def dataloader(data_list, n):\n",
        "#     for dataset in data_list:\n",
        "#         lclid = dataset['LCLid'].iloc[0]\n",
        "#         num_samples = len(dataset)\n",
        "\n",
        "#         print(f\"Loading data for LCLid {lclid}\")\n",
        "\n",
        "#         for i in range(0, num_samples, n):\n",
        "#             end_idx = min(i + n, num_samples)\n",
        "#             batch = dataset.iloc[i:end_idx]\n",
        "#             yield batch\n",
        "\n",
        "#         print(f\"Data loading completed for LCLid {lclid}\\n\")\n",
        "\n",
        "# n_samples_per_batch = 5  # Number of samples per batch\n",
        "\n",
        "# for batch_data in dataloader(FL_train_sets_list, n_samples_per_batch):\n",
        "#     print(batch_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIqXcQkm8YBG"
      },
      "source": [
        "FEDERATED CLIENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRW3-5MJ8TvU"
      },
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "\n",
        "# import flwr as fl\n",
        "# from flwr.common import Metrics\n",
        "# # from flwr.common import Weights\n",
        "# from flwr.common.typing import Parameters\n",
        "# from collections import OrderedDict\n",
        "# from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "# from flwr.common import NDArray, NDArrays\n",
        "\n",
        "\n",
        "# # # XGBoost model\n",
        "# # def define_xgboost_model(hyperparameters: dict) -> xgb.XGBModel:\n",
        "# #     # Hyperparameters for the XGBoost model\n",
        "# #     model = xgb.XGBModel(**hyperparameters)\n",
        "# #     return model\n",
        "\n",
        "# # # Extract hyperparameters from `best_xgb_model` and use it to define the FLOWER client model\n",
        "# # hyperparameters = best_xgb_model.get_xgb_params()\n",
        "# # client_xgb_model = define_xgboost_model(hyperparameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivkd9JmCIdr4"
      },
      "outputs": [],
      "source": [
        "# # Define a function to load local training data for each client\n",
        "# def load_local_data(client_data_list, client_index, batch_size):\n",
        "#     client_data = client_data_list[client_index]\n",
        "#     num_samples = len(client_data)\n",
        "\n",
        "#     # Calculate the starting and ending index for this client's data\n",
        "#     start_index = 0\n",
        "#     end_index = start_index + batch_size\n",
        "\n",
        "#     while start_index < num_samples:\n",
        "#         # Yield a batch of data\n",
        "#         yield client_data.iloc[start_index:end_index]\n",
        "\n",
        "#         # Move to the next batch\n",
        "#         start_index = end_index\n",
        "#         end_index = min(start_index + batch_size, num_samples)\n",
        "\n",
        "# # Example usage\n",
        "# client_index = 0  # Change this to select a different client\n",
        "# batch_size = 5   # Number of samples in each batch\n",
        "\n",
        "# # Load local training data for the selected client\n",
        "# for batch_data in load_local_data(FL_train_sets_list, client_index, batch_size):\n",
        "#     print(\"Loaded batch of data for client\", client_index)\n",
        "#     print(batch_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oM1iiYYPmOM"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, client_data, batch_size):\n",
        "#         self.client_data = client_data\n",
        "#         self.batch_size = batch_size\n",
        "#         self.client_xgb_model = None\n",
        "\n",
        "#     def get_parameters(self):\n",
        "#         if self.client_xgb_model is None:\n",
        "#             return np.array([])  # No parameters available initially\n",
        "#         model_params = self.client_xgb_model.get_booster().save_config()\n",
        "#         return fl.common.weights_to_parameters(model_params)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         if self.client_xgb_model is None:\n",
        "#             hyperparameters = xgb.Booster(model_str=fl.common.parameters_to_weights(parameters))\n",
        "#             self.client_xgb_model = xgb.XGBModel(**hyperparameters)\n",
        "\n",
        "#         for batch_data in self.load_local_data(self.client_data, self.batch_size):\n",
        "#             batch_features = batch_data[[\"mean_temp\", \"pressure\", \"humidity\", \"windSpeed\"]]\n",
        "#             batch_target = batch_data[\"energy_sum\"]\n",
        "\n",
        "#             self.client_xgb_model.fit(batch_features, batch_target)\n",
        "\n",
        "#         updated_model_params = self.client_xgb_model.get_booster().save_config()\n",
        "#         return fl.common.weights_to_parameters(updated_model_params)\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         if self.client_xgb_model is None:\n",
        "#             return {}  # No evaluation possible without a trained model\n",
        "#         model_params = self.client_xgb_model.get_booster().save_config()\n",
        "#         booster = xgb.Booster(model_str=model_params)\n",
        "\n",
        "#         batch_data = self.load_local_data(self.client_data, len(self.client_data)).next()\n",
        "#         batch_features = batch_data[[\"mean_temp\", \"pressure\", \"humidity\", \"windSpeed\"]]\n",
        "#         batch_target = batch_data[\"energy_sum\"]\n",
        "\n",
        "#         predictions = booster.predict(xgb.DMatrix(batch_features))\n",
        "#         mse = np.mean((predictions - batch_target)**2)\n",
        "\n",
        "#         return {\"mse\": mse}\n",
        "\n",
        "#     def load_local_data(self, client_data, batch_size):\n",
        "#         num_samples = len(client_data)\n",
        "#         start_index = 0\n",
        "#         end_index = start_index + batch_size\n",
        "\n",
        "#         while start_index < num_samples:\n",
        "#             yield client_data.iloc[start_index:end_index]\n",
        "#             start_index = end_index\n",
        "#             end_index = min(start_index + batch_size, num_samples)\n",
        "\n",
        "# # Example usage\n",
        "# client_index = 0\n",
        "# batch_size = 5\n",
        "# client_instance = XGBoostClient(FL_train_sets_list[client_index], batch_size)\n",
        "\n",
        "# # fl.client.start_numpy_client(\"[::]:8080\", client=client_instance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLQ9uzPGaoB"
      },
      "source": [
        "Instance class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qaXjq6fGaRV"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, model, dataset, targets):\n",
        "#         self.model = model\n",
        "#         self.dataset = dataset\n",
        "#         self.targets = targets\n",
        "\n",
        "#     def get_parameters(self):\n",
        "#         # Return the current local model parameters\n",
        "#         return self.model.get_xgb_params()\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         # Set the model's parameters\n",
        "#         self.model.set_params(parameters)\n",
        "\n",
        "#         # Train the model on the local data\n",
        "#         self.model.fit(self.dataset, self.targets)\n",
        "\n",
        "#         # Return the updated model parameters\n",
        "#         return self.model.get_xgb_params()\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         # Set the model's parameters\n",
        "#         self.model.set_params(parameters)\n",
        "\n",
        "#         # Evaluate the model on the local data\n",
        "#         evaluation_result = self.model.score(self.dataset, self.targets)\n",
        "\n",
        "#         # Return the evaluation result\n",
        "#         return evaluation_result\n",
        "\n",
        "# # Create an instance of XGBoost model\n",
        "# xgboost_model = define_xgboost_model(hyperparameters)  # Define this function as mentioned before\n",
        "\n",
        "# # Create an instance of the XGBoostClient class\n",
        "# xgboost_client = XGBoostClient(model=xgboost_model, dataset=FL_val_set[[\"mean_temp\", \"pressure\", \"humidity\", \"windSpeed\"]], targets=FL_val_set[\"energy_sum\"])\n",
        "\n",
        "# # # Start the Flower client\n",
        "# # fl.client.start_numpy_client(server_address=\"[SERVER_ADDRESS]\", client=xgboost_client)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDSikcPVt7Go"
      },
      "source": [
        "## Should be doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPpxpmwOpfav"
      },
      "source": [
        "### Gated Convolution Neural Network (GCNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NciAJdMzmNnl"
      },
      "outputs": [],
      "source": [
        "# class GatedCNN24(nn.Module):\n",
        "#     def __init__(self, input_channels, hidden_sizes):\n",
        "#         super(GatedCNN24, self).__init__()\n",
        "#         self.input_channels = input_channels\n",
        "#         self.hidden_sizes = hidden_sizes\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(self.input_channels, self.hidden_sizes[0], kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(self.hidden_sizes[0], self.hidden_sizes[1], kernel_size=3, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(self.hidden_sizes[1], self.hidden_sizes[2], kernel_size=3, padding=1)\n",
        "\n",
        "#         self.gate_conv1 = nn.Conv2d(self.input_channels, self.hidden_sizes[0], kernel_size=3, padding=1)\n",
        "#         self.gate_conv2 = nn.Conv2d(self.hidden_sizes[0], self.hidden_sizes[1], kernel_size=3, padding=1)\n",
        "#         self.gate_conv3 = nn.Conv2d(self.hidden_sizes[1], self.hidden_sizes[2], kernel_size=3, padding=1)\n",
        "\n",
        "#         self.fc1 = nn.Linear(self.hidden_sizes[2], 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Convolutional layers\n",
        "#         conv1_out = self.conv1(x)\n",
        "#         conv2_out = self.conv2(conv1_out)\n",
        "#         conv3_out = self.conv3(conv2_out)\n",
        "\n",
        "#         # Gating layers\n",
        "#         gate_conv1_out = torch.sigmoid(self.gate_conv1(x))\n",
        "#         gate_conv2_out = torch.sigmoid(self.gate_conv2(conv1_out))\n",
        "#         gate_conv3_out = torch.sigmoid(self.gate_conv3(conv2_out))\n",
        "\n",
        "#         # Element-wise multiplication (gating)\n",
        "#         gated_conv1 = conv1_out * gate_conv1_out\n",
        "#         gated_conv2 = conv2_out * gate_conv2_out\n",
        "#         gated_conv3 = conv3_out * gate_conv3_out\n",
        "\n",
        "#         # Sum along channels\n",
        "#         gated_sum = gated_conv1 + gated_conv2 + gated_conv3\n",
        "\n",
        "#         # Global average pooling\n",
        "#         pooled = torch.mean(gated_sum, dim=[2, 3])\n",
        "\n",
        "#         # Fully connected layer\n",
        "#         output = self.fc1(pooled)\n",
        "#         return output.squeeze()\n",
        "\n",
        "# class EnergyDataset(Dataset):\n",
        "#     def __init__(self, features, targets):\n",
        "#         self.features = features\n",
        "#         self.targets = targets\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.features)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         inputs = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
        "#         target = torch.tensor(self.targets.iloc[idx], dtype=torch.float32)\n",
        "#         return inputs, target\n",
        "\n",
        "# # Define hyperparameters\n",
        "# input_channels = 4  # Number of features\n",
        "# hidden_sizes = [10, 8, 1]\n",
        "# learning_rate = 0.001\n",
        "# num_epochs = 10\n",
        "# batch_size = 50\n",
        "\n",
        "# # Create DataLoader for training\n",
        "# train_dataset = EnergyDataset(Central_X_train, Central_y_train)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Create the model\n",
        "# GatedCNN_model = GatedCNN24(input_channels, hidden_sizes)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(GatedCNN_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     GatedCNN_model.train()  # Set the model in training mode\n",
        "#     for inputs, targets in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Reshape the input tensor to match expected shape [batch_size, channels, height, width]\n",
        "#         inputs = inputs.view(inputs.size(0), input_channels, 1, 1)\n",
        "\n",
        "#         outputs = GatedCNN_model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcM_OfWUsMvC"
      },
      "source": [
        "### Gated Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pFnAbOBsL8T"
      },
      "outputs": [],
      "source": [
        "# # Build the Gated Neural Network model\n",
        "# inputs = Input(shape=(4,))\n",
        "# dense_layer = Dense(16, activation='relu')(inputs)\n",
        "\n",
        "# # Gating mechanism\n",
        "# dense_gate = Dense(16, activation='sigmoid')(inputs)\n",
        "# gated_dense = Multiply()([dense_layer, dense_gate])\n",
        "\n",
        "# output = Dense(1, activation='linear')(gated_dense)\n",
        "\n",
        "# GNN_model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# # Compile the model\n",
        "# GNN_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# # Train the model\n",
        "# GNN_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vWU1fCXVX94"
      },
      "source": [
        "Implementing FLower Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i6mrPCqIxbc"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "\n",
        "# from flwr.client import NumPyClient\n",
        "# import xgboost as xgb\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import io\n",
        "\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, model, X_train, y_train, X_val, y_val):\n",
        "#         self.model = model\n",
        "#         self.X_train = X_train\n",
        "#         self.y_train = y_train\n",
        "#         self.X_val = X_val\n",
        "#         self.y_val = y_val\n",
        "\n",
        "#     def get_parameters(self, config=None):\n",
        "#         with io.BytesIO() as buffer:\n",
        "#             self.model.save_model(buffer)\n",
        "#             return buffer.getvalue()\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         new_model = xgb.XGBRegressor(**config)\n",
        "#         with io.BytesIO(parameters) as buffer:\n",
        "#             new_model.load_model(buffer)\n",
        "#         self.model = new_model\n",
        "\n",
        "#         self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "#         with io.BytesIO() as buffer:\n",
        "#             self.model.save_model(buffer)\n",
        "#             return buffer.getvalue()\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         new_model = xgb.Booster(params=config, model_str=parameters[0])\n",
        "#         self.model = new_model\n",
        "\n",
        "#         y_pred = self.model.predict(self.X_val)\n",
        "#         accuracy = accuracy_score(self.y_val, y_pred)\n",
        "\n",
        "#         return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeqMcrjHZGtt"
      },
      "outputs": [],
      "source": [
        "# # the best hyperparameters in best_xgb_model\n",
        "# best_hyperparameters = best_xgb_model.get_xgb_params()\n",
        "\n",
        "# # Create an instance of your XGBoost model with the best hyperparameters\n",
        "# best_xgb_model_instance = xgb.XGBRegressor(**best_hyperparameters)\n",
        "\n",
        "\n",
        "# # Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "# client_resources = None\n",
        "# if DEVICE.type == \"cuda\":\n",
        "#     client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# # Define a function to create a virtual client\n",
        "# def create_virtual_client(lclid):\n",
        "#     client_data = FL_train_set[FL_train_set['LCLid'] == lclid]\n",
        "#     X_client = client_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "#     y_client = client_data['energy_sum']\n",
        "#     return XGBoostClient(best_xgb_model_instance, X_client, y_client,  FL_test_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']], FL_test_set['energy_sum'])\n",
        "\n",
        "# # Fit the XGBoost model before creating virtual clients\n",
        "# best_xgb_model_instance.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# # Create a list of virtual clients\n",
        "# virtual_clients = [create_virtual_client(lclid) for lclid in unique_lclids]\n",
        "\n",
        "# # Define the client function for Flower simulation\n",
        "# def client_fn(cid: str) -> XGBoostClient:\n",
        "#     virtual_client = virtual_clients[int(cid)]\n",
        "#     return virtual_client\n",
        "\n",
        "# # Define the Flower simulation configuration\n",
        "# config = fl.server.ServerConfig(num_rounds=5)\n",
        "\n",
        "# # Start the Flower simulation\n",
        "# fl.simulation.start_simulation(\n",
        "#     client_fn=client_fn,\n",
        "#     num_clients=len(virtual_clients),\n",
        "#     config=config,\n",
        "#     strategy=fl.server.strategy.FedAvg(),\n",
        "#     client_resources=None,  # Adjust client resources as needed\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AcAwekLwJOSZ"
      ],
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1oLHv4e-uSTEYGSiiJDsM6CuTErnsaA5X",
      "authorship_tag": "ABX9TyOcXE/1vkHBFdaQ4AGQWdgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}