{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirank981/Project_space/blob/main/project_space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP5nJnDbJPYO"
      },
      "source": [
        "# Installing dependences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OravxoTjLhVa"
      },
      "source": [
        "Install the necessary packages for PyTorch (torch and torchvision) and Flower (flwr) and pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpAFZGHjJOOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42882807-6fba-4751-c2ce-b8a77ff9cc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQeKkPPLUkF"
      },
      "source": [
        "Import everything we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdmn7_KHLVVI",
        "outputId": "9a84e997-ca6e-4d66-91d7-82847dfc620d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2\n",
            "Training on cpu using PyTorch 2.0.1+cu118 and Flower 1.4.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "# from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # Import the train_test_split function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "!pip install catboost\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from tensorflow.keras.layers import Conv2D, Multiply, Input\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Dense, Multiply\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFfbJxnLxF7"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6l5c_376GqC"
      },
      "source": [
        "Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSv4I6ji-3of",
        "outputId": "d0ed2c87-00af-4505-c99c-e45ef2f6f245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n4a987x-7kd"
      },
      "source": [
        "Setting the path to the location of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ1t9RTF91ZQ"
      },
      "outputs": [],
      "source": [
        "# Define the path to daily dataset folder\n",
        "daily_dataset_path = Path('/content/drive/MyDrive/Federated learning implementation/dataset/dataset_archive/daily_dataset/daily_dataset')\n",
        "\n",
        "# Define the path to daily dataset folder\n",
        "weather_daily_dataset_path = Path('/content/drive/MyDrive/Federated learning implementation/dataset/dataset_archive/weather_daily_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2owcEz6M9K"
      },
      "source": [
        "## Loading daily data\n",
        "(of energy consumption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdpP9rwQBqJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initializing list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Loop through the CSV files and reading them into dataframes\n",
        "for i in range(2):\n",
        "    filename = f'block_{i}.csv'\n",
        "    df = pd.read_csv(daily_dataset_path / filename)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenating all the dataframes into a single dataframe\n",
        "energy_daily_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# NRATM(Not Required At The Moment)\n",
        "# # Group the data by LCLid and create a dictionary of dataframes\n",
        "# grouped_data = dict(tuple(energy_daily_data.groupby('LCLid')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBAsp3Ky51Bn"
      },
      "source": [
        "Loading data using file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeJMbTTY0YZY",
        "outputId": "92f830f0-7802-4a4b-cf33-c5a59e8b7f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           LCLid         day  energy_median  energy_mean  energy_max  \\\n",
            "0      MAC000002  2012-10-12         0.1385     0.154304       0.886   \n",
            "1      MAC000002  2012-10-13         0.1800     0.230979       0.933   \n",
            "2      MAC000002  2012-10-14         0.1580     0.275479       1.085   \n",
            "3      MAC000002  2012-10-15         0.1310     0.213688       1.164   \n",
            "4      MAC000002  2012-10-16         0.1450     0.203521       0.991   \n",
            "...          ...         ...            ...          ...         ...   \n",
            "25569  MAC005492  2014-02-24         0.1690     0.175042       0.378   \n",
            "25570  MAC005492  2014-02-25         0.1550     0.160792       0.545   \n",
            "25571  MAC005492  2014-02-26         0.1490     0.178542       0.687   \n",
            "25572  MAC005492  2014-02-27         0.1140     0.146167       0.478   \n",
            "25573  MAC005492  2014-02-28         0.0880     0.088000       0.088   \n",
            "\n",
            "       energy_count  energy_std  energy_sum  energy_min  \n",
            "0                46    0.196034       7.098       0.000  \n",
            "1                48    0.192329      11.087       0.076  \n",
            "2                48    0.274647      13.223       0.070  \n",
            "3                48    0.224483      10.257       0.070  \n",
            "4                48    0.184115       9.769       0.087  \n",
            "...             ...         ...         ...         ...  \n",
            "25569            48    0.073174       8.402       0.079  \n",
            "25570            48    0.082118       7.718       0.079  \n",
            "25571            48    0.120820       8.570       0.079  \n",
            "25572            48    0.082616       7.016       0.079  \n",
            "25573             1         NaN       0.088       0.088  \n",
            "\n",
            "[25574 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "# Loading data from a specific CSV file\n",
        "specific_file_data = pd.read_csv(daily_dataset_path / 'block_0.csv')\n",
        "\n",
        "# Displaying data\n",
        "print(specific_file_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0b947PkSfrG"
      },
      "source": [
        "### Loading required data\n",
        "Creating a DataFrame that have only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMsi4uZITUt8",
        "outputId": "33a12197-46ab-4935-a5e4-eebd9e8e01ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Data report of MAC000002\n",
            "         LCLid  energy_sum         day\n",
            "0    MAC000002       7.098  2012-10-12\n",
            "1    MAC000002      11.087  2012-10-13\n",
            "2    MAC000002      13.223  2012-10-14\n",
            "3    MAC000002      10.257  2012-10-15\n",
            "4    MAC000002       9.769  2012-10-16\n",
            "..         ...         ...         ...\n",
            "500  MAC000002      12.528  2014-02-24\n",
            "501  MAC000002      11.826  2014-02-25\n",
            "502  MAC000002      12.328  2014-02-26\n",
            "503  MAC000002      20.518  2014-02-27\n",
            "504  MAC000002       1.387  2014-02-28\n",
            "\n",
            "[505 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "selected_column = ['LCLid','energy_sum','day']\n",
        "energy_daily_selected=energy_daily_data[selected_column]\n",
        "# print(energy_daily_selected)\n",
        "\n",
        "# Group the data by LCLid and create a dictionary of dataframes, allowing to access each dataframe separately using the LCLid as the key\n",
        "grouped_data_selected = dict(tuple(energy_daily_selected.groupby('LCLid')))\n",
        "# Display the data for 'MAC000002'\n",
        "print('\\n')\n",
        "print('Data report of MAC000002')\n",
        "print(grouped_data_selected['MAC000002'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZumIN7H_Zt-"
      },
      "source": [
        "## Loading daily weather data\n",
        "\n",
        "Creating a 'day' column that stores only the date values from 'time' column\n",
        "(for linking weather dataset 'day' with daily dataset 'day')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch2Azrah7Cuj",
        "outputId": "70c8d3f9-7d3a-45d1-a653-344aad18379a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     temperatureMax   temperatureMaxTime  windBearing                 icon  \\\n",
            "0             11.96  2011-11-11 23:00:00          123                  fog   \n",
            "1              8.59  2011-12-11 14:00:00          198    partly-cloudy-day   \n",
            "2             10.33  2011-12-27 02:00:00          225    partly-cloudy-day   \n",
            "3              8.07  2011-12-02 23:00:00          232                 wind   \n",
            "4              8.22  2011-12-24 23:00:00          252  partly-cloudy-night   \n",
            "..              ...                  ...          ...                  ...   \n",
            "877            9.03  2014-01-26 16:00:00          233    partly-cloudy-day   \n",
            "878           10.31  2014-02-27 14:00:00          224    partly-cloudy-day   \n",
            "879           18.97  2014-03-09 14:00:00          172  partly-cloudy-night   \n",
            "880            8.83  2014-02-12 16:00:00          210                 wind   \n",
            "881            9.90  2014-02-15 12:00:00          233                 wind   \n",
            "\n",
            "     dewPoint   temperatureMinTime  cloudCover  windSpeed  pressure  \\\n",
            "0        9.40  2011-11-11 07:00:00        0.79       3.88   1016.08   \n",
            "1        4.49  2011-12-11 01:00:00        0.56       3.94   1007.71   \n",
            "2        5.47  2011-12-27 23:00:00        0.85       3.54   1032.76   \n",
            "3        3.69  2011-12-02 07:00:00        0.32       3.00   1012.12   \n",
            "4        2.79  2011-12-24 07:00:00        0.37       4.46   1028.17   \n",
            "..        ...                  ...         ...        ...       ...   \n",
            "877      2.39  2014-01-26 21:00:00        0.40       4.55   1002.10   \n",
            "878      3.08  2014-02-27 23:00:00        0.32       4.14   1007.02   \n",
            "879      4.30  2014-03-09 07:00:00        0.04       2.78   1022.44   \n",
            "880      1.94  2014-02-12 01:00:00        0.59       7.24    994.27   \n",
            "881      2.95  2014-02-15 23:00:00        0.35       9.96    988.63   \n",
            "\n",
            "    apparentTemperatureMinTime  ...          sunriseTime  temperatureHighTime  \\\n",
            "0          2011-11-11 07:00:00  ...  2011-11-11 07:12:14  2011-11-11 19:00:00   \n",
            "1          2011-12-11 02:00:00  ...  2011-12-11 07:57:02  2011-12-11 14:00:00   \n",
            "2          2011-12-27 22:00:00  ...  2011-12-27 08:07:06  2011-12-27 14:00:00   \n",
            "3          2011-12-02 07:00:00  ...  2011-12-02 07:46:09  2011-12-02 12:00:00   \n",
            "4          2011-12-24 07:00:00  ...  2011-12-24 08:06:15  2011-12-24 15:00:00   \n",
            "..                         ...  ...                  ...                  ...   \n",
            "877        2014-01-26 22:00:00  ...  2014-01-26 07:48:49  2014-01-26 16:00:00   \n",
            "878        2014-02-27 22:00:00  ...  2014-02-27 06:51:45  2014-02-27 14:00:00   \n",
            "879        2014-03-09 07:00:00  ...  2014-03-09 06:29:49  2014-03-09 14:00:00   \n",
            "880        2014-02-12 01:00:00  ...  2014-02-12 07:21:44  2014-02-12 16:00:00   \n",
            "881        2014-02-15 23:00:00  ...  2014-02-15 07:16:06  2014-02-15 12:00:00   \n",
            "\n",
            "             uvIndexTime                                            summary  \\\n",
            "0    2011-11-11 11:00:00                             Foggy until afternoon.   \n",
            "1    2011-12-11 12:00:00                  Partly cloudy throughout the day.   \n",
            "2    2011-12-27 00:00:00                  Mostly cloudy throughout the day.   \n",
            "3    2011-12-02 10:00:00  Partly cloudy throughout the day and breezy ov...   \n",
            "4    2011-12-24 13:00:00                  Mostly cloudy throughout the day.   \n",
            "..                   ...                                                ...   \n",
            "877  2014-01-26 11:00:00                       Mostly cloudy until evening.   \n",
            "878  2014-02-27 12:00:00                       Partly cloudy until evening.   \n",
            "879  2014-03-09 12:00:00                      Partly cloudy in the evening.   \n",
            "880  2014-02-12 10:00:00  Mostly cloudy until evening and breezy through...   \n",
            "881  2014-02-15 10:00:00           Windy and mostly cloudy until afternoon.   \n",
            "\n",
            "      temperatureLowTime  apparentTemperatureMin  apparentTemperatureMaxTime  \\\n",
            "0    2011-11-11 19:00:00                    6.48         2011-11-11 23:00:00   \n",
            "1    2011-12-12 07:00:00                    0.11         2011-12-11 20:00:00   \n",
            "2    2011-12-27 23:00:00                    5.59         2011-12-27 02:00:00   \n",
            "3    2011-12-02 19:00:00                    0.46         2011-12-02 12:00:00   \n",
            "4    2011-12-24 19:00:00                   -0.51         2011-12-24 23:00:00   \n",
            "..                   ...                     ...                         ...   \n",
            "877  2014-01-27 05:00:00                   -1.30         2014-01-26 15:00:00   \n",
            "878  2014-02-28 02:00:00                    1.41         2014-02-27 14:00:00   \n",
            "879  2014-03-10 05:00:00                    7.08         2014-03-09 14:00:00   \n",
            "880  2014-02-13 05:00:00                   -1.20         2014-02-12 16:00:00   \n",
            "881  2014-02-16 07:00:00                    1.77         2014-02-15 12:00:00   \n",
            "\n",
            "     apparentTemperatureLowTime moonPhase mean_temp  \n",
            "0           2011-11-11 19:00:00      0.52    10.405  \n",
            "1           2011-12-12 08:00:00      0.53     5.535  \n",
            "2           2011-12-28 00:00:00      0.10     9.180  \n",
            "3           2011-12-02 19:00:00      0.25     5.315  \n",
            "4           2011-12-24 20:00:00      0.99     5.695  \n",
            "..                          ...       ...       ...  \n",
            "877         2014-01-27 04:00:00      0.84     6.145  \n",
            "878         2014-02-28 02:00:00      0.93     7.120  \n",
            "879         2014-03-10 06:00:00      0.28    13.310  \n",
            "880         2014-02-13 02:00:00      0.42     5.930  \n",
            "881         2014-02-16 07:00:00      0.52     7.640  \n",
            "\n",
            "[882 rows x 33 columns]\n"
          ]
        }
      ],
      "source": [
        "# Load the weather dataset into a DataFrame\n",
        "weather_daily_data = pd.read_csv(weather_daily_dataset_path)\n",
        "\n",
        "# Convert the 'time' column to datetime format\n",
        "weather_daily_data['time'] = pd.to_datetime(weather_daily_data['time'])\n",
        "\n",
        "# Calculate the mean temperature for each day and store it in a new column 'mean_temp'\n",
        "weather_daily_data['mean_temp'] = (weather_daily_data['temperatureMax'] + weather_daily_data['temperatureMin']) / 2\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(weather_daily_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUsgV_WfSTVp"
      },
      "source": [
        "### Loading required data\n",
        "Creating a DataFrame that have only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRn0x1J8RlPl",
        "outputId": "d55ff52e-1b47-4195-92d1-6d545db58829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     mean_temp  pressure  humidity  windSpeed       time\n",
            "0       10.405   1016.08      0.95       3.88 2011-11-11\n",
            "1        5.535   1007.71      0.88       3.94 2011-12-11\n",
            "2        9.180   1032.76      0.74       3.54 2011-12-27\n",
            "3        5.315   1012.12      0.87       3.00 2011-12-02\n",
            "4        5.695   1028.17      0.80       4.46 2011-12-24\n",
            "..         ...       ...       ...        ...        ...\n",
            "877      6.145   1002.10      0.79       4.55 2014-01-26\n",
            "878      7.120   1007.02      0.74       4.14 2014-02-27\n",
            "879     13.310   1022.44      0.58       2.78 2014-03-09\n",
            "880      5.930    994.27      0.75       7.24 2014-02-12\n",
            "881      7.640    988.63      0.69       9.96 2014-02-15\n",
            "\n",
            "[882 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with selected columns\n",
        "selected_columns = ['mean_temp', 'pressure', 'humidity', 'windSpeed', 'time']\n",
        "weather_selected = weather_daily_data[selected_columns]\n",
        "\n",
        "# Print the new dataset\n",
        "print(weather_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset with Household energy consumption values and weather values"
      ],
      "metadata": {
        "id": "yi3pXETWEhtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For all households"
      ],
      "metadata": {
        "id": "jlEy0Fm_JaT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'time' column in weather_selected to datetime objects\n",
        "weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n",
        "\n",
        "# Create a list to store the merged data DataFrames\n",
        "merged_data_list = []\n",
        "\n",
        "# Iterate through each LCLid in energy_daily_selected\n",
        "for lclid, data in grouped_data_selected.items():\n",
        "    # Convert 'day' column in current LCLid data to datetime objects\n",
        "    data['day'] = pd.to_datetime(data['day'])\n",
        "\n",
        "    # Merge the current LCLid data with weather_selected based on the common date values\n",
        "    merged_data_lclid = pd.merge(weather_selected, data, left_on='time', right_on='day', how='inner')\n",
        "\n",
        "    # Drop the redundant 'day' column from the merged data\n",
        "    merged_data_lclid.drop(columns=['day'], inplace=True)\n",
        "\n",
        "    # Append the merged data to the merged_data_list\n",
        "    merged_data_list.append(merged_data_lclid)\n",
        "\n",
        "# Concatenate the merged data DataFrames in the list\n",
        "merged_data = pd.concat(merged_data_list, ignore_index=True)\n",
        "\n",
        "# Display the merged dataset\n",
        "print(merged_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqUqCBNEHb8P",
        "outputId": "f9e97dda-b3ce-4127-e8cd-3d26b44580c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-fc90630300b6>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0         10.000   1001.79      0.81       6.54 2012-11-25  MAC000002   \n",
            "1         12.170   1008.74      0.90       5.74 2012-11-20  MAC000002   \n",
            "2          7.830    979.63      0.85       4.07 2012-11-01  MAC000002   \n",
            "3          5.295   1020.29      0.87       3.58 2012-11-06  MAC000002   \n",
            "4          4.650   1001.72      0.80       5.63 2012-12-07  MAC000002   \n",
            "...          ...       ...       ...        ...        ...        ...   \n",
            "26732      4.470   1001.76      0.91       1.52 2014-01-30  MAC005492   \n",
            "26733      6.145   1002.10      0.79       4.55 2014-01-26  MAC005492   \n",
            "26734      7.120   1007.02      0.74       4.14 2014-02-27  MAC005492   \n",
            "26735      5.930    994.27      0.75       7.24 2014-02-12  MAC005492   \n",
            "26736      7.640    988.63      0.69       9.96 2014-02-15  MAC005492   \n",
            "\n",
            "       energy_sum  \n",
            "0          10.545  \n",
            "1          11.221  \n",
            "2          12.209  \n",
            "3          11.663  \n",
            "4          13.248  \n",
            "...           ...  \n",
            "26732       7.928  \n",
            "26733       9.634  \n",
            "26734       7.016  \n",
            "26735       8.025  \n",
            "26736       8.994  \n",
            "\n",
            "[26737 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For one household"
      ],
      "metadata": {
        "id": "Jq2mLT2oJWB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data[merged_data['LCLid'] == \"MAC003719\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBmTUuggiWZZ",
        "outputId": "35811b8d-41c3-4342-fe77-f748254486c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "21058     10.000   1001.79      0.81       6.54 2012-11-25  MAC003719   \n",
            "21059     12.170   1008.74      0.90       5.74 2012-11-20  MAC003719   \n",
            "21060      7.830    979.63      0.85       4.07 2012-11-01  MAC003719   \n",
            "21061      5.295   1020.29      0.87       3.58 2012-11-06  MAC003719   \n",
            "21062      4.650   1001.72      0.80       5.63 2012-12-07  MAC003719   \n",
            "...          ...       ...       ...        ...        ...        ...   \n",
            "21331      4.470   1001.76      0.91       1.52 2014-01-30  MAC003719   \n",
            "21332      6.145   1002.10      0.79       4.55 2014-01-26  MAC003719   \n",
            "21333      7.120   1007.02      0.74       4.14 2014-02-27  MAC003719   \n",
            "21334      5.930    994.27      0.75       7.24 2014-02-12  MAC003719   \n",
            "21335      7.640    988.63      0.69       9.96 2014-02-15  MAC003719   \n",
            "\n",
            "       energy_sum  \n",
            "21058      13.952  \n",
            "21059      12.864  \n",
            "21060      18.794  \n",
            "21061      16.750  \n",
            "21062      15.547  \n",
            "...           ...  \n",
            "21331      12.672  \n",
            "21332       9.173  \n",
            "21333       9.034  \n",
            "21334      14.075  \n",
            "21335      13.240  \n",
            "\n",
            "[278 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVelsQ53430r"
      },
      "source": [
        "# Splitting dataset to training and testing sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing dataset"
      ],
      "metadata": {
        "id": "Qw5aaS7xkIWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting amount of data for each LCLid (data available for each household)"
      ],
      "metadata": {
        "id": "FWB2ilpvq0_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'time' column in weather_selected to datetime objects\n",
        "weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n",
        "\n",
        "# Merge the data into merged_data DataFrame as described in your previous code\n",
        "\n",
        "# Count the number of data rows for each LCLid\n",
        "lclid_data_counts = merged_data['LCLid'].value_counts()\n",
        "\n",
        "# Display the counts for each LCLid\n",
        "print(\"Data row counts for each LCLid:\")\n",
        "print(lclid_data_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_1dDvPeoXFD",
        "outputId": "cfff913d-d1cf-41d3-8824-3c61e271cb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data row counts for each LCLid:\n",
            "MAC000246    392\n",
            "MAC004529    374\n",
            "MAC000323    298\n",
            "MAC000386    291\n",
            "MAC004387    288\n",
            "            ... \n",
            "MAC003463    154\n",
            "MAC004034    154\n",
            "MAC003718    154\n",
            "MAC002613    149\n",
            "MAC001074     53\n",
            "Name: LCLid, Length: 100, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-467dbfca5fe4>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identifiting the no of households with insufficient amount of data"
      ],
      "metadata": {
        "id": "wIjz5a0irBRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of data rows for each LCLid\n",
        "lclid_data_counts = merged_data['LCLid'].value_counts()\n",
        "\n",
        "\n",
        "# Get the total number of unique LCLid values\n",
        "total_lclids = len(lclid_data_counts)\n",
        "\n",
        "# Count the number of LCLid values with less than 100 data rows\n",
        "count_less_than_100 = (lclid_data_counts < 100).sum()\n",
        "\n",
        "# Display the count of LCLid values with less than 100 data rows\n",
        "print(\"Number of LCLid values with less than 100 data rows:\", count_less_than_100)\n",
        "\n",
        "# Display the total number of unique LCLid values\n",
        "print(\"Total number of unique LCLid values:\", total_lclids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlmWA354pGgO",
        "outputId": "121f60e6-3475-4577-e93f-fc610d7a240c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of LCLid values with less than 100 data rows: 1\n",
            "Total number of unique LCLid values: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing the households with less data, from the dataset"
      ],
      "metadata": {
        "id": "SA9X4z0irXP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of LCLid values with less than 100 data rows\n",
        "lclids_to_remove = lclid_data_counts[lclid_data_counts < 100].index\n",
        "\n",
        "# Remove rows corresponding to LCLid values with less than 100 data rows\n",
        "filtered_data = merged_data[~merged_data['LCLid'].isin(lclids_to_remove)]\n",
        "\n",
        "# also removing rows with NaN values in the dataset\n",
        "filtered_data = filtered_data.dropna()\n",
        "\n",
        "# Display the filtered data\n",
        "print(filtered_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VuTC5T5pphX",
        "outputId": "2e509f91-19e4-4612-fea7-40cad8f6986f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0         10.000   1001.79      0.81       6.54 2012-11-25  MAC000002   \n",
            "1         12.170   1008.74      0.90       5.74 2012-11-20  MAC000002   \n",
            "2          7.830    979.63      0.85       4.07 2012-11-01  MAC000002   \n",
            "3          5.295   1020.29      0.87       3.58 2012-11-06  MAC000002   \n",
            "4          4.650   1001.72      0.80       5.63 2012-12-07  MAC000002   \n",
            "...          ...       ...       ...        ...        ...        ...   \n",
            "26732      4.470   1001.76      0.91       1.52 2014-01-30  MAC005492   \n",
            "26733      6.145   1002.10      0.79       4.55 2014-01-26  MAC005492   \n",
            "26734      7.120   1007.02      0.74       4.14 2014-02-27  MAC005492   \n",
            "26735      5.930    994.27      0.75       7.24 2014-02-12  MAC005492   \n",
            "26736      7.640    988.63      0.69       9.96 2014-02-15  MAC005492   \n",
            "\n",
            "       energy_sum  \n",
            "0          10.545  \n",
            "1          11.221  \n",
            "2          12.209  \n",
            "3          11.663  \n",
            "4          13.248  \n",
            "...           ...  \n",
            "26732       7.928  \n",
            "26733       9.634  \n",
            "26734       7.016  \n",
            "26735       8.025  \n",
            "26736       8.994  \n",
            "\n",
            "[26684 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset to train and test\n",
        " where the split ratio(70:30) is made with every househould's data."
      ],
      "metadata": {
        "id": "IlfwiI3Rrm1X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mjDCXMXoizi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6800b1f-1598-4000-8052-47cca9cfa703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "       mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0          5.170   1006.05      0.90       1.90 2013-11-09  MAC002068   \n",
            "1          4.965   1006.45      0.86       3.61 2013-01-26  MAC002068   \n",
            "2          7.360   1001.69      0.86       3.79 2012-12-17  MAC002068   \n",
            "3          9.700   1010.05      0.80       3.17 2012-11-09  MAC002068   \n",
            "4          9.040    991.49      0.86       4.93 2012-12-15  MAC002068   \n",
            "...          ...       ...       ...        ...        ...        ...   \n",
            "18614      9.940   1019.67      0.85       3.44 2013-12-13  MAC003686   \n",
            "18615      7.195   1005.46      0.84       4.09 2012-12-27  MAC003686   \n",
            "18616      9.020   1020.10      0.80       3.67 2013-10-30  MAC003686   \n",
            "18617      3.565    991.02      0.91       0.73 2013-03-18  MAC003686   \n",
            "18618      7.450   1024.32      0.86       4.20 2012-11-07  MAC003686   \n",
            "\n",
            "       energy_sum  \n",
            "0          14.117  \n",
            "1          11.782  \n",
            "2          12.446  \n",
            "3          24.090  \n",
            "4          34.729  \n",
            "...           ...  \n",
            "18614      15.173  \n",
            "18615       5.590  \n",
            "18616      17.455  \n",
            "18617      11.930  \n",
            "18618       9.444  \n",
            "\n",
            "[18619 rows x 7 columns]\n",
            "Test set:\n",
            "      mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "0         7.360   1011.21      0.82       4.13 2012-11-19  MAC002068   \n",
            "1         0.100    997.28      0.86       4.16 2013-01-19  MAC002068   \n",
            "2         2.100   1007.72      0.82       4.23 2013-01-23  MAC002068   \n",
            "3         1.260   1013.39      0.65       4.99 2013-03-26  MAC002068   \n",
            "4         6.915   1023.52      0.70       1.11 2013-02-16  MAC002068   \n",
            "...         ...       ...       ...        ...        ...        ...   \n",
            "8060      7.560   1022.45      0.81       5.45 2013-12-14  MAC003686   \n",
            "8061      5.925   1032.90      0.83       1.02 2013-12-01  MAC003686   \n",
            "8062      7.800    995.66      0.89       4.39 2012-12-25  MAC003686   \n",
            "8063      8.895   1016.76      0.89       4.78 2012-12-28  MAC003686   \n",
            "8064      5.900   1015.79      0.79       2.61 2013-11-10  MAC003686   \n",
            "\n",
            "      energy_sum  \n",
            "0         14.621  \n",
            "1         20.322  \n",
            "2         17.787  \n",
            "3         11.331  \n",
            "4         13.932  \n",
            "...          ...  \n",
            "8060      19.748  \n",
            "8061      23.065  \n",
            "8062       5.737  \n",
            "8063       5.578  \n",
            "8064       7.980  \n",
            "\n",
            "[8065 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Define the split percentages\n",
        "train_percentage = 0.7 # 70% for training, 30% for testing\n",
        "min_data_points = 10  # Minimum number of data points required for an LCLid\n",
        "\n",
        "# Create a list to store DataFrames for training and testing\n",
        "train_data_list = []\n",
        "test_data_list = []\n",
        "\n",
        "# Iterate through each unique LCLid and split the data based on train_percentage\n",
        "unique_lclids = filtered_data['LCLid'].unique()\n",
        "for lclid in unique_lclids:\n",
        "    lclid_data = filtered_data[filtered_data['LCLid'] == lclid]\n",
        "\n",
        "    # Check if there are sufficient data points for the current LCLid\n",
        "    if len(lclid_data) >= min_data_points:\n",
        "        # Split the data for the current LCLid into training and test sets\n",
        "        train_data_lclid, test_data_lclid = train_test_split(lclid_data, train_size=train_percentage, shuffle=False)\n",
        "\n",
        "        # Randomize the rows within each subset\n",
        "        train_data_lclid = train_data_lclid.sample(frac=1, random_state=42)\n",
        "        test_data_lclid = test_data_lclid.sample(frac=1, random_state=42)\n",
        "\n",
        "        # Append to the train_data_list and test_data_list\n",
        "        train_data_list.append(train_data_lclid)\n",
        "        test_data_list.append(test_data_lclid)\n",
        "\n",
        "# Concatenate the DataFrames in the lists\n",
        "FL_train_set = pd.concat(train_data_list, ignore_index=True)\n",
        "FL_test_set = pd.concat(test_data_list, ignore_index=True)\n",
        "\n",
        "# Store the training and test sets in separate lists\n",
        "FL_train_set_list = train_data_list\n",
        "FL_test_sets_list = test_data_list\n",
        "\n",
        "# Display the training and test sets\n",
        "print(\"Training set:\")\n",
        "print(FL_train_set)\n",
        "print(\"Test set:\")\n",
        "print(FL_test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For FL training, validation and test sets"
      ],
      "metadata": {
        "id": "VAZQP_IFjqJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first training set\n",
        "print(\"First Training set:\")\n",
        "print(FL_train_set_list[0])\n",
        "print(\"First Test set:\")\n",
        "print(FL_test_sets_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RGFE7Iwxbjp",
        "outputId": "6b38e91c-1d37-4330-a8d9-81ddd9f17cd9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Training set:\n",
            "      mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "9490      5.170   1006.05      0.90       1.90 2013-11-09  MAC002068   \n",
            "9614      4.965   1006.45      0.86       3.61 2013-01-26  MAC002068   \n",
            "9453      7.360   1001.69      0.86       3.79 2012-12-17  MAC002068   \n",
            "9445      9.700   1010.05      0.80       3.17 2012-11-09  MAC002068   \n",
            "9448      9.040    991.49      0.86       4.93 2012-12-15  MAC002068   \n",
            "...         ...       ...       ...        ...        ...        ...   \n",
            "9588      7.380    991.64      0.82       2.94 2013-11-04  MAC002068   \n",
            "9589      5.565   1013.74      0.78       3.66 2013-03-04  MAC002068   \n",
            "9557      1.265   1011.47      0.88       2.08 2013-01-14  MAC002068   \n",
            "9451      6.775   1010.19      0.97       3.43 2012-11-24  MAC002068   \n",
            "9421     10.000   1001.79      0.81       6.54 2012-11-25  MAC002068   \n",
            "\n",
            "      energy_sum  \n",
            "9490      14.117  \n",
            "9614      11.782  \n",
            "9453      12.446  \n",
            "9445      24.090  \n",
            "9448      34.729  \n",
            "...          ...  \n",
            "9588      17.202  \n",
            "9589      18.535  \n",
            "9557      24.193  \n",
            "9451      27.590  \n",
            "9421      24.928  \n",
            "\n",
            "[156 rows x 7 columns]\n",
            "First Test set:\n",
            "      mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "9482      7.360   1011.21      0.82       4.13 2012-11-19  MAC002068   \n",
            "9566      0.100    997.28      0.86       4.16 2013-01-19  MAC002068   \n",
            "9528      2.100   1007.72      0.82       4.23 2013-01-23  MAC002068   \n",
            "9532      1.260   1013.39      0.65       4.99 2013-03-26  MAC002068   \n",
            "9571      6.915   1023.52      0.70       1.11 2013-02-16  MAC002068   \n",
            "...         ...       ...       ...        ...        ...        ...   \n",
            "9546     12.360   1016.56      0.89       4.42 2013-10-31  MAC002068   \n",
            "9509      8.335   1010.92      0.92       1.77 2013-11-18  MAC002068   \n",
            "9574      5.915   1037.52      0.77       3.33 2013-11-25  MAC002068   \n",
            "9518      3.345   1023.52      0.80       2.20 2013-03-03  MAC002068   \n",
            "9514      5.490   1021.19      0.90       0.93 2013-01-09  MAC002068   \n",
            "\n",
            "      energy_sum  \n",
            "9482      14.621  \n",
            "9566      20.322  \n",
            "9528      17.787  \n",
            "9532      11.331  \n",
            "9571      13.932  \n",
            "...          ...  \n",
            "9546      22.831  \n",
            "9509      21.116  \n",
            "9574      31.305  \n",
            "9518      31.994  \n",
            "9514      24.386  \n",
            "\n",
            "[67 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Centralised training, validation and test sets"
      ],
      "metadata": {
        "id": "MApq3bu5sigc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the split percentages\n",
        "train_percentage = 0.6  # 60% for training\n",
        "\n",
        "# Concatenate the DataFrames in the lists\n",
        "FL_train_set = pd.concat(train_data_list, ignore_index=True)\n",
        "FL_test_set = pd.concat(test_data_list, ignore_index=True)\n",
        "\n",
        "# Randomize the rows of FL_train_set and FL_test_set\n",
        "FL_train_set = FL_train_set.sample(frac=1, random_state=42)\n",
        "FL_test_set = FL_test_set.sample(frac=1, random_state=42)\n",
        "\n",
        "# Split FL_train_set into Central_train_data and Central_val_data\n",
        "Central_train_data, Central_val_data = train_test_split(FL_train_set, train_size=train_percentage, random_state=42)\n",
        "\n",
        "# Central_test_data is FL_test_set\n",
        "Central_test_data = FL_test_set.copy()\n",
        "\n",
        "# Display the sizes of the sets\n",
        "print(\"Central Training set size:\", len(Central_train_data))\n",
        "print(\"Central Validation set size:\", len(Central_val_data))\n",
        "print(\"Central Test set size:\", len(Central_test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQOhoHUdtI7e",
        "outputId": "629a3b5a-c54b-48ab-ae1b-014db7300b91"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Central Training set size: 11171\n",
            "Central Validation set size: 7448\n",
            "Central Test set size: 8065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying training, validation and test sets"
      ],
      "metadata": {
        "id": "fXufv5y-tO1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the training and test sets\n",
        "print(\"Training set:\")\n",
        "print(Central_train_data)\n",
        "print(\"Validation set:\")\n",
        "print(Central_val_data)\n",
        "print(\"Test set:\")\n",
        "print(Central_test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUNX3w0utFkh",
        "outputId": "fe54031c-7246-4ae2-882c-b2845fffa4ea"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "       mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "12318      2.825   1015.57      0.67       1.81 2013-03-14  MAC002601   \n",
            "15419      8.990   1034.64      0.87       1.23 2013-11-28  MAC001734   \n",
            "17920      2.295   1015.80      0.94       1.55 2013-01-10  MAC001776   \n",
            "14374     12.675    998.60      0.82       4.55 2013-11-02  MAC001628   \n",
            "9251       7.440   1009.77      0.73       5.66 2013-12-22  MAC001510   \n",
            "...          ...       ...       ...        ...        ...        ...   \n",
            "16810      5.515   1004.26      0.78       4.83 2013-11-21  MAC000323   \n",
            "4487       9.495   1011.94      0.85       4.15 2014-01-08  MAC003680   \n",
            "9839       4.140   1014.89      0.84       3.07 2013-12-29  MAC004179   \n",
            "17551      8.145   1005.53      0.90       2.00 2013-11-08  MAC000974   \n",
            "2416       8.310   1010.88      0.83       6.58 2013-12-30  MAC003553   \n",
            "\n",
            "       energy_sum  \n",
            "12318   36.984000  \n",
            "15419   35.806000  \n",
            "17920    3.163000  \n",
            "14374    5.485000  \n",
            "9251    31.064000  \n",
            "...           ...  \n",
            "16810   17.893000  \n",
            "4487    23.775000  \n",
            "9839   108.278001  \n",
            "17551    7.823000  \n",
            "2416    20.136000  \n",
            "\n",
            "[11171 rows x 7 columns]\n",
            "Validation set:\n",
            "       mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "13641      8.400    994.74      0.83       3.96 2013-11-05  MAC002249   \n",
            "6536       1.710   1014.56      0.68       2.80 2013-03-31  MAC003719   \n",
            "16661     10.325   1019.25      0.91       6.12 2013-12-15  MAC000323   \n",
            "10        -0.685    994.89      0.91       3.18 2013-01-20  MAC002068   \n",
            "3533       6.005   1021.59      0.90       1.29 2013-12-17  MAC003737   \n",
            "...          ...       ...       ...        ...        ...        ...   \n",
            "16062      6.015   1019.70      0.77       2.49 2013-02-15  MAC000778   \n",
            "14736     -0.490   1017.44      0.83       1.01 2013-01-17  MAC000002   \n",
            "13964     12.360   1016.56      0.89       4.42 2013-10-31  MAC000535   \n",
            "16052      3.570   1022.18      0.79       3.93 2012-12-10  MAC000778   \n",
            "3307       9.975   1026.48      0.86       1.86 2012-11-14  MAC000569   \n",
            "\n",
            "       energy_sum  \n",
            "13641      11.062  \n",
            "6536        9.353  \n",
            "16661      10.468  \n",
            "10         28.317  \n",
            "3533       10.111  \n",
            "...           ...  \n",
            "16062      23.203  \n",
            "14736       9.857  \n",
            "13964      35.639  \n",
            "16052      19.939  \n",
            "3307       23.730  \n",
            "\n",
            "[7448 rows x 7 columns]\n",
            "Test set:\n",
            "      mean_temp  pressure  humidity  windSpeed       time      LCLid  \\\n",
            "742       6.095   1002.38      0.85       2.83 2013-02-01  MAC003110   \n",
            "2126      8.245   1004.48      0.84       2.37 2012-10-30  MAC003805   \n",
            "4076      5.400   1016.89      0.82       4.00 2013-01-01  MAC001510   \n",
            "2941      5.335    997.47      0.77       3.17 2013-12-28  MAC001239   \n",
            "3872     -0.490   1017.44      0.83       1.01 2013-01-17  MAC003212   \n",
            "...         ...       ...       ...        ...        ...        ...   \n",
            "5226     12.360   1016.56      0.89       4.42 2013-10-31  MAC000713   \n",
            "5390      3.725   1015.96      0.75       4.73 2013-03-21  MAC002601   \n",
            "860      12.170   1008.74      0.90       5.74 2012-11-20  MAC003579   \n",
            "7603      6.780   1005.47      0.84       5.95 2013-01-28  MAC000974   \n",
            "7270      5.705   1017.66      0.76       4.31 2013-11-22  MAC000323   \n",
            "\n",
            "      energy_sum  \n",
            "742        9.137  \n",
            "2126       8.920  \n",
            "4076      31.079  \n",
            "2941       5.985  \n",
            "3872      20.007  \n",
            "...          ...  \n",
            "5226      19.472  \n",
            "5390      43.338  \n",
            "860       11.072  \n",
            "7603       6.383  \n",
            "7270      21.223  \n",
            "\n",
            "[8065 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Centralised Training"
      ],
      "metadata": {
        "id": "IVpGqOqfnfFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining features and target column"
      ],
      "metadata": {
        "id": "pe8CeDeXpYMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable\n",
        "Central_X_train = Central_train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_train = Central_train_data['energy_sum']\n",
        "\n",
        "# Separate features and target variable\n",
        "Central_X_val = Central_val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_val = Central_val_data['energy_sum']\n",
        "\n",
        "# Separate features and target variable\n",
        "Central_X_test = Central_test_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_test = Central_test_data['energy_sum']\n"
      ],
      "metadata": {
        "id": "aPxOdMTSpORn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing (scaler and imputer)"
      ],
      "metadata": {
        "id": "5FrtqxMEqk_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the datasets\n",
        "datasets = [\n",
        "    (Central_X_train, Central_y_train, \"Central Training\"),\n",
        "    (Central_X_val, Central_y_val, \"Central Validation\"),\n",
        "    (Central_X_test, Central_y_test, \"Central Test\")\n",
        "]\n",
        "\n",
        "# Preprocess each dataset\n",
        "for X, y, name in datasets:\n",
        "    # Impute missing values and scale features\n",
        "    X_scaled = StandardScaler().fit_transform(SimpleImputer(strategy='mean').fit_transform(X))\n",
        "\n",
        "    # Display dataset name and shape\n",
        "    print(f\"{name} Dataset:\")\n",
        "    print(\"Original Shape:\", X.shape)\n",
        "    print(\"Preprocessed Shape:\", X_scaled.shape)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNGXlzX_sH77",
        "outputId": "64beae0d-2b3c-4355-d5d0-979cd74835db"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Central Training Dataset:\n",
            "Original Shape: (11171, 4)\n",
            "Preprocessed Shape: (11171, 4)\n",
            "\n",
            "Central Validation Dataset:\n",
            "Original Shape: (7448, 4)\n",
            "Preprocessed Shape: (7448, 4)\n",
            "\n",
            "Central Test Dataset:\n",
            "Original Shape: (8065, 4)\n",
            "Preprocessed Shape: (8065, 4)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(Central_y_train.isnull().sum())\n",
        "print(Central_X_train.isnull().sum())\n",
        "\n",
        "# Replace missing values with mean (you can choose another strategy)\n",
        "Central_y_train.fillna(Central_y_train.mean(), inplace=True)\n",
        "Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(Central_y_train.isnull().sum())\n",
        "print(Central_X_train.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqoTpCpfHd8D",
        "outputId": "aba257a4-9760-42ec-b512-81c1069604cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "mean_temp    0\n",
            "pressure     0\n",
            "humidity     0\n",
            "windSpeed    0\n",
            "dtype: int64\n",
            "0\n",
            "mean_temp    0\n",
            "pressure     0\n",
            "humidity     0\n",
            "windSpeed    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
            "<ipython-input-21-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
            "<ipython-input-21-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
            "<ipython-input-21-e51efdbfd96c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model selection"
      ],
      "metadata": {
        "id": "pDqGsD-tGiAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # List of models to train\n",
        "# models = [\n",
        "#     LinearRegression(),\n",
        "#     DecisionTreeRegressor(),\n",
        "#     RandomForestRegressor(),\n",
        "#     GradientBoostingRegressor(),\n",
        "#     XGBRegressor(),\n",
        "#     LGBMRegressor(),\n",
        "#     CatBoostRegressor(),\n",
        "#     SVR(),\n",
        "#     MLPRegressor(),\n",
        "#     KNeighborsRegressor()\n",
        "# ]\n",
        "\n",
        "# # Dictionary to store model performances\n",
        "# model_performances = {}\n",
        "\n",
        "# # Train and evaluate each model on the validation set\n",
        "# for model in models:\n",
        "#     model_name = model.__class__.__name__\n",
        "#     print(f\"Training {model_name}...\")\n",
        "#     model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "#     # Predict on the validation set\n",
        "#     y_pred_val = model.predict(Central_X_val)\n",
        "#     # print(y_pred_val)\n",
        "\n",
        "#     # Calculate Mean Squared Error\n",
        "#     # mse_val = mean_squared_error(Central_y_val, y_pred_val)\n",
        "#     model_performances[model_name] = (Central_y_val-y_pred_val)\n",
        "#     print(model_performances[model_name])\n",
        "\n",
        "# # Find the best-performing model\n",
        "# best_model = min(model_performances, key=model_performances.get)\n",
        "# print(f\"Best-performing model: {best_model} (MSE: {model_performances[best_model]})\")\n"
      ],
      "metadata": {
        "id": "w9syWdcgGg9Y"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define models\n",
        "linear_model = LinearRegression()\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, objective='reg:squarederror', random_state=0)\n",
        "\n",
        "# Remove rows with missing values from both features and target variables\n",
        "Central_train_data_cleaned = Central_train_data.dropna()\n",
        "Central_X_train = Central_train_data_cleaned[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_train = Central_train_data_cleaned['energy_sum']\n",
        "\n",
        "# Drop rows with NaN values in the target variable in the validation set\n",
        "Central_val_data.dropna(subset=['energy_sum'], inplace=True)\n",
        "\n",
        "# Train linear model\n",
        "linear_model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Train random forest model\n",
        "rf_model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model.fit(Central_X_train, Central_y_train)\n"
      ],
      "metadata": {
        "id": "xZDeUas4R_tS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "0e617e8d-ab3e-4000-f55d-aa599a48f134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "             predictor=None, random_state=0, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple feedforward neural network(FNN)\n",
        "fnn_model = keras.Sequential([\n",
        "    layers.Input(shape=(4,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "fnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "fnn_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_data=(Central_X_val, Central_y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOApr5TMrh6g",
        "outputId": "9b6cb921-2969-4d1d-fcf5-b391f0c3c3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "501/501 [==============================] - 8s 4ms/step - loss: 7357.3325 - mse: 7357.3325 - val_loss: 434.7622 - val_mse: 434.7622\n",
            "Epoch 2/10\n",
            "501/501 [==============================] - 2s 4ms/step - loss: 420.3943 - mse: 420.3943 - val_loss: 437.6509 - val_mse: 437.6509\n",
            "Epoch 3/10\n",
            "501/501 [==============================] - 2s 4ms/step - loss: 421.2672 - mse: 421.2672 - val_loss: 437.2649 - val_mse: 437.2649\n",
            "Epoch 4/10\n",
            "501/501 [==============================] - 2s 5ms/step - loss: 422.0505 - mse: 422.0505 - val_loss: 435.8417 - val_mse: 435.8417\n",
            "Epoch 5/10\n",
            "501/501 [==============================] - 3s 5ms/step - loss: 421.1493 - mse: 421.1493 - val_loss: 434.3184 - val_mse: 434.3184\n",
            "Epoch 6/10\n",
            "501/501 [==============================] - 2s 5ms/step - loss: 421.1183 - mse: 421.1183 - val_loss: 448.0761 - val_mse: 448.0761\n",
            "Epoch 7/10\n",
            "501/501 [==============================] - 2s 4ms/step - loss: 421.0824 - mse: 421.0824 - val_loss: 433.8567 - val_mse: 433.8567\n",
            "Epoch 8/10\n",
            "501/501 [==============================] - 2s 4ms/step - loss: 422.7591 - mse: 422.7591 - val_loss: 434.1648 - val_mse: 434.1648\n",
            "Epoch 9/10\n",
            "501/501 [==============================] - 2s 4ms/step - loss: 423.5908 - mse: 423.5908 - val_loss: 435.8257 - val_mse: 435.8257\n",
            "Epoch 10/10\n",
            "501/501 [==============================] - 2s 4ms/step - loss: 422.6234 - mse: 422.6234 - val_loss: 439.1777 - val_mse: 439.1777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e957fdc3fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a simple RNN model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Create an RNN model\n",
        "simple_rnn_model = Sequential()\n",
        "simple_rnn_model.add(SimpleRNN(64, activation='relu', input_shape=(Central_X_train.shape[1], 1)))\n",
        "simple_rnn_model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "simple_rnn_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "simple_rnn_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_data=(Central_X_val, Central_y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BMpFLFqfXsd",
        "outputId": "ef7cca7a-15f4-4073-bca0-c6a7ade770f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "501/501 [==============================] - 7s 10ms/step - loss: 466.6749 - mse: 466.6749 - val_loss: 434.1538 - val_mse: 434.1538\n",
            "Epoch 2/10\n",
            "501/501 [==============================] - 3s 6ms/step - loss: 428.7281 - mse: 428.7281 - val_loss: 436.5609 - val_mse: 436.5609\n",
            "Epoch 3/10\n",
            "501/501 [==============================] - 4s 7ms/step - loss: 427.6934 - mse: 427.6934 - val_loss: 435.1342 - val_mse: 435.1342\n",
            "Epoch 4/10\n",
            "501/501 [==============================] - 4s 9ms/step - loss: 425.3786 - mse: 425.3786 - val_loss: 435.0079 - val_mse: 435.0079\n",
            "Epoch 5/10\n",
            "501/501 [==============================] - 6s 11ms/step - loss: 425.2627 - mse: 425.2627 - val_loss: 434.7839 - val_mse: 434.7839\n",
            "Epoch 6/10\n",
            "501/501 [==============================] - 5s 9ms/step - loss: 420.9419 - mse: 420.9419 - val_loss: 448.7663 - val_mse: 448.7663\n",
            "Epoch 7/10\n",
            "501/501 [==============================] - 3s 7ms/step - loss: 420.8588 - mse: 420.8588 - val_loss: 439.7771 - val_mse: 439.7771\n",
            "Epoch 8/10\n",
            "501/501 [==============================] - 3s 7ms/step - loss: 421.6550 - mse: 421.6550 - val_loss: 445.7986 - val_mse: 445.7986\n",
            "Epoch 9/10\n",
            "501/501 [==============================] - 5s 9ms/step - loss: 421.5675 - mse: 421.5675 - val_loss: 433.6360 - val_mse: 433.6360\n",
            "Epoch 10/10\n",
            "501/501 [==============================] - 4s 9ms/step - loss: 423.3574 - mse: 423.3574 - val_loss: 432.7701 - val_mse: 432.7701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e95775ac940>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Reshape data for LSTM input (samples, timesteps, features)\n",
        "num_timesteps = 1\n",
        "num_features = Central_X_train.shape[1]\n",
        "\n",
        "X_train_reshaped = Central_X_train.values.reshape((Central_X_train.shape[0], num_timesteps, num_features))\n",
        "X_val_reshaped = Central_X_val.values.reshape((Central_X_val.shape[0], num_timesteps, num_features))\n",
        "\n",
        "# Create an LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, activation='relu', input_shape=(num_timesteps, num_features)))\n",
        "lstm_model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_reshaped, Central_y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, Central_y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK9tFvolsTkV",
        "outputId": "26fd4b3a-397b-47b9-d9d6-4705a42e4245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "501/501 [==============================] - 5s 6ms/step - loss: 438.3049 - val_loss: 433.8857\n",
            "Epoch 2/10\n",
            "501/501 [==============================] - 3s 6ms/step - loss: 421.3251 - val_loss: 444.0783\n",
            "Epoch 3/10\n",
            "501/501 [==============================] - 3s 7ms/step - loss: 421.2768 - val_loss: 466.5037\n",
            "Epoch 4/10\n",
            "501/501 [==============================] - 5s 10ms/step - loss: 420.6349 - val_loss: 433.2047\n",
            "Epoch 5/10\n",
            "501/501 [==============================] - 3s 6ms/step - loss: 421.9791 - val_loss: 441.8762\n",
            "Epoch 6/10\n",
            "501/501 [==============================] - 3s 7ms/step - loss: 421.0463 - val_loss: 437.5651\n",
            "Epoch 7/10\n",
            "501/501 [==============================] - 3s 6ms/step - loss: 420.1023 - val_loss: 432.5734\n",
            "Epoch 8/10\n",
            "501/501 [==============================] - 5s 9ms/step - loss: 420.3820 - val_loss: 432.7023\n",
            "Epoch 9/10\n",
            "501/501 [==============================] - 4s 7ms/step - loss: 419.5619 - val_loss: 432.3607\n",
            "Epoch 10/10\n",
            "501/501 [==============================] - 3s 6ms/step - loss: 420.2477 - val_loss: 432.3139\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e95769e14b0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Evaluation function"
      ],
      "metadata": {
        "id": "cq6pYV2vajNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "def evaluate_model(model, X, y):\n",
        "    predictions = model.predict(X)\n",
        "    mse = mean_squared_error(y, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y, predictions)\n",
        "    r2 = r2_score(y, predictions)\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(\"Mean Squared Error:\", mse)\n",
        "    print(\"Root Mean Squared Error:\", rmse)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    print(\"R-squared:\", r2)\n",
        "    print(\"R-squared variance:\", r2*100)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "Nv4rwuSLaikH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Evalution Function"
      ],
      "metadata": {
        "id": "rOuTnMTmalw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models as trained earlier\n",
        "models = [\n",
        "    ('Linear Regression', linear_model),\n",
        "    ('Random Forest', rf_model),\n",
        "    ('XGBoost', xgb_model),\n",
        "    ('Neural Network', fnn_model),\n",
        "    ('RNN',simple_rnn_model),\n",
        "    ('LSTM',lstm_model)\n",
        "]\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models:\n",
        "    print(f'\\n{model_name}:')\n",
        "    if model_name == 'LSTM':\n",
        "        X_val_reshaped = Central_X_val.values.reshape((Central_X_val.shape[0], num_timesteps, num_features))\n",
        "        evaluate_model(model, X_val_reshaped, Central_y_val)\n",
        "    else:\n",
        "        evaluate_model(model, Central_X_val, Central_y_val)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALw9fSQTaTlC",
        "outputId": "3377397e-5658-4644-9e38-0563b54eb87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Linear Regression:\n",
            "Model: LinearRegression\n",
            "Mean Squared Error: 431.6772158559222\n",
            "Root Mean Squared Error: 20.7768432601279\n",
            "Mean Absolute Error: 13.603564854307745\n",
            "R-squared: 0.0045177866416201695\n",
            "R-squared variance: 0.45177866416201695\n",
            "\n",
            "\n",
            "Random Forest:\n",
            "Model: RandomForestRegressor\n",
            "Mean Squared Error: 438.707994809373\n",
            "Root Mean Squared Error: 20.94535735692693\n",
            "Mean Absolute Error: 13.664868603764097\n",
            "R-squared: -0.01169575240360654\n",
            "R-squared variance: -1.169575240360654\n",
            "\n",
            "\n",
            "XGBoost:\n",
            "Model: XGBRegressor\n",
            "Mean Squared Error: 438.3924033014123\n",
            "Root Mean Squared Error: 20.93782231516478\n",
            "Mean Absolute Error: 13.669758902878602\n",
            "R-squared: -0.010967973124732744\n",
            "R-squared variance: -1.0967973124732744\n",
            "\n",
            "\n",
            "Neural Network:\n",
            "167/167 [==============================] - 0s 2ms/step\n",
            "Model: Sequential\n",
            "Mean Squared Error: 439.17764744601584\n",
            "Root Mean Squared Error: 20.95656573596962\n",
            "Mean Absolute Error: 12.937968727007302\n",
            "R-squared: -0.012778808977041312\n",
            "R-squared variance: -1.2778808977041312\n",
            "\n",
            "\n",
            "RNN:\n",
            "167/167 [==============================] - 1s 3ms/step\n",
            "Model: Sequential\n",
            "Mean Squared Error: 432.77019068647843\n",
            "Root Mean Squared Error: 20.803129348405218\n",
            "Mean Absolute Error: 13.763680914553438\n",
            "R-squared: 0.0019972991952076358\n",
            "R-squared variance: 0.19972991952076358\n",
            "\n",
            "\n",
            "LSTM:\n",
            "167/167 [==============================] - 1s 3ms/step\n",
            "Model: Sequential\n",
            "Mean Squared Error: 432.3140736668485\n",
            "Root Mean Squared Error: 20.792163756253185\n",
            "Mean Absolute Error: 13.657132982054188\n",
            "R-squared: 0.0030491415523524745\n",
            "R-squared variance: 0.30491415523524745\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Create XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_xgb_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll_KLWBJv55X",
        "outputId": "fa7f4d99-3e6d-4a51-bc7a-2fcfca803389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   3.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   2.5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Should be doing"
      ],
      "metadata": {
        "id": "vDSikcPVt7Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gated Convolution Neural Network (GCNN)"
      ],
      "metadata": {
        "id": "PPpxpmwOpfav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class GatedCNN24(nn.Module):\n",
        "#     def __init__(self, input_channels, hidden_sizes):\n",
        "#         super(GatedCNN24, self).__init__()\n",
        "#         self.input_channels = input_channels\n",
        "#         self.hidden_sizes = hidden_sizes\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(self.input_channels, self.hidden_sizes[0], kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(self.hidden_sizes[0], self.hidden_sizes[1], kernel_size=3, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(self.hidden_sizes[1], self.hidden_sizes[2], kernel_size=3, padding=1)\n",
        "\n",
        "#         self.gate_conv1 = nn.Conv2d(self.input_channels, self.hidden_sizes[0], kernel_size=3, padding=1)\n",
        "#         self.gate_conv2 = nn.Conv2d(self.hidden_sizes[0], self.hidden_sizes[1], kernel_size=3, padding=1)\n",
        "#         self.gate_conv3 = nn.Conv2d(self.hidden_sizes[1], self.hidden_sizes[2], kernel_size=3, padding=1)\n",
        "\n",
        "#         self.fc1 = nn.Linear(self.hidden_sizes[2], 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Convolutional layers\n",
        "#         conv1_out = self.conv1(x)\n",
        "#         conv2_out = self.conv2(conv1_out)\n",
        "#         conv3_out = self.conv3(conv2_out)\n",
        "\n",
        "#         # Gating layers\n",
        "#         gate_conv1_out = torch.sigmoid(self.gate_conv1(x))\n",
        "#         gate_conv2_out = torch.sigmoid(self.gate_conv2(conv1_out))\n",
        "#         gate_conv3_out = torch.sigmoid(self.gate_conv3(conv2_out))\n",
        "\n",
        "#         # Element-wise multiplication (gating)\n",
        "#         gated_conv1 = conv1_out * gate_conv1_out\n",
        "#         gated_conv2 = conv2_out * gate_conv2_out\n",
        "#         gated_conv3 = conv3_out * gate_conv3_out\n",
        "\n",
        "#         # Sum along channels\n",
        "#         gated_sum = gated_conv1 + gated_conv2 + gated_conv3\n",
        "\n",
        "#         # Global average pooling\n",
        "#         pooled = torch.mean(gated_sum, dim=[2, 3])\n",
        "\n",
        "#         # Fully connected layer\n",
        "#         output = self.fc1(pooled)\n",
        "#         return output.squeeze()\n",
        "\n",
        "# class EnergyDataset(Dataset):\n",
        "#     def __init__(self, features, targets):\n",
        "#         self.features = features\n",
        "#         self.targets = targets\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.features)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         inputs = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
        "#         target = torch.tensor(self.targets.iloc[idx], dtype=torch.float32)\n",
        "#         return inputs, target\n",
        "\n",
        "# # Define hyperparameters\n",
        "# input_channels = 4  # Number of features\n",
        "# hidden_sizes = [10, 8, 1]\n",
        "# learning_rate = 0.001\n",
        "# num_epochs = 10\n",
        "# batch_size = 50\n",
        "\n",
        "# # Create DataLoader for training\n",
        "# train_dataset = EnergyDataset(Central_X_train, Central_y_train)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Create the model\n",
        "# GatedCNN_model = GatedCNN24(input_channels, hidden_sizes)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(GatedCNN_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     GatedCNN_model.train()  # Set the model in training mode\n",
        "#     for inputs, targets in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Reshape the input tensor to match expected shape [batch_size, channels, height, width]\n",
        "#         inputs = inputs.view(inputs.size(0), input_channels, 1, 1)\n",
        "\n",
        "#         outputs = GatedCNN_model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "NciAJdMzmNnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gated Neural Network model"
      ],
      "metadata": {
        "id": "gcM_OfWUsMvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Build the Gated Neural Network model\n",
        "# inputs = Input(shape=(4,))\n",
        "# dense_layer = Dense(16, activation='relu')(inputs)\n",
        "\n",
        "# # Gating mechanism\n",
        "# dense_gate = Dense(16, activation='sigmoid')(inputs)\n",
        "# gated_dense = Multiply()([dense_layer, dense_gate])\n",
        "\n",
        "# output = Dense(1, activation='linear')(gated_dense)\n",
        "\n",
        "# GNN_model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# # Compile the model\n",
        "# GNN_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# # Train the model\n",
        "# GNN_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "2pFnAbOBsL8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a dataset, a model, a training function, and a test function.\n"
      ],
      "metadata": {
        "id": "7QlrR4FIfyCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated learning"
      ],
      "metadata": {
        "id": "B1cZVyxpIupo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing FLower Client"
      ],
      "metadata": {
        "id": "_vWU1fCXVX94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flwr as fl\n",
        "\n",
        "from flwr.client import NumPyClient\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class XGBoostClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, X_train, y_train, X_val, y_val):\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.model.get_booster().get_dump()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        new_model = xgb.Booster(params=config, model_str=parameters[0])\n",
        "        self.model = new_model\n",
        "\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        return self.model.get_booster().get_dump()\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        new_model = xgb.Booster(params=config, model_str=parameters[0])\n",
        "        self.model = new_model\n",
        "\n",
        "        y_pred = self.model.predict(self.X_val)\n",
        "        accuracy = accuracy_score(self.y_val, y_pred)\n",
        "\n",
        "        return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "1i6mrPCqIxbc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the best hyperparameters in best_xgb_model\n",
        "best_hyperparameters = best_xgb_model.get_xgb_params()\n",
        "\n",
        "# Create an instance of your XGBoost model with the best hyperparameters\n",
        "best_xgb_model_instance = xgb.XGBRegressor(**best_hyperparameters)\n",
        "\n",
        "\n",
        "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "client_resources = None\n",
        "if DEVICE.type == \"cuda\":\n",
        "    client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# Define a function to create a virtual client\n",
        "def create_virtual_client(lclid):\n",
        "    client_data = FL_train_set[FL_train_set['LCLid'] == lclid]\n",
        "    X_client = client_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "    y_client = client_data['energy_sum']\n",
        "    return XGBoostClient(best_xgb_model_instance, X_client, y_client,  FL_test_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']], FL_test_set['energy_sum'])\n",
        "\n",
        "# Create a list of virtual clients\n",
        "virtual_clients = [create_virtual_client(lclid) for lclid in unique_lclids]\n",
        "\n",
        "# Define the client function for Flower simulation\n",
        "def client_fn(cid: str) -> XGBoostClient:\n",
        "    virtual_client = virtual_clients[int(cid)]\n",
        "    return virtual_client\n",
        "\n",
        "# Define the Flower simulation configuration\n",
        "config = fl.server.ServerConfig(num_rounds=5)\n",
        "\n",
        "# Start the Flower simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=len(virtual_clients),\n",
        "    config=config,\n",
        "    strategy=fl.server.strategy.FedAvg(),\n",
        "    client_resources=None,  # Adjust client resources as needed\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "jeqMcrjHZGtt",
        "outputId": "654d17dc-bc39-483c-a012-04eb4540450c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2023-08-14 20:08:03,665 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
            "2023-08-14 20:08:07,947\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-14 20:08:10,130 | app.py:180 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'memory': 7815023003.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3907511500.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'memory': 7815023003.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3907511500.0}\n",
            "INFO flwr 2023-08-14 20:08:10,139 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-14 20:08:10,143 | server.py:273 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=31666)\u001b[0m 2023-08-14 20:08:11.578945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "ERROR flwr 2023-08-14 20:08:14,323 | ray_client_proxy.py:72 | \u001b[36mray::launch_and_get_parameters()\u001b[39m (pid=31666, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 136, in launch_and_get_parameters\n",
            "    return maybe_call_get_parameters(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 163, in maybe_call_get_parameters\n",
            "    return client.get_parameters(get_parameters_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/app.py\", line 283, in _get_parameters\n",
            "    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore\n",
            "TypeError: XGBoostClient.get_parameters() got an unexpected keyword argument 'config'\n",
            "ERROR:flwr:\u001b[36mray::launch_and_get_parameters()\u001b[39m (pid=31666, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 136, in launch_and_get_parameters\n",
            "    return maybe_call_get_parameters(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 163, in maybe_call_get_parameters\n",
            "    return client.get_parameters(get_parameters_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/app.py\", line 283, in _get_parameters\n",
            "    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore\n",
            "TypeError: XGBoostClient.get_parameters() got an unexpected keyword argument 'config'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RayTaskError(TypeError)",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-7befdd84f2af>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Start the Flower simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m fl.simulation.start_simulation(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mclient_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnum_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvirtual_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     hist = _fl(\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialized_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialized_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/app.py\u001b[0m in \u001b[0;36m_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    215\u001b[0m ) -> History:\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"app_fit: losses_distributed %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses_distributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"app_fit: metrics_distributed_fit %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_distributed_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Initialize parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Initializing global parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_initial_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Evaluating initial parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36m_get_initial_parameters\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mrandom_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetParametersIns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mget_parameters_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Received initial parameters from one random client\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_parameters_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\u001b[0m in \u001b[0;36mget_parameters\u001b[0;34m(self, ins, timeout)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         return cast(\n\u001b[1;32m     75\u001b[0m             \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetParametersRes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\u001b[0m in \u001b[0;36mget_parameters\u001b[0;34m(self, ins, timeout)\u001b[0m\n\u001b[1;32m     68\u001b[0m         ).remote(self.client_fn, self.cid, ins)\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_paramseters_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2518\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2520\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2521\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::launch_and_get_parameters()\u001b[39m (pid=31666, ip=172.28.0.12)\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 136, in launch_and_get_parameters\n    return maybe_call_get_parameters(\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 163, in maybe_call_get_parameters\n    return client.get_parameters(get_parameters_ins)\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/app.py\", line 283, in _get_parameters\n    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore\nTypeError: XGBoostClient.get_parameters() got an unexpected keyword argument 'config'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "federated learning using X"
      ],
      "metadata": {
        "id": "tG7auHUdXb86"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "vDSikcPVt7Go",
        "PPpxpmwOpfav",
        "gcM_OfWUsMvC"
      ],
      "gpuType": "T4",
      "mount_file_id": "1oLHv4e-uSTEYGSiiJDsM6CuTErnsaA5X",
      "authorship_tag": "ABX9TyONf73noGKAT7UOEtnfQVT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}