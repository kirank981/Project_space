{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirank981/Project_space/blob/main/project_space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP5nJnDbJPYO"
      },
      "source": [
        "# Installing dependences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OravxoTjLhVa"
      },
      "source": [
        "Install the necessary packages for PyTorch (torch and torchvision) and Flower (flwr) and pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kpAFZGHjJOOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e2e067-2878-4593-ec40-ef79904ab020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQeKkPPLUkF"
      },
      "source": [
        "Import everything we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdmn7_KHLVVI"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "# from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # Import the train_test_split function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "!pip install catboost\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from tensorflow.keras.layers import Conv2D, Multiply, Input\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Dense, Multiply\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFfbJxnLxF7"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6l5c_376GqC"
      },
      "source": [
        "Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSv4I6ji-3of"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n4a987x-7kd"
      },
      "source": [
        "Setting the path to the location of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ1t9RTF91ZQ"
      },
      "outputs": [],
      "source": [
        "# Define the path to daily dataset folder\n",
        "daily_dataset_path = Path('/content/drive/MyDrive/Federated learning implementation/dataset/dataset_archive/daily_dataset/daily_dataset')\n",
        "\n",
        "# Define the path to daily dataset folder\n",
        "weather_daily_dataset_path = Path('/content/drive/MyDrive/Federated learning implementation/dataset/dataset_archive/weather_daily_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2owcEz6M9K"
      },
      "source": [
        "## Loading daily data\n",
        "(of energy consumption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdpP9rwQBqJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initializing list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Loop through the CSV files and reading them into dataframes\n",
        "for i in range(1):\n",
        "    filename = f'block_{i}.csv'\n",
        "    df = pd.read_csv(daily_dataset_path / filename)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenating all the dataframes into a single dataframe\n",
        "energy_daily_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# NRATM(Not Required At The Moment)\n",
        "# # Group the data by LCLid and create a dictionary of dataframes\n",
        "# grouped_data = dict(tuple(energy_daily_data.groupby('LCLid')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBAsp3Ky51Bn"
      },
      "source": [
        "Loading data using file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeJMbTTY0YZY"
      },
      "outputs": [],
      "source": [
        "# Loading data from a specific CSV file\n",
        "specific_file_data = pd.read_csv(daily_dataset_path / 'LCLid_3nos.csv')\n",
        "\n",
        "# Displaying data\n",
        "print(specific_file_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0b947PkSfrG"
      },
      "source": [
        "### Loading required data\n",
        "Creating a DataFrame that have only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMsi4uZITUt8"
      },
      "outputs": [],
      "source": [
        "selected_column = ['LCLid','energy_sum','day']\n",
        "# energy_daily_selected=energy_daily_data[selected_column] (check)\n",
        "energy_daily_selected=specific_file_data[selected_column]\n",
        "# print(energy_daily_selected)\n",
        "\n",
        "# Group the data by LCLid and create a dictionary of dataframes, allowing to access each dataframe separately using the LCLid as the key\n",
        "grouped_data_selected = dict(tuple(energy_daily_selected.groupby('LCLid')))\n",
        "# Display the data for 'MAC000002'\n",
        "print('\\n')\n",
        "print('Data report of MAC000002')\n",
        "print(grouped_data_selected['MAC000002'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZumIN7H_Zt-"
      },
      "source": [
        "## Loading daily weather data\n",
        "\n",
        "Creating a 'day' column that stores only the date values from 'time' column\n",
        "(for linking weather dataset 'day' with daily dataset 'day')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch2Azrah7Cuj"
      },
      "outputs": [],
      "source": [
        "# Load the weather dataset into a DataFrame\n",
        "weather_daily_data = pd.read_csv(weather_daily_dataset_path)\n",
        "\n",
        "# Convert the 'time' column to datetime format\n",
        "weather_daily_data['time'] = pd.to_datetime(weather_daily_data['time'])\n",
        "\n",
        "# Calculate the mean temperature for each day and store it in a new column 'mean_temp'\n",
        "weather_daily_data['mean_temp'] = (weather_daily_data['temperatureMax'] + weather_daily_data['temperatureMin']) / 2\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(weather_daily_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUsgV_WfSTVp"
      },
      "source": [
        "### Loading required data\n",
        "Creating a DataFrame that have only the required data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRn0x1J8RlPl"
      },
      "outputs": [],
      "source": [
        "# Create a new DataFrame with selected columns\n",
        "selected_columns = ['mean_temp', 'pressure', 'humidity', 'windSpeed', 'time']\n",
        "weather_selected = weather_daily_data[selected_columns]\n",
        "\n",
        "# Print the new dataset\n",
        "print(weather_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi3pXETWEhtL"
      },
      "source": [
        "# Dataset with Household energy consumption values and weather values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlEy0Fm_JaT5"
      },
      "source": [
        "## For all households"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqUqCBNEHb8P"
      },
      "outputs": [],
      "source": [
        "# Convert 'time' column in weather_selected to datetime objects\n",
        "weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n",
        "\n",
        "# Create a list to store the merged data DataFrames\n",
        "merged_data_list = []\n",
        "\n",
        "# Iterate through each LCLid in energy_daily_selected\n",
        "for lclid, data in grouped_data_selected.items():\n",
        "    # Convert 'day' column in current LCLid data to datetime objects\n",
        "    data['day'] = pd.to_datetime(data['day'])\n",
        "\n",
        "    # Merge the current LCLid data with weather_selected based on the common date values\n",
        "    merged_data_lclid = pd.merge(weather_selected, data, left_on='time', right_on='day', how='inner')\n",
        "\n",
        "    # Drop the redundant 'day' column from the merged data\n",
        "    merged_data_lclid.drop(columns=['day'], inplace=True)\n",
        "\n",
        "    # Append the merged data to the merged_data_list\n",
        "    merged_data_list.append(merged_data_lclid)\n",
        "\n",
        "# Concatenate the merged data DataFrames in the list\n",
        "merged_data = pd.concat(merged_data_list, ignore_index=True)\n",
        "\n",
        "# Display the merged dataset\n",
        "print(merged_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq2mLT2oJWB5"
      },
      "source": [
        "## For one household"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBmTUuggiWZZ"
      },
      "outputs": [],
      "source": [
        "print(merged_data[merged_data['LCLid'] == \"MAC000002\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVelsQ53430r"
      },
      "source": [
        "# Splitting dataset to training and testing sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw5aaS7xkIWr"
      },
      "source": [
        "## Preprocessing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWB2ilpvq0_-"
      },
      "source": [
        "### Counting amount of data for each LCLid (data available for each household)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_1dDvPeoXFD"
      },
      "outputs": [],
      "source": [
        "# Convert 'time' column in weather_selected to datetime objects\n",
        "weather_selected['time'] = pd.to_datetime(weather_selected['time'])\n",
        "\n",
        "# Merge the data into merged_data DataFrame as described in your previous code\n",
        "\n",
        "# Count the number of data rows for each LCLid\n",
        "lclid_data_counts = merged_data['LCLid'].value_counts()\n",
        "\n",
        "# Display the counts for each LCLid\n",
        "print(\"Data row counts for each LCLid:\")\n",
        "print(lclid_data_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIjz5a0irBRy"
      },
      "source": [
        "### Identifiting the no of households with insufficient amount of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlmWA354pGgO"
      },
      "outputs": [],
      "source": [
        "# Count the number of data rows for each LCLid\n",
        "lclid_data_counts = merged_data['LCLid'].value_counts()\n",
        "\n",
        "\n",
        "# Get the total number of unique LCLid values\n",
        "total_lclids = len(lclid_data_counts)\n",
        "\n",
        "# Count the number of LCLid values with less than 100 data rows\n",
        "count_less_than_100 = (lclid_data_counts < 100).sum()\n",
        "\n",
        "# Display the count of LCLid values with less than 100 data rows\n",
        "print(\"Number of LCLid values with less than 100 data rows:\", count_less_than_100)\n",
        "\n",
        "# Display the total number of unique LCLid values\n",
        "print(\"Total number of unique LCLid values:\", total_lclids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA9X4z0irXP7"
      },
      "source": [
        "### Removing the households with less data, from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VuTC5T5pphX"
      },
      "outputs": [],
      "source": [
        "# Get the list of LCLid values with less than 100 data rows\n",
        "lclids_to_remove = lclid_data_counts[lclid_data_counts < 100].index\n",
        "\n",
        "# Remove rows corresponding to LCLid values with less than 100 data rows\n",
        "filtered_data = merged_data[~merged_data['LCLid'].isin(lclids_to_remove)]\n",
        "\n",
        "# also removing rows with NaN values in the dataset\n",
        "filtered_data = filtered_data.dropna()\n",
        "\n",
        "# Display the filtered data\n",
        "print(filtered_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlfwiI3Rrm1X"
      },
      "source": [
        "## Splitting the dataset to train and test\n",
        " where the split ratio(70:30) is made with every househould's data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjDCXMXoizi1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the split percentages\n",
        "train_percentage = 0.6 # 60% for training\n",
        "val_percentage = 0.3   # 30% for validation\n",
        "test_percentage = 0.3  # 30% for testing\n",
        "min_data_points = 10   # Minimum number of data points required for an LCLid\n",
        "\n",
        "# Create lists to store DataFrames for training, validation, and testing\n",
        "train_data_list = []\n",
        "val_data_list = []\n",
        "test_data_list = []\n",
        "\n",
        "# Iterate through each unique LCLid and split the data based on train_percentage, val_percentage, and test_percentage\n",
        "unique_lclids = filtered_data['LCLid'].unique()\n",
        "for lclid in unique_lclids:\n",
        "    lclid_data = filtered_data[filtered_data['LCLid'] == lclid]\n",
        "\n",
        "    # Check if there are sufficient data points for the current LCLid\n",
        "    if len(lclid_data) >= min_data_points:\n",
        "        # Split the data for the current LCLid into training, validation, and test sets\n",
        "        train_data_lclid, remaining_data_lclid = train_test_split(lclid_data, train_size=train_percentage, shuffle=False)\n",
        "        val_data_lclid, test_data_lclid = train_test_split(remaining_data_lclid, train_size=val_percentage/(val_percentage + test_percentage), shuffle=False)\n",
        "\n",
        "        # Randomize the rows within each subset\n",
        "        train_data_lclid = train_data_lclid.sample(frac=1, random_state=42)\n",
        "        val_data_lclid = val_data_lclid.sample(frac=1, random_state=42)\n",
        "        test_data_lclid = test_data_lclid.sample(frac=1, random_state=42)\n",
        "\n",
        "        # Append to the train_data_list, val_data_list, and test_data_list\n",
        "        train_data_list.append(train_data_lclid)\n",
        "        val_data_list.append(val_data_lclid)\n",
        "        test_data_list.append(test_data_lclid)\n",
        "\n",
        "# Concatenate the DataFrames in the lists\n",
        "FL_train_set = pd.concat(train_data_list, ignore_index=True)\n",
        "FL_val_set = pd.concat(val_data_list, ignore_index=True)\n",
        "FL_test_set = pd.concat(test_data_list, ignore_index=True)\n",
        "\n",
        "# Store the training, validation, and test sets in separate lists\n",
        "FL_train_sets_list = train_data_list\n",
        "FL_val_sets_list = val_data_list\n",
        "FL_test_sets_list = test_data_list\n",
        "\n",
        "# Display the training, validation, and test sets\n",
        "print(\"Training set:\")\n",
        "print(FL_train_set)\n",
        "print(\"Validation set:\")\n",
        "print(FL_val_set)\n",
        "print(\"Test set:\")\n",
        "print(FL_test_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAZQP_IFjqJd"
      },
      "source": [
        "### For FL training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RGFE7Iwxbjp"
      },
      "outputs": [],
      "source": [
        "# Display the first training set\n",
        "print(\"First Training set:\")\n",
        "print(FL_train_sets_list[0])\n",
        "print(\"First Validation set:\")\n",
        "print(FL_val_sets_list[0])\n",
        "print(\"First Test set:\")\n",
        "print(FL_test_sets_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc6V-SGNCaAv"
      },
      "outputs": [],
      "source": [
        "# Display the first training set\n",
        "print(\"First Training set:\")\n",
        "print(FL_train_sets_list[1])\n",
        "print(\"First Validation set:\")\n",
        "print(FL_val_sets_list[1])\n",
        "print(\"First Test set:\")\n",
        "print(FL_test_sets_list[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MApq3bu5sigc"
      },
      "source": [
        "### Centralised training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQOhoHUdtI7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Copy FL training set to central training set\n",
        "Central_train_data = FL_train_set.copy()\n",
        "\n",
        "# Copy FL validation set to central validation set\n",
        "Central_val_data = FL_val_set.copy()\n",
        "\n",
        "# Copy FL test set to central test set\n",
        "Central_test_data = FL_test_set.copy()\n",
        "\n",
        "# Display the sizes of the sets\n",
        "print(\"Central Training set size:\", len(Central_train_data))\n",
        "print(\"Central Validation set size:\", len(Central_val_data))\n",
        "print(\"Central Test set size:\", len(Central_test_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXufv5y-tO1x"
      },
      "source": [
        "Displaying training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUNX3w0utFkh"
      },
      "outputs": [],
      "source": [
        "# Display the training and test sets\n",
        "print(\"Training set:\")\n",
        "print(Central_train_data)\n",
        "print(\"Validation set:\")\n",
        "print(Central_val_data)\n",
        "print(\"Test set:\")\n",
        "print(Central_test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpGqOqfnfFW"
      },
      "source": [
        "# Centralised Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe8CeDeXpYMQ"
      },
      "source": [
        "## Defining features and target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPxOdMTSpORn"
      },
      "outputs": [],
      "source": [
        "# Separate features and target variable\n",
        "Central_X_train = Central_train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_train = Central_train_data['energy_sum']\n",
        "\n",
        "# Separate features and target variable\n",
        "Central_X_val = Central_val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_val = Central_val_data['energy_sum']\n",
        "\n",
        "# Separate features and target variable\n",
        "Central_X_test = Central_test_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_test = Central_test_data['energy_sum']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrtqxMEqk_2"
      },
      "source": [
        "## Preprocessing (scaler and imputer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNGXlzX_sH77"
      },
      "outputs": [],
      "source": [
        "# Define the datasets\n",
        "datasets = [\n",
        "    (Central_X_train, Central_y_train, \"Central Training\"),\n",
        "    (Central_X_val, Central_y_val, \"Central Validation\"),\n",
        "    (Central_X_test, Central_y_test, \"Central Test\")\n",
        "]\n",
        "\n",
        "# Preprocess each dataset\n",
        "for X, y, name in datasets:\n",
        "    # Impute missing values and scale features\n",
        "    X_scaled = StandardScaler().fit_transform(SimpleImputer(strategy='mean').fit_transform(X))\n",
        "\n",
        "    # Display dataset name and shape\n",
        "    print(f\"{name} Dataset:\")\n",
        "    print(\"Original Shape:\", X.shape)\n",
        "    print(\"Preprocessed Shape:\", X_scaled.shape)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqoTpCpfHd8D"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(Central_y_train.isnull().sum())\n",
        "print(Central_X_train.isnull().sum())\n",
        "\n",
        "# Replace missing values with mean (you can choose another strategy)\n",
        "Central_y_train.fillna(Central_y_train.mean(), inplace=True)\n",
        "Central_X_train.fillna(Central_X_train.mean(), inplace=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(Central_y_train.isnull().sum())\n",
        "print(Central_X_train.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDqGsD-tGiAH"
      },
      "source": [
        "## Model selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9syWdcgGg9Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # List of models to train\n",
        "# models = [\n",
        "#     LinearRegression(),\n",
        "#     DecisionTreeRegressor(),\n",
        "#     RandomForestRegressor(),\n",
        "#     GradientBoostingRegressor(),\n",
        "#     XGBRegressor(),\n",
        "#     LGBMRegressor(),\n",
        "#     CatBoostRegressor(),\n",
        "#     SVR(),\n",
        "#     MLPRegressor(),\n",
        "#     KNeighborsRegressor()\n",
        "# ]\n",
        "\n",
        "# # Dictionary to store model performances\n",
        "# model_performances = {}\n",
        "\n",
        "# # Train and evaluate each model on the validation set\n",
        "# for model in models:\n",
        "#     model_name = model.__class__.__name__\n",
        "#     print(f\"Training {model_name}...\")\n",
        "#     model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "#     # Predict on the validation set\n",
        "#     y_pred_val = model.predict(Central_X_val)\n",
        "#     # print(y_pred_val)\n",
        "\n",
        "#     # Calculate Mean Squared Error\n",
        "#     # mse_val = mean_squared_error(Central_y_val, y_pred_val)\n",
        "#     model_performances[model_name] = (Central_y_val-y_pred_val)\n",
        "#     print(model_performances[model_name])\n",
        "\n",
        "# # Find the best-performing model\n",
        "# best_model = min(model_performances, key=model_performances.get)\n",
        "# print(f\"Best-performing model: {best_model} (MSE: {model_performances[best_model]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZDeUas4R_tS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define models\n",
        "linear_model = LinearRegression()\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, objective='reg:squarederror', random_state=0)\n",
        "\n",
        "# Remove rows with missing values from both features and target variables\n",
        "Central_train_data_cleaned = Central_train_data.dropna()\n",
        "Central_X_train = Central_train_data_cleaned[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "Central_y_train = Central_train_data_cleaned['energy_sum']\n",
        "\n",
        "# Drop rows with NaN values in the target variable in the validation set\n",
        "Central_val_data.dropna(subset=['energy_sum'], inplace=True)\n",
        "\n",
        "# Train linear model\n",
        "linear_model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Train random forest model\n",
        "rf_model.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model.fit(Central_X_train, Central_y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOApr5TMrh6g"
      },
      "outputs": [],
      "source": [
        "# Build a simple feedforward neural network(FNN)\n",
        "fnn_model = keras.Sequential([\n",
        "    layers.Input(shape=(4,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "fnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "fnn_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_data=(Central_X_val, Central_y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BMpFLFqfXsd"
      },
      "outputs": [],
      "source": [
        "# Building a simple RNN model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Create an RNN model\n",
        "simple_rnn_model = Sequential()\n",
        "simple_rnn_model.add(SimpleRNN(64, activation='relu', input_shape=(Central_X_train.shape[1], 1)))\n",
        "simple_rnn_model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "simple_rnn_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "simple_rnn_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_data=(Central_X_val, Central_y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK9tFvolsTkV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Reshape data for LSTM input (samples, timesteps, features)\n",
        "num_timesteps = 1\n",
        "num_features = Central_X_train.shape[1]\n",
        "\n",
        "X_train_reshaped = Central_X_train.values.reshape((Central_X_train.shape[0], num_timesteps, num_features))\n",
        "X_val_reshaped = Central_X_val.values.reshape((Central_X_val.shape[0], num_timesteps, num_features))\n",
        "\n",
        "# Create an LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, activation='relu', input_shape=(num_timesteps, num_features)))\n",
        "lstm_model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_reshaped, Central_y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, Central_y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq6pYV2vajNg"
      },
      "source": [
        "### Defining the Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv4rwuSLaikH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Store the evaluation results for each model\n",
        "eval_results = []\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    predictions = model.predict(X)\n",
        "    mse = mean_squared_error(y, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y, predictions)\n",
        "    r2 = r2_score(y, predictions)\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(\"Mean Squared Error:\", mse)\n",
        "    print(\"Root Mean Squared Error:\", rmse)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    print(\"R-squared:\", r2)\n",
        "    print(\"R-squared variance:\", r2*100)\n",
        "    print()\n",
        "    return mse, rmse, mae, r2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOuTnMTmalw5"
      },
      "source": [
        "### Using the Evalution Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALw9fSQTaTlC"
      },
      "outputs": [],
      "source": [
        "# Define the models as trained earlier\n",
        "models = [\n",
        "    ('Linear Regression', linear_model),\n",
        "    ('Random Forest', rf_model),\n",
        "    ('XGBoost', xgb_model),\n",
        "    ('Neural Network', fnn_model),\n",
        "    ('RNN',simple_rnn_model),\n",
        "    ('LSTM',lstm_model)\n",
        "]\n",
        "\n",
        "# Store the evaluation results for each model\n",
        "eval_results = []\n",
        "\n",
        "# Evaluate each model and store results\n",
        "for model_name, model in models:\n",
        "    if model_name == 'LSTM':\n",
        "        X_val_reshaped = Central_X_val.values.reshape((Central_X_val.shape[0], num_timesteps, num_features))\n",
        "        mse, rmse, mae, r2 = evaluate_model(model, X_val_reshaped, Central_y_val)\n",
        "    else:\n",
        "        mse, rmse, mae, r2 = evaluate_model(model, Central_X_val, Central_y_val)\n",
        "\n",
        "    eval_results.append((model_name, mse, rmse, mae, r2))\n",
        "\n",
        "# Convert eval_results to a numpy array for plotting\n",
        "eval_results = np.array(eval_results)\n",
        "\n",
        "# Plot the evaluation metrics for each model\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(models))\n",
        "metrics = ['MSE', 'RMSE', 'MAE', 'R-squared']\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.bar(x, eval_results[:, i + 1].astype(float), tick_label=eval_results[:, 0])\n",
        "    plt.title(metric)\n",
        "    plt.xticks(rotation=15)\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1rDcfEIXR2Q"
      },
      "source": [
        "### Hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll_KLWBJv55X"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Create XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_xgb_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best_xgb_model.fit(FL_val_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']],FL_val_set['energy_sum'])\n",
        "best_xgb_model.fit(Central_X_train, Central_y_train)"
      ],
      "metadata": {
        "id": "EXsPBLka-Xqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxA3oa-qR9NI"
      },
      "source": [
        "# Federated Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing more modules"
      ],
      "metadata": {
        "id": "9Ri8FEKkbpXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import bz2\n",
        "import shutil\n",
        "\n",
        "!nvidia-smi\n",
        "!pip install matplotlib scikit-learn tqdm torch torchmetrics torchsummary xgboost\n",
        "!pip install -U \"flwr-nightly[simulation]\"\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchmetrics import Accuracy, MeanSquaredError\n",
        "from tqdm import trange, tqdm\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Flower related modules for federated XGBoost\n",
        "import flwr as fl\n",
        "from flwr.common.typing import Parameters\n",
        "from collections import OrderedDict\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from flwr.common import NDArray, NDArrays\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "ulFjWJZEGep9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_xgbtree(tree: XGBRegressor, n_tree: int) -> None:\n",
        "    \"\"\"Visualize the built xgboost tree.\"\"\"\n",
        "    xgb.plot_tree(tree, num_trees=n_tree)\n",
        "    plt.rcParams[\"figure.figsize\"] = [50, 10]\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def construct_tree(\n",
        "    dataset: Dataset, label: NDArray, n_estimators: int, tree_type: str\n",
        ") -> XGBRegressor:\n",
        "    \"\"\"Construct a xgboost tree form tabular dataset.\"\"\"\n",
        "    if tree_type == \"REG\":\n",
        "      tree = xgb.XGBRegressor(\n",
        "          objective=\"reg:squarederror\",\n",
        "          learning_rate=0.1,\n",
        "          max_depth=8,\n",
        "          n_estimators=n_estimators,\n",
        "          subsample=0.8,\n",
        "          colsample_bylevel=1,\n",
        "          colsample_bynode=1,\n",
        "          colsample_bytree=1,\n",
        "          alpha=5,\n",
        "          gamma=5,\n",
        "          num_parallel_tree=1,\n",
        "          min_child_weight=1,\n",
        "      )\n",
        "\n",
        "    tree.fit(dataset, label)\n",
        "    return tree\n",
        "\n",
        "def construct_tree_from_loader(\n",
        "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
        ") -> XGBRegressor:\n",
        "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
        "    for dataset in dataset_loader:\n",
        "        data, label = dataset[0], dataset[1]\n",
        "    return construct_tree(data, label, n_estimators, tree_type)\n",
        "\n",
        "def single_tree_prediction(\n",
        "    tree: XGBRegressor, n_tree: int, dataset: NDArray\n",
        ") -> Optional[NDArray]:\n",
        "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
        "    ensemble.\"\"\"\n",
        "    num_t = len(tree.get_booster().get_dump())\n",
        "    if n_tree > num_t:\n",
        "        print(\n",
        "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    return tree.predict(\n",
        "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
        "    )\n",
        "\n",
        "def tree_encoding(\n",
        "    trainloader: DataLoader,\n",
        "    client_trees: Union[\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Tuple[XGBRegressor, int]],\n",
        "    ],\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> Optional[Tuple[NDArray, NDArray]]:\n",
        "    \"\"\"Transform the tabular dataset into prediction results using the\n",
        "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
        "    if trainloader is None:\n",
        "        return None\n",
        "\n",
        "    for local_dataset in trainloader:\n",
        "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
        "\n",
        "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
        "    x_train_enc = np.array(x_train_enc, copy=True)\n",
        "\n",
        "    temp_trees: Any = None\n",
        "    if isinstance(client_trees, list) is False:\n",
        "        temp_trees = [client_trees[0]] * client_num\n",
        "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
        "        temp_trees = [client_trees[0][0]] * client_num\n",
        "    else:\n",
        "        cids = []\n",
        "        temp_trees = []\n",
        "        for i, _ in enumerate(client_trees):\n",
        "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
        "            cids.append(client_trees[i][1])  # type: ignore\n",
        "        sorted_index = np.argsort(np.asarray(cids))\n",
        "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
        "\n",
        "    for i, _ in enumerate(temp_trees):\n",
        "        for j in range(client_tree_num):\n",
        "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
        "                temp_trees[i], j, x_train\n",
        "            )\n",
        "\n",
        "    x_train_enc32: Any = np.float32(x_train_enc)\n",
        "    y_train32: Any = np.float32(y_train)\n",
        "\n",
        "    x_train_enc32, y_train32 = torch.from_numpy(\n",
        "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
        "    ), torch.from_numpy(\n",
        "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
        "    )\n",
        "    return x_train_enc32, y_train32"
      ],
      "metadata": {
        "id": "rYBOwdF7GwR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_train = FL_train_set\n",
        "reg_test = FL_test_set\n",
        "task_type = \"REG\"\n",
        "\n",
        "# Select the downloaded training and test dataset\n",
        "if task_type == \"REG\":\n",
        "    train = reg_train\n",
        "    test = reg_test\n",
        "    data_train = reg_train\n",
        "    data_test = reg_test\n",
        "\n",
        "print(\"Task type selected is: \" + task_type)\n",
        "print(\"Training dataset is: \")\n",
        "print(train)\n",
        "print(\"Test dataset is: \")\n",
        "print(test)"
      ],
      "metadata": {
        "id": "x4tqBX7BHSXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeDataset(Dataset):\n",
        "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
        "        self.labels = labels\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
        "        label = self.labels[idx]\n",
        "        data = self.data[idx, :]\n",
        "        sample = {0: data, 1: label}\n",
        "        return sample\n",
        "\n",
        "task_type = \"REG\"\n",
        "\n",
        "# Load and preprocess regression data\n",
        "X_train = FL_train_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "y_train = FL_train_set['energy_sum'].values\n",
        "\n",
        "X_test = FL_test_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "y_test = FL_test_set['energy_sum'].values\n",
        "\n",
        "X_train.flags.writeable = True\n",
        "y_train.flags.writeable = True\n",
        "\n",
        "X_test.flags.writeable = True\n",
        "y_test.flags.writeable = True\n",
        "\n",
        "print(\"Feature dimension of the dataset:\", X_train.shape[1])\n",
        "print(\"Size of the trainset:\", X_train.shape[0])\n",
        "print(\"Size of the testset:\", X_test.shape[0])\n",
        "assert X_train.shape[1] == X_test.shape[1]\n",
        "\n",
        "trainset = TreeDataset(np.array(X_train, copy=True), np.array(y_train, copy=True))\n",
        "testset = TreeDataset(np.array(X_test, copy=True), np.array(y_test, copy=True))\n",
        "print(trainset)\n",
        "print(testset)\n"
      ],
      "metadata": {
        "id": "gOilCVu1HVja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(\n",
        "    dataset: Dataset, partition: str, batch_size: Union[int, str]\n",
        ") -> DataLoader:\n",
        "    if batch_size == \"whole\":\n",
        "        batch_size = len(dataset)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, pin_memory=True, shuffle=(partition == \"train\")\n",
        "    )\n",
        "\n",
        "def do_fl_partitioning(\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    pool_size: int,\n",
        "    batch_size: Union[int, str],\n",
        "    val_ratio: float = 0.0,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
        "    partition_size = len(trainset) // pool_size\n",
        "    lengths = [partition_size] * pool_size\n",
        "    if sum(lengths) != len(trainset):\n",
        "        lengths[-1] = len(trainset) - sum(lengths[0:-1])\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(0))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = int(len(ds) * val_ratio)\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(0))\n",
        "        trainloaders.append(get_dataloader(ds_train, \"train\", batch_size))\n",
        "        if len_val != 0:\n",
        "            valloaders.append(get_dataloader(ds_val, \"val\", batch_size))\n",
        "        else:\n",
        "            valloaders = None\n",
        "    testloader = get_dataloader(testset, \"test\", batch_size)\n",
        "\n",
        "    # columns_to_extract = ['mean_temp', 'pressure', 'humidity', 'windSpeed', 'energy_sum']\n",
        "    # trainloaders = [get_dataloader(dataset, \"train\", batch_size) for dataset in FL_train_sets_list]\n",
        "\n",
        "    # trainloaders=FL_train_sets_list[['mean_temp', 'pressure', 'humidity', 'windSpeed','energy_sum']]\n",
        "    # valloaders=FL_val_sets_list[['mean_temp', 'pressure', 'humidity', 'windSpeed','energy_sum']]\n",
        "    # testloader=FL_test_sets_list[['mean_temp', 'pressure', 'humidity', 'windSpeed','energy_sum']]\n",
        "\n",
        "\n",
        "    return trainloaders, valloaders, testloader"
      ],
      "metadata": {
        "id": "Wgf71XTUH0l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of clients participated in the federated learning\n",
        "client_num = 2\n",
        "\n",
        "# The number of XGBoost trees in the tree ensemble that will be built for each client\n",
        "client_tree_num = 500 // client_num"
      ],
      "metadata": {
        "id": "108M9aycH49T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(X_train)\n",
        "print(FL_train_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']])\n",
        "global_tree = construct_tree(X_train, y_train, client_tree_num, task_type)\n",
        "preds_train = global_tree.predict(X_train)\n",
        "preds_test = global_tree.predict(X_test)\n",
        "\n",
        "result_train = mean_squared_error(y_train, preds_train)\n",
        "result_test = mean_squared_error(y_test, preds_test)\n",
        "print(\"Global XGBoost Training MSE: %f\" % (result_train))\n",
        "print(\"Global XGBoost Testing MSE: %f\" % (result_test))\n",
        "# print(y_train)\n",
        "# print(y_test)\n",
        "# print(X_train)\n",
        "\n",
        "print(global_tree)\n"
      ],
      "metadata": {
        "id": "-HM8v1lYH8V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client_trees_comparison = []\n",
        "trainloaders, _, testloader = do_fl_partitioning(\n",
        "    trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
        ")\n",
        "for i, trainloader in enumerate(trainloaders):\n",
        "    for local_dataset in trainloader:\n",
        "        local_X_train, local_y_train = local_dataset[0], local_dataset[1]\n",
        "        tree = construct_tree(local_X_train, local_y_train, client_tree_num, task_type)\n",
        "        client_trees_comparison.append(tree)\n",
        "\n",
        "        preds_train = client_trees_comparison[-1].predict(local_X_train)\n",
        "        preds_test = client_trees_comparison[-1].predict(X_test)\n",
        "\n",
        "        result_train = mean_squared_error(local_y_train, preds_train)\n",
        "        result_test = mean_squared_error(y_test, preds_test)\n",
        "        print(\"Local Client %d XGBoost Training MSE: %f\" % (i, result_train))\n",
        "        print(\"Local Client %d XGBoost Testing MSE: %f\" % (i, result_test))\n"
      ],
      "metadata": {
        "id": "6Hki4Uc_H-xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_channel: int = 64) -> None:\n",
        "        super(CNN, self).__init__()\n",
        "        n_out = 1\n",
        "        self.task_type = task_type\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            1, n_channel, kernel_size=client_tree_num, stride=client_tree_num, padding=0\n",
        "        )\n",
        "        self.layer_direct = nn.Linear(n_channel * client_num, n_out)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()\n",
        "        self.Identity = nn.Identity()\n",
        "\n",
        "        # Add weight initialization\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ReLU(self.conv1d(x))\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.Identity(self.layer_direct(x))\n",
        "        return x\n",
        "\n",
        "    def get_weights(self) -> fl.common.NDArrays:\n",
        "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
        "        return [\n",
        "            np.array(val.cpu().numpy(), copy=True)\n",
        "            for _, val in self.state_dict().items()\n",
        "        ]\n",
        "\n",
        "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
        "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "        layer_dict = {}\n",
        "        for k, v in zip(self.state_dict().keys(), weights):\n",
        "            if v.ndim != 0:\n",
        "                layer_dict[k] = torch.Tensor(np.array(v, copy=True))\n",
        "        state_dict = OrderedDict(layer_dict)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(\n",
        "    task_type: str,\n",
        "    net: CNN,\n",
        "    trainloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_iterations: int,\n",
        "    log_progress: bool = True,\n",
        ") -> Tuple[float, float, int]:\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    # optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-6)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
        "\n",
        "    def cycle(iterable):\n",
        "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
        "        while True:\n",
        "            for x in iterable:\n",
        "                yield x\n",
        "\n",
        "    # Train the network\n",
        "    net.train()\n",
        "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
        "    pbar = (\n",
        "        tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f\"TRAIN\")\n",
        "        if log_progress\n",
        "        else iter(cycle(trainloader))\n",
        "    )\n",
        "\n",
        "    # Unusually, this training is formulated in terms of number of updates/iterations/batches processed\n",
        "    # by the network. This will be helpful later on, when partitioning the data across clients: resulting\n",
        "    # in differences between dataset sizes and hence inconsistent numbers of updates per 'epoch'.\n",
        "    for i, data in zip(range(num_iterations), pbar):\n",
        "        tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(tree_outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collected training loss and accuracy statistics\n",
        "        total_loss += loss.item()\n",
        "        n_samples += labels.size(0)\n",
        "\n",
        "        if task_type == \"REG\":\n",
        "            mse = MeanSquaredError()(outputs, labels.type(torch.int))\n",
        "            total_result += mse * labels.size(0)\n",
        "\n",
        "        if log_progress:\n",
        "             if task_type == \"REG\":\n",
        "                pbar.set_postfix(\n",
        "                    {\n",
        "                        \"train_loss\": total_loss / n_samples,\n",
        "                        \"train_mse\": total_result / n_samples,\n",
        "                    }\n",
        "                )\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return total_loss / n_samples, total_result / n_samples, n_samples\n",
        "\n",
        "\n",
        "def test(\n",
        "    task_type: str,\n",
        "    net: CNN,\n",
        "    testloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    log_progress: bool = True,\n",
        ") -> Tuple[float, float, int]:\n",
        "    \"\"\"Evaluates the network on test data.\"\"\"\n",
        "    if task_type == \"REG\":\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
        "        for data in pbar:\n",
        "            tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(tree_outputs)\n",
        "\n",
        "            # Collected testing loss and accuracy statistics\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            n_samples += labels.size(0)\n",
        "\n",
        "            if task_type == \"REG\":\n",
        "                mse = MeanSquaredError()(outputs.cpu(), labels.type(torch.int).cpu())\n",
        "                total_result += mse * labels.size(0)\n",
        "\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return total_loss / n_samples, total_result / n_samples, n_samples"
      ],
      "metadata": {
        "id": "ZxnV5c3IIBUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower client\n",
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "    FitRes,\n",
        "    GetPropertiesIns,\n",
        "    GetPropertiesRes,\n",
        "    GetParametersIns,\n",
        "    GetParametersRes,\n",
        "    Status,\n",
        "    Code,\n",
        "    parameters_to_ndarrays,\n",
        "    ndarrays_to_parameters,\n",
        ")"
      ],
      "metadata": {
        "id": "siMTvAMaIFaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_encoding_loader(\n",
        "    dataloader: DataLoader,\n",
        "    batch_size: int,\n",
        "    client_trees: Union[\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Tuple[XGBRegressor, int]],\n",
        "    ],\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> DataLoader:\n",
        "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
        "    if encoding is None:\n",
        "        return None\n",
        "    data, labels = encoding\n",
        "    tree_dataset = TreeDataset(data, labels)\n",
        "    return get_dataloader(tree_dataset, \"tree\", batch_size)\n",
        "\n",
        "class FL_Client(fl.client.Client):\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_type: str,\n",
        "        trainloader: DataLoader,\n",
        "        valloader: DataLoader,\n",
        "        client_tree_num: int,\n",
        "        client_num: int,\n",
        "        cid: str,\n",
        "        log_progress: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a client for training `network.Net` on tabular dataset.\n",
        "        \"\"\"\n",
        "        self.task_type = task_type\n",
        "        self.cid = cid\n",
        "        self.tree = construct_tree_from_loader(trainloader, client_tree_num, task_type)\n",
        "        self.trainloader_original = trainloader\n",
        "        self.valloader_original = valloader\n",
        "        self.trainloader = None\n",
        "        self.valloader = None\n",
        "        self.client_tree_num = client_tree_num\n",
        "        self.client_num = client_num\n",
        "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "        self.log_progress = log_progress\n",
        "\n",
        "        # instantiate model\n",
        "        self.net = CNN()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_properties(self, ins: GetPropertiesIns) -> GetPropertiesRes:\n",
        "        return GetPropertiesRes(properties=self.properties)\n",
        "\n",
        "    def get_parameters(\n",
        "        self, ins: GetParametersIns\n",
        "    ) -> Tuple[\n",
        "        GetParametersRes,  Tuple[XGBRegressor, int]\n",
        "    ]:\n",
        "        return [\n",
        "            GetParametersRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=ndarrays_to_parameters(self.net.get_weights()),\n",
        "            ),\n",
        "            (self.tree, int(self.cid)),\n",
        "        ]\n",
        "\n",
        "    def set_parameters(\n",
        "        self,\n",
        "        parameters: Tuple[\n",
        "            Parameters,\n",
        "            Union[\n",
        "                Tuple[XGBRegressor, int],\n",
        "                List[Tuple[XGBRegressor, int]],\n",
        "            ],\n",
        "        ],\n",
        "    ) ->  Union[\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Tuple[XGBRegressor, int]],\n",
        "    ]:\n",
        "        self.net.set_weights(parameters_to_ndarrays(parameters[0]))\n",
        "        return parameters[1]\n",
        "\n",
        "    def fit(self, fit_params: FitIns) -> FitRes:\n",
        "        # Process incoming request to train\n",
        "        num_iterations = fit_params.config[\"num_iterations\"]\n",
        "        batch_size = fit_params.config[\"batch_size\"]\n",
        "        aggregated_trees = self.set_parameters(fit_params.parameters)\n",
        "\n",
        "        if type(aggregated_trees) is list:\n",
        "            print(\"Client \" + self.cid + \": received\", len(aggregated_trees), \"trees\")\n",
        "        else:\n",
        "            print(\"Client \" + self.cid + \": only had its own tree\")\n",
        "        self.trainloader = tree_encoding_loader(\n",
        "            self.trainloader_original,\n",
        "            batch_size,\n",
        "            aggregated_trees,\n",
        "            self.client_tree_num,\n",
        "            self.client_num,\n",
        "        )\n",
        "        self.valloader = tree_encoding_loader(\n",
        "            self.valloader_original,\n",
        "            batch_size,\n",
        "            aggregated_trees,\n",
        "            self.client_tree_num,\n",
        "            self.client_num,\n",
        "        )\n",
        "\n",
        "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
        "        num_iterations = num_iterations or len(self.trainloader)\n",
        "\n",
        "        # Train the model\n",
        "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
        "        self.net.to(self.device)\n",
        "        train_loss, train_result, num_examples = train(\n",
        "            self.task_type,\n",
        "            self.net,\n",
        "            self.trainloader,\n",
        "            device=self.device,\n",
        "            num_iterations=num_iterations,\n",
        "            log_progress=self.log_progress,\n",
        "        )\n",
        "        print(\n",
        "            f\"Client {self.cid}: training round complete, {num_examples} examples processed\"\n",
        "        )\n",
        "\n",
        "        # Return training information: model, number of examples processed and metrics\n",
        "        if self.task_type == \"REG\":\n",
        "            return FitRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=self.get_parameters(fit_params.config),\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"loss\": train_loss, \"mse\": train_result},\n",
        "            )\n",
        "\n",
        "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
        "        # Process incoming request to evaluate\n",
        "        self.set_parameters(eval_params.parameters)\n",
        "\n",
        "        # Evaluate the model\n",
        "        self.net.to(self.device)\n",
        "        loss, result, num_examples = test(\n",
        "            self.task_type,\n",
        "            self.net,\n",
        "            self.valloader,\n",
        "            device=self.device,\n",
        "            log_progress=self.log_progress,\n",
        "        )\n",
        "\n",
        "        # Return evaluation information\n",
        "        if self.task_type == \"REG\":\n",
        "            print(\n",
        "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, mse={result:.4f}\"\n",
        "            )\n",
        "            return EvaluateRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                loss=loss,\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"mse\": result},\n",
        "            )\n"
      ],
      "metadata": {
        "id": "JDFHpSFmIpDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower server\n",
        "import functools\n",
        "from flwr.server.strategy import FedXgbNnAvg\n",
        "from flwr.server.app import ServerConfig\n",
        "\n",
        "import timeit\n",
        "from logging import DEBUG, INFO\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from flwr.common import DisconnectRes, Parameters, ReconnectIns, Scalar\n",
        "from flwr.common.logger import log\n",
        "from flwr.common.typing import GetParametersIns\n",
        "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.history import History\n",
        "from flwr.server.strategy import Strategy\n",
        "from flwr.server.server import (\n",
        "    reconnect_clients,\n",
        "    reconnect_client,\n",
        "    fit_clients,\n",
        "    fit_client,\n",
        "    _handle_finished_future_after_fit,\n",
        "    evaluate_clients,\n",
        "    evaluate_client,\n",
        "    _handle_finished_future_after_evaluate,\n",
        ")\n",
        "\n",
        "FitResultsAndFailures = Tuple[\n",
        "    List[Tuple[ClientProxy, FitRes]],\n",
        "    List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "]\n",
        "EvaluateResultsAndFailures = Tuple[\n",
        "    List[Tuple[ClientProxy, EvaluateRes]],\n",
        "    List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
        "]"
      ],
      "metadata": {
        "id": "NS0N-9TeIrCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FL_Server(fl.server.Server):\n",
        "    \"\"\"Flower server.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, *, client_manager: ClientManager, strategy: Optional[Strategy] = None\n",
        "    ) -> None:\n",
        "        self._client_manager: ClientManager = client_manager\n",
        "        self.parameters: Parameters = Parameters(\n",
        "            tensors=[], tensor_type=\"numpy.ndarray\"\n",
        "        )\n",
        "        self.strategy: Strategy = strategy\n",
        "        self.max_workers: Optional[int] = None\n",
        "\n",
        "\n",
        "    def fit(self, num_rounds: int, timeout: Optional[float]) -> History:\n",
        "        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n",
        "        history = History()\n",
        "\n",
        "        # Initialize parameters\n",
        "        log(INFO, \"Initializing global parameters\")\n",
        "        self.parameters = self._get_initial_parameters(timeout=timeout)\n",
        "\n",
        "        log(INFO, \"Evaluating initial parameters\")\n",
        "        res = self.strategy.evaluate(0, parameters=self.parameters)\n",
        "        if res is not None:\n",
        "            log(\n",
        "                INFO,\n",
        "                \"initial parameters (loss, other metrics): %s, %s\",\n",
        "                res,\n",
        "            )\n",
        "            history.add_loss_centralized(server_round=0, loss=res[0])\n",
        "            history.add_metrics_centralized(server_round=0, metrics=res[1])\n",
        "\n",
        "        # Run federated learning for num_rounds\n",
        "        log(INFO, \"FL starting\")\n",
        "        start_time = timeit.default_timer()\n",
        "\n",
        "        for current_round in range(1, num_rounds + 1):\n",
        "            # Train model and replace previous global model\n",
        "            res_fit = self.fit_round(server_round=current_round, timeout=timeout)\n",
        "            if res_fit:\n",
        "                parameters_prime, _, _ = res_fit  # fit_metrics_aggregated\n",
        "                if parameters_prime:\n",
        "                    self.parameters = parameters_prime\n",
        "\n",
        "            # Evaluate model using strategy implementation\n",
        "            res_cen = self.strategy.evaluate(current_round, parameters=self.parameters)\n",
        "            if res_cen is not None:\n",
        "                loss_cen, metrics_cen = res_cen\n",
        "                log(\n",
        "                    INFO,\n",
        "                    \"fit progress: (%s, %s, %s, %s)\",\n",
        "                    current_round,\n",
        "                    loss_cen,\n",
        "                    metrics_cen,\n",
        "                    timeit.default_timer() - start_time,\n",
        "                )\n",
        "                history.add_loss_centralized(server_round=current_round, loss=loss_cen)\n",
        "                history.add_metrics_centralized(\n",
        "                    server_round=current_round, metrics=metrics_cen\n",
        "                )\n",
        "\n",
        "            # Evaluate model on a sample of available clients\n",
        "            res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
        "            if res_fed:\n",
        "                loss_fed, evaluate_metrics_fed, _ = res_fed\n",
        "                if loss_fed:\n",
        "                    history.add_loss_distributed(\n",
        "                        server_round=current_round, loss=loss_fed\n",
        "                    )\n",
        "                    history.add_metrics_distributed(\n",
        "                        server_round=current_round, metrics=evaluate_metrics_fed\n",
        "                    )\n",
        "\n",
        "        # Bookkeeping\n",
        "        end_time = timeit.default_timer()\n",
        "        elapsed = end_time - start_time\n",
        "        log(INFO, \"FL finished in %s\", elapsed)\n",
        "        return history\n",
        "\n",
        "    def evaluate_round(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        timeout: Optional[float],\n",
        "    ) -> Optional[\n",
        "        Tuple[Optional[float], Dict[str, Scalar], EvaluateResultsAndFailures]\n",
        "    ]:\n",
        "        \"\"\"Validate current global model on a number of clients.\"\"\"\n",
        "\n",
        "        # Get clients and their respective instructions from strategy\n",
        "        client_instructions = self.strategy.configure_evaluate(\n",
        "            server_round=server_round,\n",
        "            parameters=self.parameters,\n",
        "            client_manager=self._client_manager,\n",
        "        )\n",
        "        if not client_instructions:\n",
        "            log(INFO, \"evaluate_round %s: no clients selected, cancel\", server_round)\n",
        "            return None\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"evaluate_round %s: strategy sampled %s clients (out of %s)\",\n",
        "            server_round,\n",
        "            len(client_instructions),\n",
        "            self._client_manager.num_available(),\n",
        "        )\n",
        "\n",
        "        # Collect `evaluate` results from all clients participating in this round\n",
        "        results, failures = evaluate_clients(\n",
        "            client_instructions,\n",
        "            max_workers=self.max_workers,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"evaluate_round %s received %s results and %s failures\",\n",
        "            server_round,\n",
        "            len(results),\n",
        "            len(failures),\n",
        "        )\n",
        "\n",
        "        # Aggregate the evaluation results\n",
        "        aggregated_result: Tuple[\n",
        "            Optional[float],\n",
        "            Dict[str, Scalar],\n",
        "        ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
        "\n",
        "        loss_aggregated, metrics_aggregated = aggregated_result\n",
        "        return loss_aggregated, metrics_aggregated, (results, failures)\n",
        "\n",
        "    def fit_round(\n",
        "          self,\n",
        "          server_round: int,\n",
        "          timeout: Optional[float],\n",
        "      ) -> Optional[\n",
        "          Tuple[\n",
        "              Optional[\n",
        "                  Tuple[\n",
        "                      Parameters,\n",
        "                      Union[\n",
        "                          Tuple[XGBRegressor, int],\n",
        "                          List[\n",
        "                              Tuple[XGBRegressor, int]\n",
        "                          ],\n",
        "                      ],\n",
        "                  ]\n",
        "              ],\n",
        "              Dict[str, Scalar],\n",
        "              FitResultsAndFailures,\n",
        "          ]\n",
        "      ]:\n",
        "        \"\"\"Perform a single round of federated averaging.\"\"\"\n",
        "\n",
        "        # Get clients and their respective instructions from strategy\n",
        "        client_instructions = self.strategy.configure_fit(\n",
        "            server_round=server_round,\n",
        "            parameters=self.parameters,\n",
        "            client_manager=self._client_manager,\n",
        "        )\n",
        "\n",
        "        if not client_instructions:\n",
        "            log(INFO, \"fit_round %s: no clients selected, cancel\", server_round)\n",
        "            return None\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"fit_round %s: strategy sampled %s clients (out of %s)\",\n",
        "            server_round,\n",
        "            len(client_instructions),\n",
        "            self._client_manager.num_available(),\n",
        "        )\n",
        "\n",
        "        # Collect `fit` results from all clients participating in this round\n",
        "        results, failures = fit_clients(\n",
        "            client_instructions=client_instructions,\n",
        "            max_workers=self.max_workers,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"fit_round %s received %s results and %s failures\",\n",
        "            server_round,\n",
        "            len(results),\n",
        "            len(failures),\n",
        "        )\n",
        "\n",
        "        # Aggregate training results\n",
        "        NN_aggregated: Parameters\n",
        "        trees_aggregated: Union[\n",
        "            Tuple[XGBRegressor, int],\n",
        "            List[ Tuple[XGBRegressor, int]],\n",
        "        ]\n",
        "        metrics_aggregated: Dict[str, Scalar]\n",
        "        aggregated, metrics_aggregated = self.strategy.aggregate_fit(\n",
        "            server_round, results, failures\n",
        "        )\n",
        "        NN_aggregated, trees_aggregated = aggregated[0], aggregated[1]\n",
        "\n",
        "        if type(trees_aggregated) is list:\n",
        "            print(\"Server side aggregated\", len(trees_aggregated), \"trees.\")\n",
        "        else:\n",
        "            print(\"Server side did not aggregate trees.\")\n",
        "\n",
        "        return (\n",
        "            [NN_aggregated, trees_aggregated],\n",
        "            metrics_aggregated,\n",
        "            (results, failures),\n",
        "        )\n",
        "\n",
        "    def _get_initial_parameters(\n",
        "        self, timeout: Optional[float]\n",
        "      ) -> Tuple[Parameters, Tuple[XGBRegressor, int]]:\n",
        "        \"\"\"Get initial parameters from one of the available clients.\"\"\"\n",
        "\n",
        "        # Server-side parameter initialization\n",
        "        parameters: Optional[Parameters] = self.strategy.initialize_parameters(\n",
        "            client_manager=self._client_manager\n",
        "        )\n",
        "        if parameters is not None:\n",
        "            log(INFO, \"Using initial parameters provided by strategy\")\n",
        "            return parameters\n",
        "\n",
        "        # Get initial parameters from one of the clients\n",
        "        log(INFO, \"Requesting initial parameters from one random client\")\n",
        "        random_client = self._client_manager.sample(1)[0]\n",
        "        ins = GetParametersIns(config={})\n",
        "        get_parameters_res_tree = random_client.get_parameters(ins=ins, timeout=timeout)\n",
        "        parameters = [get_parameters_res_tree[0].parameters, get_parameters_res_tree[1]]\n",
        "        log(INFO, \"Received initial parameters from one random client\")\n",
        "\n",
        "        return parameters\n"
      ],
      "metadata": {
        "id": "pbGqqV8cIuMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_layers(model: nn.Module) -> None:\n",
        "    print(model)\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "eval_fl_loss=[]\n",
        "eval_fl_mse=[]\n",
        "def serverside_eval(\n",
        "    server_round: int,\n",
        "    parameters: Tuple[\n",
        "        Parameters,\n",
        "        Union[\n",
        "            Tuple[XGBRegressor, int],\n",
        "            List[Tuple[XGBRegressor, int]],\n",
        "        ],\n",
        "    ],\n",
        "    config: Dict[str, Scalar],\n",
        "    task_type: str,\n",
        "    testloader: DataLoader,\n",
        "    batch_size: int,\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"An evaluation function for centralized/serverside evaluation over the entire test set.\"\"\"\n",
        "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = \"cpu\"\n",
        "    model = CNN()\n",
        "    # print_model_layers(model)\n",
        "\n",
        "    model.set_weights(parameters_to_ndarrays(parameters[0]))\n",
        "    model.to(device)\n",
        "\n",
        "    trees_aggregated = parameters[1]\n",
        "    testloader = tree_encoding_loader(\n",
        "        testloader, batch_size, trees_aggregated, client_tree_num, client_num\n",
        "    )\n",
        "    loss, result, _ = test(\n",
        "        task_type, model, testloader, device=device, log_progress=False\n",
        "    )\n",
        "\n",
        "    if task_type == \"REG\":\n",
        "        print(f\"Evaluation on the server: test_loss={loss:.4f}, test_mse={result:.4f}\")\n",
        "        eval_fl_loss.append(loss)\n",
        "        eval_fl_mse.append(result)\n",
        "        return loss, {\"mse\": result}\n",
        "\n",
        "\n",
        "def start_experiment(\n",
        "    task_type: str,\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    num_rounds: int = 5,\n",
        "    client_tree_num: int = 50,\n",
        "    client_pool_size: int = 5,\n",
        "    num_iterations: int = 100,\n",
        "    fraction_fit: float = 1.0,\n",
        "    min_fit_clients: int = 2,\n",
        "    batch_size: int = 32,\n",
        "    val_ratio: float = 0.1,\n",
        ") -> History:\n",
        "    client_resources = {\"num_cpus\": 0.5}  # 2 clients per CPU\n",
        "\n",
        "    # Partition the dataset into subsets reserved for each client.\n",
        "    # - 'val_ratio' controls the proportion of the (local) client reserved as a local test set\n",
        "    # (good for testing how the final model performs on the client's local unseen data)\n",
        "    trainloaders, valloaders, testloader = do_fl_partitioning(\n",
        "        trainset,\n",
        "        testset,\n",
        "        batch_size=\"whole\",\n",
        "        pool_size=client_pool_size,\n",
        "        val_ratio=val_ratio,\n",
        "    )\n",
        "    print(\n",
        "        f\"Data partitioned across {client_pool_size} clients\"\n",
        "        f\" and {val_ratio} of local dataset reserved for validation.\"\n",
        "    )\n",
        "\n",
        "    # Configure the strategy\n",
        "    def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
        "        print(f\"Configuring round {server_round}\")\n",
        "        return {\n",
        "            \"num_iterations\": num_iterations,\n",
        "            \"batch_size\": batch_size,\n",
        "        }\n",
        "\n",
        "    # FedXgbNnAvg\n",
        "    strategy = FedXgbNnAvg(\n",
        "        fraction_fit=fraction_fit,\n",
        "        fraction_evaluate=fraction_fit if val_ratio > 0.0 else 0.0,\n",
        "        min_fit_clients=min_fit_clients,\n",
        "        min_evaluate_clients=min_fit_clients,\n",
        "        min_available_clients=client_pool_size,  # all clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        on_evaluate_config_fn=(lambda r: {\"batch_size\": batch_size}),\n",
        "        evaluate_fn=functools.partial(\n",
        "            serverside_eval,\n",
        "            task_type=task_type,\n",
        "            testloader=testloader,\n",
        "            batch_size=batch_size,\n",
        "            client_tree_num=client_tree_num,\n",
        "            client_num=client_pool_size,\n",
        "        ),\n",
        "        accept_failures=False,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"FL experiment configured for {num_rounds} rounds with {client_pool_size} clients in the pool.\"\n",
        "    )\n",
        "    print(\n",
        "        f\"FL round will proceed with {fraction_fit * 100}% of clients sampled, at least {min_fit_clients}.\"\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str) -> fl.client.Client:\n",
        "        \"\"\"Creates a federated learning client\"\"\"\n",
        "        if val_ratio > 0.0 and val_ratio <= 1.0:\n",
        "            return FL_Client(\n",
        "                task_type,\n",
        "                trainloaders[int(cid)],\n",
        "                valloaders[int(cid)],\n",
        "                client_tree_num,\n",
        "                client_pool_size,\n",
        "                cid,\n",
        "                log_progress=False,\n",
        "            )\n",
        "        else:\n",
        "            return FL_Client(\n",
        "                task_type,\n",
        "                trainloaders[int(cid)],\n",
        "                None,\n",
        "                client_tree_num,\n",
        "                client_pool_size,\n",
        "                cid,\n",
        "                log_progress=False,\n",
        "            )\n",
        "\n",
        "    # Start the simulation\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        server=FL_Server(client_manager=SimpleClientManager(), strategy=strategy),\n",
        "        num_clients=client_pool_size,\n",
        "        client_resources=client_resources,\n",
        "        config=ServerConfig(num_rounds=num_rounds),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "    print(history)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xJPjGwB0IyIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_experiment(\n",
        "    task_type=task_type,\n",
        "    trainset=trainset,\n",
        "    testset=testset,\n",
        "    num_rounds=2,\n",
        "    client_tree_num=client_tree_num,\n",
        "    client_pool_size=client_num,\n",
        "    num_iterations=100,\n",
        "    batch_size=64,\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=1,\n",
        "    val_ratio=0.0,\n",
        ")"
      ],
      "metadata": {
        "id": "mfHdv8WBI2Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing evaluation results"
      ],
      "metadata": {
        "id": "n_Ixyxh1I3L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('loss')\n",
        "print('\\n')\n",
        "print(eval_fl_loss)\n",
        "print('\\n')\n",
        "print('mse')\n",
        "print(eval_fl_mse)\n"
      ],
      "metadata": {
        "id": "ZNLXgIZgKJLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FL evaluation results\n",
        "fl_rounds = list(range(len(eval_fl_loss)))  # Number of rounds for FL evaluation\n",
        "fl_loss = eval_fl_loss\n",
        "fl_mse = [mse.item() for mse in eval_fl_mse]\n",
        "\n",
        "# Plot FL evaluation results against each round\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(fl_rounds, fl_loss, label='FL Loss')\n",
        "plt.title('FL Evaluation Loss')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(fl_rounds, fl_mse, label='FL MSE')\n",
        "plt.title('FL Evaluation MSE')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZJpPmtwunWdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWzlItEfnWRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Personal Experiments"
      ],
      "metadata": {
        "id": "AcAwekLwJOSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBKw06SHQohf"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from types import SimpleNamespace\n",
        "\n",
        "# # Define the XGBoost model creation function\n",
        "# def define_xgboost_model(hyperparameters):\n",
        "#     return xgb.XGBRegressor(**hyperparameters)\n",
        "\n",
        "\n",
        "# # Helper function to update the local model parameters\n",
        "# def set_parameters(model, parameters):\n",
        "#     model.set_params(**parameters)\n",
        "\n",
        "# # Helper function to get the local model parameters\n",
        "# def get_parameters(model):\n",
        "#     return model.get_xgb_params()\n",
        "\n",
        "# # Flower Client class\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, xgb_model, train_data, val_data):\n",
        "#         self.xgb_model = xgb_model\n",
        "#         self.train_data = train_data\n",
        "#         self.val_data = val_data\n",
        "\n",
        "#     def get_parameters(self, config):\n",
        "#         return self.xgb_model.get_booster().save_raw()\n",
        "\n",
        "#     def set_parameters(self, parameters):\n",
        "#         booster = self.xgb_model.get_booster()\n",
        "#         booster.load_model(parameters)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Train the XGBoost model on the client's data\n",
        "#         features = self.train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.train_data['energy_sum'].values\n",
        "#         self.xgb_model.fit(features, target)\n",
        "\n",
        "#         # Return the updated model parameters\n",
        "#         return self.get_parameters(config)  # Pass config argument here\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Evaluate the XGBoost model on the client's validation data\n",
        "#         features = self.val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.val_data['energy_sum'].values\n",
        "#         predictions = self.xgb_model.predict(features)\n",
        "#         loss = mean_squared_error(target, predictions)\n",
        "\n",
        "#         return loss, len(self.val_data), {}\n",
        "\n",
        "# # Create an instance of XGBoost model\n",
        "# hyperparameters = {'objective': 'reg:squarederror', 'learning_rate': 0.1, 'max_depth': 3}\n",
        "# xgboost_model = define_xgboost_model(hyperparameters)\n",
        "\n",
        "# # Fit the xgboost_model using training data\n",
        "# xgboost_model.fit(\n",
        "#     FL_train_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']],\n",
        "#     FL_train_set['energy_sum']\n",
        "# )\n",
        "\n",
        "# # Create FlowerClient instances\n",
        "# NUM_CLIENTS = len(FL_train_sets_list)\n",
        "# clients = [client_fn(cid) for cid in range(NUM_CLIENTS)]\n",
        "\n",
        "# # Convert XGBoost model parameters to a dictionary\n",
        "# model_params = xgboost_model.get_xgb_params()\n",
        "\n",
        "# # Before starting the simulation loop, call fit on each client\n",
        "# for client in clients:\n",
        "#     # Fit each client using the XGBoost model parameters\n",
        "#     client.fit(model_params, config)\n",
        "\n",
        "# # Start simulation\n",
        "# fl.simulation.start_simulation(\n",
        "#     client_fn=lambda cid: clients[int(cid)],\n",
        "#     num_clients=NUM_CLIENTS,\n",
        "#     config=fl.server.ServerConfig(num_rounds=5),\n",
        "#     strategy=strategy,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWRIPsKl7D5m"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# # Define the XGBoost model creation function\n",
        "# def define_xgboost_model(hyperparameters):\n",
        "#     return xgb.XGBRegressor(**hyperparameters)\n",
        "\n",
        "# # Helper function to update the local model parameters\n",
        "# def set_parameters(model, parameters):\n",
        "#     booster = model.get_booster()\n",
        "#     booster.load_model(parameters)\n",
        "\n",
        "# # Helper function to get the local model parameters\n",
        "# def get_parameters(model):\n",
        "#     return model.get_booster().save_raw()\n",
        "\n",
        "# # Flower Client class\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, xgb_model, train_data, val_data):\n",
        "#         self.xgb_model = xgb_model\n",
        "#         self.train_data = train_data\n",
        "#         self.val_data = val_data\n",
        "\n",
        "#     def get_parameters(self):\n",
        "#         return get_parameters(self.xgb_model)\n",
        "\n",
        "#     def set_parameters(self, parameters):\n",
        "#         set_parameters(self.xgb_model, parameters)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Train the XGBoost model on the client's data\n",
        "#         features = self.train_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.train_data['energy_sum'].values\n",
        "#         self.xgb_model.fit(features, target)\n",
        "\n",
        "#         # Return the updated model parameters\n",
        "#         return self.get_parameters()\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         self.set_parameters(parameters)\n",
        "\n",
        "#         # Evaluate the XGBoost model on the client's validation data\n",
        "#         features = self.val_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']].values\n",
        "#         target = self.val_data['energy_sum'].values\n",
        "#         predictions = self.xgb_model.predict(features)\n",
        "#         loss = mean_squared_error(target, predictions)\n",
        "\n",
        "#         return loss, len(self.val_data), {}\n",
        "\n",
        "# # Create an instance of XGBoost model\n",
        "# hyperparameters = {'objective': 'reg:squarederror', 'learning_rate': 0.1, 'max_depth': 3}\n",
        "# xgboost_model = define_xgboost_model(hyperparameters)\n",
        "\n",
        "\n",
        "# # Create FlowerClient instances\n",
        "# NUM_CLIENTS = 5  # Adjust the number of clients as needed\n",
        "# clients = [XGBoostClient(xgboost_model, FL_train_set, FL_val_set) for _ in range(NUM_CLIENTS)]\n",
        "\n",
        "# # Before starting the simulation loop, call fit on each client\n",
        "# for client in clients:\n",
        "#     # Fit each client using the XGBoost model parameters\n",
        "#     client.fit(None, None)  # Initial fit with None parameters\n",
        "\n",
        "# # Start simulation\n",
        "# fl.simulation.start_simulation(\n",
        "#     client_fn=lambda cid: clients[cid],\n",
        "#     num_clients=NUM_CLIENTS,\n",
        "#     config=fl.server.ServerConfig(num_rounds=5),\n",
        "#     strategy=strategy,  # Define your strategy here\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yqTp44RAAZ-"
      },
      "outputs": [],
      "source": [
        "# def dataloader(data_list, n):\n",
        "#     for dataset in data_list:\n",
        "#         lclid = dataset['LCLid'].iloc[0]\n",
        "#         num_samples = len(dataset)\n",
        "\n",
        "#         print(f\"Loading data for LCLid {lclid}\")\n",
        "\n",
        "#         for i in range(0, num_samples, n):\n",
        "#             end_idx = min(i + n, num_samples)\n",
        "#             batch = dataset.iloc[i:end_idx]\n",
        "#             yield batch\n",
        "\n",
        "#         print(f\"Data loading completed for LCLid {lclid}\\n\")\n",
        "\n",
        "# n_samples_per_batch = 5  # Number of samples per batch\n",
        "\n",
        "# for batch_data in dataloader(FL_train_sets_list, n_samples_per_batch):\n",
        "#     print(batch_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIqXcQkm8YBG"
      },
      "source": [
        "FEDERATED CLIENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRW3-5MJ8TvU"
      },
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "\n",
        "# import flwr as fl\n",
        "# from flwr.common import Metrics\n",
        "# # from flwr.common import Weights\n",
        "# from flwr.common.typing import Parameters\n",
        "# from collections import OrderedDict\n",
        "# from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "# from flwr.common import NDArray, NDArrays\n",
        "\n",
        "\n",
        "# # # XGBoost model\n",
        "# # def define_xgboost_model(hyperparameters: dict) -> xgb.XGBModel:\n",
        "# #     # Hyperparameters for the XGBoost model\n",
        "# #     model = xgb.XGBModel(**hyperparameters)\n",
        "# #     return model\n",
        "\n",
        "# # # Extract hyperparameters from `best_xgb_model` and use it to define the FLOWER client model\n",
        "# # hyperparameters = best_xgb_model.get_xgb_params()\n",
        "# # client_xgb_model = define_xgboost_model(hyperparameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivkd9JmCIdr4"
      },
      "outputs": [],
      "source": [
        "# # Define a function to load local training data for each client\n",
        "# def load_local_data(client_data_list, client_index, batch_size):\n",
        "#     client_data = client_data_list[client_index]\n",
        "#     num_samples = len(client_data)\n",
        "\n",
        "#     # Calculate the starting and ending index for this client's data\n",
        "#     start_index = 0\n",
        "#     end_index = start_index + batch_size\n",
        "\n",
        "#     while start_index < num_samples:\n",
        "#         # Yield a batch of data\n",
        "#         yield client_data.iloc[start_index:end_index]\n",
        "\n",
        "#         # Move to the next batch\n",
        "#         start_index = end_index\n",
        "#         end_index = min(start_index + batch_size, num_samples)\n",
        "\n",
        "# # Example usage\n",
        "# client_index = 0  # Change this to select a different client\n",
        "# batch_size = 5   # Number of samples in each batch\n",
        "\n",
        "# # Load local training data for the selected client\n",
        "# for batch_data in load_local_data(FL_train_sets_list, client_index, batch_size):\n",
        "#     print(\"Loaded batch of data for client\", client_index)\n",
        "#     print(batch_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oM1iiYYPmOM"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, client_data, batch_size):\n",
        "#         self.client_data = client_data\n",
        "#         self.batch_size = batch_size\n",
        "#         self.client_xgb_model = None\n",
        "\n",
        "#     def get_parameters(self):\n",
        "#         if self.client_xgb_model is None:\n",
        "#             return np.array([])  # No parameters available initially\n",
        "#         model_params = self.client_xgb_model.get_booster().save_config()\n",
        "#         return fl.common.weights_to_parameters(model_params)\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         if self.client_xgb_model is None:\n",
        "#             hyperparameters = xgb.Booster(model_str=fl.common.parameters_to_weights(parameters))\n",
        "#             self.client_xgb_model = xgb.XGBModel(**hyperparameters)\n",
        "\n",
        "#         for batch_data in self.load_local_data(self.client_data, self.batch_size):\n",
        "#             batch_features = batch_data[[\"mean_temp\", \"pressure\", \"humidity\", \"windSpeed\"]]\n",
        "#             batch_target = batch_data[\"energy_sum\"]\n",
        "\n",
        "#             self.client_xgb_model.fit(batch_features, batch_target)\n",
        "\n",
        "#         updated_model_params = self.client_xgb_model.get_booster().save_config()\n",
        "#         return fl.common.weights_to_parameters(updated_model_params)\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         if self.client_xgb_model is None:\n",
        "#             return {}  # No evaluation possible without a trained model\n",
        "#         model_params = self.client_xgb_model.get_booster().save_config()\n",
        "#         booster = xgb.Booster(model_str=model_params)\n",
        "\n",
        "#         batch_data = self.load_local_data(self.client_data, len(self.client_data)).next()\n",
        "#         batch_features = batch_data[[\"mean_temp\", \"pressure\", \"humidity\", \"windSpeed\"]]\n",
        "#         batch_target = batch_data[\"energy_sum\"]\n",
        "\n",
        "#         predictions = booster.predict(xgb.DMatrix(batch_features))\n",
        "#         mse = np.mean((predictions - batch_target)**2)\n",
        "\n",
        "#         return {\"mse\": mse}\n",
        "\n",
        "#     def load_local_data(self, client_data, batch_size):\n",
        "#         num_samples = len(client_data)\n",
        "#         start_index = 0\n",
        "#         end_index = start_index + batch_size\n",
        "\n",
        "#         while start_index < num_samples:\n",
        "#             yield client_data.iloc[start_index:end_index]\n",
        "#             start_index = end_index\n",
        "#             end_index = min(start_index + batch_size, num_samples)\n",
        "\n",
        "# # Example usage\n",
        "# client_index = 0\n",
        "# batch_size = 5\n",
        "# client_instance = XGBoostClient(FL_train_sets_list[client_index], batch_size)\n",
        "\n",
        "# # fl.client.start_numpy_client(\"[::]:8080\", client=client_instance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLQ9uzPGaoB"
      },
      "source": [
        "Instance class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qaXjq6fGaRV"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, model, dataset, targets):\n",
        "#         self.model = model\n",
        "#         self.dataset = dataset\n",
        "#         self.targets = targets\n",
        "\n",
        "#     def get_parameters(self):\n",
        "#         # Return the current local model parameters\n",
        "#         return self.model.get_xgb_params()\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         # Set the model's parameters\n",
        "#         self.model.set_params(parameters)\n",
        "\n",
        "#         # Train the model on the local data\n",
        "#         self.model.fit(self.dataset, self.targets)\n",
        "\n",
        "#         # Return the updated model parameters\n",
        "#         return self.model.get_xgb_params()\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         # Set the model's parameters\n",
        "#         self.model.set_params(parameters)\n",
        "\n",
        "#         # Evaluate the model on the local data\n",
        "#         evaluation_result = self.model.score(self.dataset, self.targets)\n",
        "\n",
        "#         # Return the evaluation result\n",
        "#         return evaluation_result\n",
        "\n",
        "# # Create an instance of XGBoost model\n",
        "# xgboost_model = define_xgboost_model(hyperparameters)  # Define this function as mentioned before\n",
        "\n",
        "# # Create an instance of the XGBoostClient class\n",
        "# xgboost_client = XGBoostClient(model=xgboost_model, dataset=FL_val_set[[\"mean_temp\", \"pressure\", \"humidity\", \"windSpeed\"]], targets=FL_val_set[\"energy_sum\"])\n",
        "\n",
        "# # # Start the Flower client\n",
        "# # fl.client.start_numpy_client(server_address=\"[SERVER_ADDRESS]\", client=xgboost_client)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDSikcPVt7Go"
      },
      "source": [
        "## Should be doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPpxpmwOpfav"
      },
      "source": [
        "### Gated Convolution Neural Network (GCNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NciAJdMzmNnl"
      },
      "outputs": [],
      "source": [
        "# class GatedCNN24(nn.Module):\n",
        "#     def __init__(self, input_channels, hidden_sizes):\n",
        "#         super(GatedCNN24, self).__init__()\n",
        "#         self.input_channels = input_channels\n",
        "#         self.hidden_sizes = hidden_sizes\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(self.input_channels, self.hidden_sizes[0], kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(self.hidden_sizes[0], self.hidden_sizes[1], kernel_size=3, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(self.hidden_sizes[1], self.hidden_sizes[2], kernel_size=3, padding=1)\n",
        "\n",
        "#         self.gate_conv1 = nn.Conv2d(self.input_channels, self.hidden_sizes[0], kernel_size=3, padding=1)\n",
        "#         self.gate_conv2 = nn.Conv2d(self.hidden_sizes[0], self.hidden_sizes[1], kernel_size=3, padding=1)\n",
        "#         self.gate_conv3 = nn.Conv2d(self.hidden_sizes[1], self.hidden_sizes[2], kernel_size=3, padding=1)\n",
        "\n",
        "#         self.fc1 = nn.Linear(self.hidden_sizes[2], 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Convolutional layers\n",
        "#         conv1_out = self.conv1(x)\n",
        "#         conv2_out = self.conv2(conv1_out)\n",
        "#         conv3_out = self.conv3(conv2_out)\n",
        "\n",
        "#         # Gating layers\n",
        "#         gate_conv1_out = torch.sigmoid(self.gate_conv1(x))\n",
        "#         gate_conv2_out = torch.sigmoid(self.gate_conv2(conv1_out))\n",
        "#         gate_conv3_out = torch.sigmoid(self.gate_conv3(conv2_out))\n",
        "\n",
        "#         # Element-wise multiplication (gating)\n",
        "#         gated_conv1 = conv1_out * gate_conv1_out\n",
        "#         gated_conv2 = conv2_out * gate_conv2_out\n",
        "#         gated_conv3 = conv3_out * gate_conv3_out\n",
        "\n",
        "#         # Sum along channels\n",
        "#         gated_sum = gated_conv1 + gated_conv2 + gated_conv3\n",
        "\n",
        "#         # Global average pooling\n",
        "#         pooled = torch.mean(gated_sum, dim=[2, 3])\n",
        "\n",
        "#         # Fully connected layer\n",
        "#         output = self.fc1(pooled)\n",
        "#         return output.squeeze()\n",
        "\n",
        "# class EnergyDataset(Dataset):\n",
        "#     def __init__(self, features, targets):\n",
        "#         self.features = features\n",
        "#         self.targets = targets\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.features)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         inputs = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
        "#         target = torch.tensor(self.targets.iloc[idx], dtype=torch.float32)\n",
        "#         return inputs, target\n",
        "\n",
        "# # Define hyperparameters\n",
        "# input_channels = 4  # Number of features\n",
        "# hidden_sizes = [10, 8, 1]\n",
        "# learning_rate = 0.001\n",
        "# num_epochs = 10\n",
        "# batch_size = 50\n",
        "\n",
        "# # Create DataLoader for training\n",
        "# train_dataset = EnergyDataset(Central_X_train, Central_y_train)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Create the model\n",
        "# GatedCNN_model = GatedCNN24(input_channels, hidden_sizes)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(GatedCNN_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     GatedCNN_model.train()  # Set the model in training mode\n",
        "#     for inputs, targets in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Reshape the input tensor to match expected shape [batch_size, channels, height, width]\n",
        "#         inputs = inputs.view(inputs.size(0), input_channels, 1, 1)\n",
        "\n",
        "#         outputs = GatedCNN_model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcM_OfWUsMvC"
      },
      "source": [
        "### Gated Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pFnAbOBsL8T"
      },
      "outputs": [],
      "source": [
        "# # Build the Gated Neural Network model\n",
        "# inputs = Input(shape=(4,))\n",
        "# dense_layer = Dense(16, activation='relu')(inputs)\n",
        "\n",
        "# # Gating mechanism\n",
        "# dense_gate = Dense(16, activation='sigmoid')(inputs)\n",
        "# gated_dense = Multiply()([dense_layer, dense_gate])\n",
        "\n",
        "# output = Dense(1, activation='linear')(gated_dense)\n",
        "\n",
        "# GNN_model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# # Compile the model\n",
        "# GNN_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# # Train the model\n",
        "# GNN_model.fit(Central_X_train, Central_y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vWU1fCXVX94"
      },
      "source": [
        "Implementing FLower Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i6mrPCqIxbc"
      },
      "outputs": [],
      "source": [
        "# import flwr as fl\n",
        "\n",
        "# from flwr.client import NumPyClient\n",
        "# import xgboost as xgb\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import io\n",
        "\n",
        "# class XGBoostClient(fl.client.NumPyClient):\n",
        "#     def __init__(self, model, X_train, y_train, X_val, y_val):\n",
        "#         self.model = model\n",
        "#         self.X_train = X_train\n",
        "#         self.y_train = y_train\n",
        "#         self.X_val = X_val\n",
        "#         self.y_val = y_val\n",
        "\n",
        "#     def get_parameters(self, config=None):\n",
        "#         with io.BytesIO() as buffer:\n",
        "#             self.model.save_model(buffer)\n",
        "#             return buffer.getvalue()\n",
        "\n",
        "#     def fit(self, parameters, config):\n",
        "#         new_model = xgb.XGBRegressor(**config)\n",
        "#         with io.BytesIO(parameters) as buffer:\n",
        "#             new_model.load_model(buffer)\n",
        "#         self.model = new_model\n",
        "\n",
        "#         self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "#         with io.BytesIO() as buffer:\n",
        "#             self.model.save_model(buffer)\n",
        "#             return buffer.getvalue()\n",
        "\n",
        "#     def evaluate(self, parameters, config):\n",
        "#         new_model = xgb.Booster(params=config, model_str=parameters[0])\n",
        "#         self.model = new_model\n",
        "\n",
        "#         y_pred = self.model.predict(self.X_val)\n",
        "#         accuracy = accuracy_score(self.y_val, y_pred)\n",
        "\n",
        "#         return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeqMcrjHZGtt"
      },
      "outputs": [],
      "source": [
        "# # the best hyperparameters in best_xgb_model\n",
        "# best_hyperparameters = best_xgb_model.get_xgb_params()\n",
        "\n",
        "# # Create an instance of your XGBoost model with the best hyperparameters\n",
        "# best_xgb_model_instance = xgb.XGBRegressor(**best_hyperparameters)\n",
        "\n",
        "\n",
        "# # Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "# client_resources = None\n",
        "# if DEVICE.type == \"cuda\":\n",
        "#     client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# # Define a function to create a virtual client\n",
        "# def create_virtual_client(lclid):\n",
        "#     client_data = FL_train_set[FL_train_set['LCLid'] == lclid]\n",
        "#     X_client = client_data[['mean_temp', 'pressure', 'humidity', 'windSpeed']]\n",
        "#     y_client = client_data['energy_sum']\n",
        "#     return XGBoostClient(best_xgb_model_instance, X_client, y_client,  FL_test_set[['mean_temp', 'pressure', 'humidity', 'windSpeed']], FL_test_set['energy_sum'])\n",
        "\n",
        "# # Fit the XGBoost model before creating virtual clients\n",
        "# best_xgb_model_instance.fit(Central_X_train, Central_y_train)\n",
        "\n",
        "# # Create a list of virtual clients\n",
        "# virtual_clients = [create_virtual_client(lclid) for lclid in unique_lclids]\n",
        "\n",
        "# # Define the client function for Flower simulation\n",
        "# def client_fn(cid: str) -> XGBoostClient:\n",
        "#     virtual_client = virtual_clients[int(cid)]\n",
        "#     return virtual_client\n",
        "\n",
        "# # Define the Flower simulation configuration\n",
        "# config = fl.server.ServerConfig(num_rounds=5)\n",
        "\n",
        "# # Start the Flower simulation\n",
        "# fl.simulation.start_simulation(\n",
        "#     client_fn=client_fn,\n",
        "#     num_clients=len(virtual_clients),\n",
        "#     config=config,\n",
        "#     strategy=fl.server.strategy.FedAvg(),\n",
        "#     client_resources=None,  # Adjust client resources as needed\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AcAwekLwJOSZ"
      ],
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1oLHv4e-uSTEYGSiiJDsM6CuTErnsaA5X",
      "authorship_tag": "ABX9TyOwUd53PGVSFcec/t59uiwD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}